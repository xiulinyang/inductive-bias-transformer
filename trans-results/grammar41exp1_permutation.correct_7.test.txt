2024-12-19 09:47:49 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX
2024-12-19 09:47:51 | INFO | fairseq_cli.eval_lm | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/grammar41exp1_permutation/7-transformer/checkpoint_best.pt', 'post_process': None, 'quiet': True, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': True, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': None, 'task': {'_name': 'language_modeling', 'data': 'data-bin/grammar41exp1_permutation/correct_7-dataset', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2024-12-19 09:47:51 | INFO | fairseq.tasks.language_modeling | dictionary: 1136 types
2024-12-19 09:47:51 | INFO | fairseq_cli.eval_lm | loading model(s) from checkpoints/grammar41exp1_permutation/7-transformer/checkpoint_best.pt
/workspace/artificial-languages/fairseq/fairseq/checkpoint_utils.py:340: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(f, map_location=torch.device("cpu"))
2024-12-19 09:47:54 | INFO | fairseq_cli.eval_lm | num. model params: 10,038,784
2024-12-19 09:47:54 | INFO | fairseq.data.data_utils | loaded 136 examples from: data-bin/grammar41exp1_permutation/correct_7-dataset/test
2024-12-19 09:47:54 | INFO | fairseq_cli.eval_lm | data-bin/grammar41exp1_permutation/correct_7-dataset test 1 examples
2024-12-19 09:47:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-12-19 09:47:54 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2024-12-19 09:47:54 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2024-12-19 09:47:54 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1
2024-12-19 09:47:56 | INFO | fairseq_cli.eval_lm | 0 introvert [-15.766090]	the [-5.861256]	</s> [-2.208235]	close [-17.988558]	one [-3.978254]	</s> [-2.522750]	intercede [-18.835783]	the [-4.193919]	</s> [-2.581975]	folk [-17.249752]	hundred [-8.766147]	</s> [-3.094724]	slaughter [-18.679798]	a [-4.406407]	</s> [-3.528887]	slaughter [-18.599009]	the [-3.965356]	</s> [-2.650827]	rich [-15.538389]	a [-3.379685]	</s> [-3.365673]	rich [-15.610134]	the [-3.282521]	</s> [-2.800534]	rich [-15.442177]	one [-3.418022]	</s> [-2.716643]	secularize [-19.747059]	your [-7.637535]	</s> [-2.773758]	secularize [-19.698977]	hundred [-8.102160]	</s> [-2.982927]	secularize [-19.716707]	one [-4.196473]	</s> [-2.561188]	racketeer [-18.706869]	a [-3.704916]	</s> [-2.930608]	racketeer [-18.787800]	hundred [-8.673487]	</s> [-2.887480]	still [-18.207916]	a [-3.532426]	</s> [-3.146034]	number [-19.061007]	a [-4.489400]	</s> [-3.013551]	revitalise [-19.328245]	one [-4.086176]	</s> [-2.719214]	overspecialize [-19.277054]	your [-7.620530]	</s> [-2.932950]	weird [-17.632807]	your [-3.943236]	</s> [-2.896355]	weird [-17.533726]	hundred [-4.148805]	</s> [-2.946181]	rediscover [-17.978653]	a [-4.252030]	</s> [-3.033774]	mispronounce [-19.027405]	the [-3.938146]	</s> [-2.734093]	overemphasize [-17.807119]	one [-3.552047]	</s> [-2.537390]	choke [-18.477764]	my [-4.071475]	</s> [-2.656313]	choke [-18.531445]	an [-4.532514]	</s> [-2.956617]	choke [-18.371809]	your [-7.916158]	</s> [-2.817276]	crosscheck [-17.374590]	your [-9.305077]	</s> [-2.766487]	crosscheck [-17.422659]	hundred [-8.879991]	</s> [-2.972761]	crash [-17.694204]	the [-4.727674]	</s> [-2.809079]	crash [-17.512018]	hundred [-8.104390]	</s> [-2.897601]	splash [-18.694988]	an [-5.323658]	</s> [-2.965022]	splash [-18.645706]	my [-4.772377]	</s> [-2.628007]	spearhead [-18.282290]	an [-4.125925]	</s> [-2.987183]	spearhead [-18.186317]	your [-8.071877]	</s> [-2.892626]	spearhead [-18.368851]	my [-4.608848]	</s> [-2.674083]	influence [-17.565310]	an [-4.026293]	</s> [-3.200197]	influence [-17.309206]	one [-4.350398]	</s> [-2.786476]	jeopardize [-17.016033]	your [-7.961825]	</s> [-2.893770]	sweet [-15.789441]	the [-3.268782]	</s> [-2.740735]	sweet [-16.020998]	your [-3.427402]	</s> [-2.898730]	get [-19.194597]	the [-4.113772]	</s> [-2.773190]	dehumanize [-17.605410]	an [-4.022829]	</s> [-3.148857]	dehumanize [-17.533602]	the [-4.274917]	</s> [-2.652264]	slap [-18.484068]	hundred [-8.555530]	</s> [-3.040604]	slap [-18.516748]	my [-4.337894]	</s> [-2.848161]	blaspheme [-19.310900]	my [-4.101739]	</s> [-2.707211]	blaspheme [-19.524281]	the [-4.147216]	</s> [-2.727769]	back [-16.019547]	one [-3.460940]	</s> [-2.714604]	follow [-18.101748]	an [-4.074790]	</s> [-3.219811]	blackguard [-19.007196]	one [-3.928900]	</s> [-2.813841]	blackguard [-19.137026]	a [-3.973115]	</s> [-3.161793]	shove [-19.446947]	an [-4.031631]	</s> [-2.993575]	provision [-17.898046]	one [-3.986780]	</s> [-2.651497]	undervalue [-17.842772]	one [-4.270576]	</s> [-2.617667]	undervalue [-17.846628]	the [-3.960748]	</s> [-2.647012]	give [-17.732965]	an [-4.178811]	</s> [-2.985289]	privilege [-18.920176]	a [-4.854198]	</s> [-3.029039]	privilege [-19.014912]	one [-4.824158]	</s> [-2.694604]	decentralize [-17.506268]	a [-3.892035]	</s> [-3.024488]	misbehave [-17.602646]	one [-4.513391]	</s> [-2.710871]	misbehave [-17.650957]	your [-8.054607]	</s> [-2.970036]	brutalize [-19.410284]	hundred [-8.361800]	</s> [-2.999553]	brutalize [-19.472338]	an [-4.678305]	</s> [-3.037874]	dress [-18.922857]	hundred [-7.588859]	</s> [-2.994066]	rustle [-19.254448]	a [-4.479623]	</s> [-3.151565]	rustle [-19.272648]	one [-4.581303]	</s> [-2.756668]	wheelbarrow [-18.591415]	an [-3.612819]	</s> [-3.137638]	wheelbarrow [-18.488045]	my [-4.527805]	</s> [-2.695056]	wheelbarrow [-18.637545]	hundred [-7.579048]	</s> [-2.933197]	bring [-19.176989]	a [-3.920278]	</s> [-3.297736]	bring [-18.759670]	your [-8.087515]	</s> [-2.915173]	hydroplane [-16.884932]	the [-3.928629]	</s> [-2.862990]	hydroplane [-16.959553]	one [-3.418153]	</s> [-2.730404]	inconvenience [-17.876511]	hundred [-7.714940]	</s> [-3.039598]	inconvenience [-18.212118]	a [-3.462080]	</s> [-3.121992]	inconvenience [-17.970871]	the [-3.511216]	</s> [-2.877603]	inconvenience [-18.009377]	my [-4.278910]	</s> [-2.673808]	track [-18.470633]	the [-4.363721]	</s> [-2.758254]	mountebank [-18.981987]	my [-4.617253]	</s> [-2.668194]	pressurize [-18.103876]	your [-7.387118]	</s> [-2.947098]	soft [-17.035769]	one [-4.074251]	</s> [-2.759251]	promenade [-18.243986]	hundred [-8.464767]	</s> [-2.902296]	sneeze [-18.577951]	my [-4.460256]	</s> [-2.748607]	booby [-17.260185]	a [-4.166136]	</s> [-3.267162]	booby [-16.908295]	one [-3.737792]	</s> [-2.854864]	carry [-19.194994]	an [-4.463853]	</s> [-2.935318]	weasel [-19.084866]	your [-7.329909]	</s> [-2.906768]	glamorize [-18.164175]	your [-7.816252]	</s> [-2.861855]	headquarter [-19.217171]	the [-4.151801]	</s> [-2.914350]	headquarter [-19.241354]	your [-7.589437]	</s> [-2.980535]	come [-17.962099]	a [-4.088983]	</s> [-3.070713]	criticise [-18.436190]	a [-3.898515]	</s> [-3.107792]	criticise [-18.396900]	my [-4.414034]	</s> [-2.755934]	key [-18.345304]	one [-4.334887]	</s> [-2.792882]	bump [-18.105658]	your [-8.486731]	</s> [-2.949888]	power [-18.361406]	your [-8.464757]	</s> [-2.893029]	power [-18.290676]	the [-4.273957]	</s> [-2.756195]	intermarry [-17.549171]	a [-4.856646]	</s> [-3.097144]	hero [-18.299232]	my [-3.548835]	</s> [-2.725037]	hero [-18.057240]	a [-3.911877]	</s> [-3.003630]	vault [-18.182573]	your [-8.799297]	</s> [-3.019638]	vault [-18.337179]	one [-4.496971]	</s> [-2.751491]	vault [-18.703287]	hundred [-8.106835]	</s> [-2.932434]	prescribe [-18.495560]	your [-7.803756]	</s> [-2.976139]	buttonhole [-17.837992]	your [-7.744127]	</s> [-2.968814]	apologize [-18.685879]	one [-4.628591]	</s> [-2.648201]	medium [-14.159640]	the [-2.004952]	</s> [-2.855775]	medium [-14.446926]	my [-2.647836]	</s> [-2.771792]	replenish [-18.134720]	an [-4.160883]	</s> [-3.034874]	replenish [-18.324640]	my [-4.421669]	</s> [-2.656404]	stanchion [-18.906441]	an [-4.447968]	</s> [-2.930310]	stanchion [-18.895746]	hundred [-7.756701]	</s> [-3.001893]	interfere [-18.349236]	one [-4.127214]	</s> [-2.570499]	interfere [-18.319208]	your [-8.145888]	</s> [-2.877201]	disrespect [-17.991337]	a [-4.114750]	</s> [-3.167930]	rubber [-17.596001]	one [-4.373499]	</s> [-2.700704]	demagnetise [-18.325600]	one [-4.091419]	</s> [-2.607545]	misconduct [-18.477791]	an [-4.454122]	</s> [-2.936597]	misconduct [-18.254652]	your [-7.545093]	</s> [-2.858976]	constrict [-18.710369]	an [-5.153410]	</s> [-2.912300]	constrict [-19.001406]	a [-3.859999]	</s> [-2.944354]	constrict [-19.113331]	my [-4.679315]	</s> [-2.665337]	bucket [-19.366919]	your [-8.279879]	</s> [-2.921238]	bucket [-19.137007]	the [-3.894186]	</s> [-2.700293]	interchange [-19.816521]	my [-4.089542]	</s> [-2.596927]	disembowel [-18.812796]	an [-4.955735]	</s> [-2.968946]	disembowel [-18.782803]	the [-5.098232]	</s> [-2.479599]	square [-17.069004]	your [-8.573313]	</s> [-2.850055]	distemper [-18.974197]	hundred [-8.490405]	</s> [-2.980098]	fall [-19.644892]	one [-4.007708]	</s> [-2.641943]	fall [-19.607174]	hundred [-8.036305]	</s> [-3.019818]	pitchfork [-18.066835]	my [-4.379318]	</s> [-2.838034]	pitchfork [-18.033066]	your [-7.789762]	</s> [-2.973952]	intellectualize [-18.759554]	one [-4.050009]	</s> [-2.681632]	run [-15.421254]	my [-3.827575]	</s> [-2.823418]	run [-15.649494]	an [-4.083318]	</s> [-2.870039]
2024-12-19 09:47:56 | INFO | fairseq_cli.eval_lm | Evaluated 408 tokens in 1.2s (338.10 tokens/s)
2024-12-19 09:47:56 | INFO | fairseq_cli.eval_lm | Loss (base 2): 12.6311, Perplexity: 6343.79
