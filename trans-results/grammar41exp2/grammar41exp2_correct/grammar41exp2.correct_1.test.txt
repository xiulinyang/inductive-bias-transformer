2024-12-19 07:07:43 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX
2024-12-19 07:07:45 | INFO | fairseq_cli.eval_lm | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/grammar41exp2/1-transformer/checkpoint_best.pt', 'post_process': None, 'quiet': True, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': True, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': None, 'task': {'_name': 'language_modeling', 'data': 'data-bin/grammar41exp2/correct_1-dataset', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2024-12-19 07:07:45 | INFO | fairseq.tasks.language_modeling | dictionary: 1136 types
2024-12-19 07:07:45 | INFO | fairseq_cli.eval_lm | loading model(s) from checkpoints/grammar41exp2/1-transformer/checkpoint_best.pt
/workspace/artificial-languages/fairseq/fairseq/checkpoint_utils.py:340: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(f, map_location=torch.device("cpu"))
2024-12-19 07:07:50 | INFO | fairseq_cli.eval_lm | num. model params: 10,038,784
2024-12-19 07:07:50 | INFO | fairseq.data.data_utils | loaded 210 examples from: data-bin/grammar41exp2/correct_1-dataset/test
2024-12-19 07:07:50 | INFO | fairseq_cli.eval_lm | data-bin/grammar41exp2/correct_1-dataset test 2 examples
2024-12-19 07:07:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-12-19 07:07:50 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2024-12-19 07:07:50 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2024-12-19 07:07:50 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1
/opt/conda/envs/art/lib/python3.9/site-packages/torch/nn/functional.py:5849: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
2024-12-19 07:07:52 | INFO | fairseq_cli.eval_lm | 1 </s> [-0.497518]	an [-14.751393]	get [-8.831474]	</s> [-0.806439]	your [-13.861310]	get [-8.032101]	</s> [-0.779700]	the [-12.563669]	get [-8.751636]	</s> [-0.801865]	my [-14.266735]	sound [-9.583845]	</s> [-1.072000]	your [-13.515327]	sound [-9.795242]	</s> [-1.017884]	hundred [-13.255730]	sound [-9.300021]	</s> [-0.997001]	your [-13.329799]	soft [-9.602473]	</s> [-1.021035]	my [-13.626609]	soft [-9.529903]	</s> [-1.054230]	the [-12.517825]	come [-8.259993]	</s> [-0.901357]	an [-12.785302]	follow [-9.384677]	</s> [-0.898512]	my [-13.538312]	hydroplane [-9.065502]	</s> [-0.991175]	your [-12.944055]	rich [-5.631955]	</s> [-1.996742]	a [-13.056545]	rich [-5.562854]	</s> [-2.069230]	hundred [-13.040757]	rich [-5.102136]	</s> [-2.113235]	one [-11.983855]	privilege [-8.992738]	</s> [-0.792326]	your [-13.247598]	privilege [-9.104138]	</s> [-0.816652]	a [-12.932973]	privilege [-9.375843]	</s> [-0.817575]	my [-13.220604]	overemphasize [-9.164860]	</s> [-0.880195]	an [-12.578378]	rustle [-9.553875]	</s> [-0.854625]	your [-13.297917]	rustle [-9.464500]	</s> [-0.800349]	the [-12.633517]	rustle [-10.324305]	</s> [-0.843735]	my [-13.589891]	rustle [-9.840102]	</s> [-0.823326]	one [-11.993244]	keep [-9.804631]	</s> [-0.782393]	your [-13.202630]	keep [-9.238022]	</s> [-0.858281]	your [-13.112279]	undervalue [-9.529361]	</s> [-1.014539]	one [-11.956869]	undervalue [-10.115727]	</s> [-1.036760]	a [-13.059816]	undervalue [-10.022451]	</s> [-0.956371]	the [-12.377709]	disrespect [-9.644039]	</s> [-0.673876]	my [-13.215306]	disrespect [-8.611165]	</s> [-0.669841]	a [-12.882947]	interfere [-9.898866]	</s> [-0.723868]	hundred [-13.050879]	interfere [-9.521950]	</s> [-0.739500]	the [-12.251426]	prescribe [-10.417894]	</s> [-0.887646]	hundred [-12.545989]	prescribe [-9.689696]	</s> [-0.878645]	a [-12.874865]	slaughter [-8.860086]	</s> [-0.704635]	one [-11.809519]	slaughter [-9.621367]	</s> [-0.728135]	my [-13.100627]	slaughter [-8.904896]	</s> [-0.695007]	your [-13.044168]	backtrack [-8.859320]	</s> [-0.894228]	a [-12.924916]	rubber [-10.714813]	</s> [-0.822218]	hundred [-12.476413]	rubber [-8.949728]	</s> [-0.801388]
2024-12-19 07:07:52 | INFO | fairseq_cli.eval_lm | 0 my [-15.717536]	rediscover [-9.645403]	</s> [-1.050883]	hundred [-13.317016]	rediscover [-9.167508]	</s> [-0.981123]	a [-13.974529]	rediscover [-10.111774]	</s> [-0.986515]	your [-13.380293]	bring [-8.584139]	</s> [-0.984543]	a [-13.196486]	bring [-9.175840]	</s> [-0.888879]	a [-13.177249]	number [-8.117011]	</s> [-0.751872]	the [-12.172102]	number [-8.733925]	</s> [-0.722298]	hundred [-12.762826]	number [-8.125401]	</s> [-0.739675]	a [-13.247011]	folk [-9.270919]	</s> [-0.855776]	hundred [-12.811530]	folk [-7.833376]	</s> [-0.881995]	your [-13.009354]	still [-9.444193]	</s> [-1.049752]	a [-12.992225]	still [-10.043736]	</s> [-0.943708]	the [-12.191617]	prejudice [-9.991023]	</s> [-0.979993]	my [-13.399677]	prejudice [-8.989363]	</s> [-1.070463]	your [-12.794868]	prejudice [-9.542963]	</s> [-1.040936]	a [-13.112322]	prejudice [-9.307281]	</s> [-1.049088]	a [-13.086532]	crosscheck [-9.203850]	</s> [-0.984560]	an [-12.195033]	crosscheck [-9.354692]	</s> [-0.979481]	my [-13.312982]	crosscheck [-9.035449]	</s> [-1.011253]	hundred [-13.027885]	crosscheck [-8.764852]	</s> [-0.924445]	an [-12.756824]	power [-9.995755]	</s> [-1.256647]	your [-13.325076]	intercede [-8.646203]	</s> [-1.179313]	one [-12.128732]	intercede [-8.702244]	</s> [-1.087100]	hundred [-13.060855]	intercede [-8.621477]	</s> [-1.138265]	a [-13.252838]	sneeze [-9.461712]	</s> [-0.703470]	your [-13.148520]	sneeze [-9.490551]	</s> [-0.750753]	an [-12.531117]	sneeze [-9.892143]	</s> [-0.719783]	the [-12.530470]	smooth [-11.256659]	</s> [-0.891057]	one [-11.949983]	smooth [-10.533989]	</s> [-0.956028]	one [-11.835419]	back [-5.467936]	</s> [-1.420648]	your [-13.209341]	back [-5.581200]	</s> [-1.436064]	my [-13.345898]	interchange [-9.395077]	</s> [-1.173787]	your [-12.843019]	interchange [-9.406084]	</s> [-1.185411]	one [-11.887624]	interchange [-9.428579]	</s> [-1.213844]	hundred [-12.819024]	intermarry [-9.264292]	</s> [-0.910052]	the [-12.116275]	intermarry [-9.830140]	</s> [-0.892057]	the [-12.332891]	carry [-8.763510]	</s> [-0.894519]	my [-13.505678]	carry [-9.314243]	</s> [-0.885035]	the [-12.030545]	misbehave [-10.074815]	</s> [-1.254668]	your [-13.213798]	misbehave [-9.711842]	</s> [-1.257485]	hundred [-12.986509]	stand [-8.418250]	</s> [-0.952834]	an [-12.381120]	disembowel [-10.104683]	</s> [-0.881507]	my [-13.286125]	disembowel [-9.901802]	</s> [-0.785303]	hundred [-12.931284]	disembowel [-9.752777]	</s> [-0.796690]	your [-13.162471]	disembowel [-9.776157]	</s> [-0.867435]	hundred [-12.923770]	wheelbarrow [-9.664583]	</s> [-0.794445]	one [-12.228814]	wheelbarrow [-9.552376]	</s> [-0.762187]	an [-12.318755]	circumcise [-9.096412]	</s> [-0.833262]	your [-13.242770]	circumcise [-8.739290]	</s> [-0.814686]	hundred [-12.751453]	give [-6.138169]	</s> [-0.962989]	a [-12.885735]	give [-6.213512]	</s> [-1.036181]	my [-13.215876]	burlesque [-8.844309]	</s> [-0.673811]	one [-11.958269]	burlesque [-9.133441]	</s> [-0.692257]	my [-13.021926]	dillydally [-9.415991]	</s> [-0.973446]	my [-13.120051]	listen [-9.419273]	</s> [-1.040907]	hundred [-12.799802]	listen [-9.176208]	</s> [-1.051158]	a [-12.577719]	bucket [-10.239525]	</s> [-0.830082]	hundred [-13.011124]	bucket [-9.171999]	</s> [-0.745263]	an [-12.485727]	bucket [-9.900169]	</s> [-0.779009]	an [-12.570324]	distemper [-9.110733]	</s> [-1.183858]	your [-13.310830]	criticise [-9.751768]	</s> [-1.116005]	my [-13.373734]	predetermine [-8.253696]	</s> [-1.180783]	the [-12.351802]	predetermine [-8.537735]	</s> [-1.105425]	my [-13.131551]	hero [-9.257872]	</s> [-1.090519]	your [-13.508482]	dress [-9.445555]	</s> [-1.041234]	one [-12.231000]	intellectualize [-9.176126]	</s> [-0.813797]	your [-13.330853]	intellectualize [-8.815826]	</s> [-0.804006]	a [-12.939086]	intellectualize [-8.949872]	</s> [-0.707361]	the [-12.246278]	constrict [-10.145240]	</s> [-0.958977]	hundred [-12.918941]	constrict [-9.290831]	</s> [-0.969251]	an [-12.615872]	mountebank [-10.204651]	</s> [-0.862909]	your [-12.945477]	run [-5.786248]	</s> [-1.768694]	one [-11.789154]	run [-6.371195]	</s> [-1.621624]	an [-12.214141]	run [-6.607753]	</s> [-1.629745]	my [-13.157670]	run [-5.505290]	</s> [-1.812482]	a [-13.101437]	run [-6.269441]	</s> [-1.728950]	one [-12.231436]	booby [-8.760026]	</s> [-0.837023]	an [-12.481141]	booby [-9.819348]	</s> [-0.796946]	the [-12.222852]	introvert [-9.614733]	</s> [-1.102241]	an [-12.416873]	introvert [-9.459042]	</s> [-1.144036]	an [-12.569167]	square [-9.700016]	</s> [-1.029923]	the [-12.340981]	square [-10.046846]	</s> [-1.372484]	one [-11.706907]	blind [-5.028954]	</s> [-2.676993]	your [-13.056650]	headquarter [-9.841463]	</s> [-0.803030]	hundred [-12.618045]	headquarter [-9.584593]	</s> [-0.756883]	my [-12.989311]	headquarter [-9.449989]	</s> [-0.813173]	a [-12.905280]	headquarter [-9.955034]	</s> [-0.782295]	hundred [-12.759336]	ginger [-9.228266]	</s> [-1.043484]	your [-13.327268]	sweet [-6.432633]	</s> [-1.816605]	my [-13.276677]	gallivant [-9.173108]	</s> [-1.059871]	hundred [-12.893156]	weasel [-9.658018]	</s> [-1.482076]	my [-13.275363]	weasel [-9.585974]	</s> [-1.430239]	an [-12.681091]	weasel [-9.301213]	</s> [-1.422407]	a [-13.203445]	weasel [-9.632179]	</s> [-1.579477]	hundred [-12.936732]	stanchion [-9.114826]	</s> [-0.872523]	the [-12.198652]	replenish [-10.327666]	</s> [-1.039498]	a [-13.015446]	replenish [-10.933491]	</s> [-1.018146]	your [-13.370320]	replenish [-10.140425]	</s> [-0.954903]	my [-13.257477]	replenish [-9.985460]	</s> [-1.049068]	a [-12.934012]	weird [-5.965285]	</s> [-3.183241]	hundred [-12.646168]	misconduct [-9.445118]	</s> [-0.934707]	an [-12.443933]	misconduct [-9.771735]	</s> [-0.850265]	my [-13.239867]	inconvenience [-9.733604]	</s> [-0.861292]	one [-12.050462]	promenade [-9.910936]	</s> [-0.684339]	an [-12.488451]	slap [-10.346324]	</s> [-0.927082]	a [-12.868567]	slap [-9.821932]	</s> [-0.846066]	an [-12.759527]	racketeer [-9.099116]	</s> [-0.763359]	one [-12.093340]	racketeer [-9.233694]	</s> [-0.756694]	my [-12.864067]	racketeer [-9.234631]	</s> [-0.769904]	my [-12.942075]	fertilize [-9.816277]	</s> [-1.361573]	a [-12.380666]	fertilize [-10.711432]	</s> [-1.447983]	hundred [-12.276223]	fertilize [-9.501802]	</s> [-1.429892]	the [-11.825331]	mispronounce [-10.384964]	</s> [-1.218380]	a [-12.644165]	mispronounce [-9.701077]	</s> [-1.051267]	my [-13.108165]	break [-10.085577]	</s> [-0.998790]	an [-12.549700]	close [-8.832151]	</s> [-1.113834]	your [-12.925212]	influence [-9.754436]	</s> [-0.957979]	hundred [-12.166400]	influence [-10.440899]	</s> [-1.112106]	your [-12.986539]	crash [-8.879911]	</s> [-0.808779]	the [-11.937311]	crash [-9.684885]	</s> [-0.822203]	my [-13.133039]	buttonhole [-9.463536]	</s> [-0.798298]	a [-12.640266]	buttonhole [-9.883406]	</s> [-0.846278]	an [-12.053940]	vault [-10.629052]	</s> [-0.867624]	your [-12.813306]	glamorize [-9.775961]	</s> [-0.963033]	the [-12.257482]	glamorize [-10.323744]	</s> [-1.005669]	a [-12.681720]	glamorize [-10.139709]	</s> [-1.066350]	an [-12.491999]	blackguard [-10.244170]	</s> [-0.965456]	your [-13.352427]	blackguard [-9.381165]	</s> [-0.915929]	an [-12.489594]	reemphasize [-9.977983]	</s> [-1.362701]	one [-11.912993]	reemphasize [-10.095150]	</s> [-1.346576]	hundred [-12.638075]	reemphasize [-10.096721]	</s> [-1.265101]	the [-12.024345]	apologize [-10.559880]	</s> [-1.114598]	my [-13.029272]	apologize [-10.109307]	</s> [-1.028210]	your [-12.771872]	demagnetise [-9.982609]	</s> [-1.054457]	one [-11.855917]	demagnetise [-9.356868]	</s> [-0.938905]	my [-13.161636]	demagnetise [-10.057699]	</s> [-0.900574]	hundred [-12.563688]	shove [-10.000538]	</s> [-0.800209]	one [-12.159150]	shove [-10.201246]	</s> [-0.826705]	my [-13.341519]	choke [-9.502446]	</s> [-0.928550]	your [-13.179306]	choke [-8.785296]	</s> [-0.883404]	an [-12.646241]	decentralize [-10.194920]	</s> [-0.816484]	one [-11.697534]	outbalance [-10.283660]	</s> [-0.843150]	your [-12.975249]	track [-9.769436]	</s> [-0.787999]	a [-12.691002]	secularize [-10.671474]	</s> [-0.770522]	hundred [-12.450457]	secularize [-9.517925]	</s> [-0.784499]	the [-12.065132]	splash [-10.453829]	</s> [-0.938618]	one [-11.824908]	splash [-9.788246]	</s> [-1.008556]	the [-12.113399]	bump [-9.057136]	</s> [-0.908425]	your [-13.033324]	overspecialize [-9.592742]	</s> [-0.847562]	your [-12.759460]	blaspheme [-8.696894]	</s> [-1.022691]	one [-11.537649]	blaspheme [-8.616987]	</s> [-0.896339]	an [-12.448031]	blaspheme [-9.206397]	</s> [-0.863705]	your [-12.871443]	spearhead [-10.102558]	</s> [-0.765536]	one [-11.900627]	spearhead [-9.962811]	</s> [-0.823239]	an [-12.808638]	jeopardize [-9.808027]	</s> [-0.698686]	your [-13.161758]	jeopardize [-9.419785]	</s> [-0.696441]	my [-13.293852]	pressurize [-10.185307]	</s> [-0.716195]	a [-12.893156]	commercialize [-8.862981]	</s> [-0.950400]	an [-12.417729]	commercialize [-9.116287]	</s> [-0.900512]	your [-13.071661]	revitalise [-9.711799]	</s> [-0.873872]	one [-11.896805]	revitalise [-9.695822]	</s> [-0.841755]	hundred [-12.607436]	revitalise [-9.603043]	</s> [-0.839414]	a [-12.686114]	overcharge [-9.817608]	</s> [-0.843925]	an [-12.519733]	overcharge [-10.701500]	</s> [-0.887529]	my [-13.111455]	provision [-10.456611]	</s> [-0.864320]	an [-12.548073]	pitchfork [-8.792618]	</s> [-0.864380]	my [-13.077743]	brutalize [-10.406942]	</s> [-0.603880]	my [-13.081045]	key [-9.986719]	</s> [-0.907644]	hundred [-12.334517]	key [-10.125737]	</s> [-0.876436]	a [-12.706013]	key [-10.368362]	</s> [-0.961332]	one [-11.683266]	get [-9.290562]
2024-12-19 07:07:52 | INFO | fairseq_cli.eval_lm | Evaluated 630 tokens in 1.3s (489.53 tokens/s)
2024-12-19 07:07:52 | INFO | fairseq_cli.eval_lm | Loss (base 2): 11.0901, Perplexity: 2179.95
