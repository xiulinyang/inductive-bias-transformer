2024-12-19 10:24:18 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX
2024-12-19 10:24:21 | INFO | fairseq_cli.eval_lm | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/grammar41exp2/8-transformer/checkpoint_best.pt', 'post_process': None, 'quiet': True, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': True, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': None, 'task': {'_name': 'language_modeling', 'data': 'data-bin/grammar41exp2/correct_8-dataset', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2024-12-19 10:24:21 | INFO | fairseq.tasks.language_modeling | dictionary: 1136 types
2024-12-19 10:24:21 | INFO | fairseq_cli.eval_lm | loading model(s) from checkpoints/grammar41exp2/8-transformer/checkpoint_best.pt
/workspace/artificial-languages/fairseq/fairseq/checkpoint_utils.py:340: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(f, map_location=torch.device("cpu"))
2024-12-19 10:24:23 | INFO | fairseq_cli.eval_lm | num. model params: 10,038,784
2024-12-19 10:24:23 | INFO | fairseq.data.data_utils | loaded 227 examples from: data-bin/grammar41exp2/correct_8-dataset/test
2024-12-19 10:24:23 | INFO | fairseq_cli.eval_lm | data-bin/grammar41exp2/correct_8-dataset test 2 examples
2024-12-19 10:24:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-12-19 10:24:23 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2024-12-19 10:24:23 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2024-12-19 10:24:23 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1
/opt/conda/envs/art/lib/python3.9/site-packages/torch/nn/functional.py:5849: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
2024-12-19 10:24:25 | INFO | fairseq_cli.eval_lm | 1 </s> [-0.823945]	an [-14.934041]	constrict [-9.185596]	</s> [-1.073756]	hundred [-13.728774]	constrict [-9.092564]	</s> [-1.152371]	one [-14.006289]	burlesque [-10.301653]	</s> [-1.480771]	your [-14.061276]	burlesque [-10.039957]	</s> [-1.445674]	the [-14.109845]	burlesque [-10.487104]	</s> [-1.328343]	a [-14.136349]	burlesque [-10.493713]	</s> [-1.432204]	a [-14.094712]	fertilize [-10.603887]	</s> [-1.387636]	an [-14.780801]	fertilize [-10.715014]	</s> [-1.358037]	the [-13.745293]	provision [-10.888346]	</s> [-1.215258]	an [-14.569082]	provision [-10.577605]	</s> [-1.270519]	an [-14.648202]	hero [-10.817301]	</s> [-1.355159]	your [-13.623472]	hero [-9.729682]	</s> [-1.358556]	the [-13.772285]	inconvenience [-11.120541]	</s> [-1.064797]	the [-13.741689]	circumcise [-10.117440]	</s> [-1.249703]	one [-14.049320]	circumcise [-10.263257]	</s> [-1.292564]	your [-13.628841]	circumcise [-10.550957]	</s> [-1.347614]	a [-13.917581]	circumcise [-10.192192]	</s> [-1.222272]	hundred [-13.676074]	spearhead [-11.687500]	</s> [-1.328299]	the [-14.149284]	spearhead [-11.404265]	</s> [-1.231542]	one [-14.405431]	spearhead [-11.385541]	</s> [-1.229545]	an [-14.566480]	dress [-10.826340]	</s> [-1.671478]	your [-13.607243]	dress [-10.403049]	</s> [-1.409235]	a [-14.052131]	dress [-10.315482]	</s> [-1.379364]	an [-14.661236]	promenade [-9.899813]	</s> [-1.445504]	one [-14.317001]	promenade [-10.405996]	</s> [-1.473080]	a [-13.847816]	headquarter [-10.390825]	</s> [-1.131201]	an [-14.728638]	headquarter [-10.884782]	</s> [-1.133814]	hundred [-13.737913]	headquarter [-10.217326]	</s> [-1.256210]	your [-13.546070]	listen [-10.622682]	</s> [-1.685432]	a [-13.959218]	listen [-11.312037]	</s> [-1.537608]	your [-13.429484]	outbalance [-10.467387]	</s> [-1.782038]	my [-13.666576]	outbalance [-9.766095]	</s> [-1.663997]	an [-14.364903]	outbalance [-10.456087]	</s> [-1.709801]	the [-13.769273]	outbalance [-10.980292]	</s> [-1.722086]	an [-14.525650]	gallivant [-10.016362]	</s> [-1.276229]	hundred [-13.289995]	reemphasize [-10.410572]	</s> [-1.230042]	an [-14.683051]	pressurize [-10.503064]	</s> [-1.103810]	my [-13.719826]	pressurize [-10.387499]	</s> [-1.089490]	your [-13.321482]	pressurize [-10.072048]	</s> [-1.072306]	a [-13.961610]	pressurize [-10.114685]	</s> [-1.129174]	an [-14.571041]	prescribe [-10.036887]	</s> [-0.739900]	your [-13.302316]	prescribe [-9.540167]	</s> [-0.739202]	one [-14.252158]	key [-9.958780]	</s> [-1.145426]	hundred [-13.491138]	key [-9.502214]	</s> [-1.233852]	an [-14.654667]	interchange [-9.729086]	</s> [-1.001602]	hundred [-13.449334]	interchange [-10.309471]	</s> [-0.884345]	one [-14.278848]	misconduct [-10.285102]	</s> [-0.941496]	a [-13.798289]	misconduct [-10.415235]	</s> [-1.045726]	hundred [-13.492320]	misconduct [-10.243274]	</s> [-1.011707]	your [-13.225661]	bucket [-10.279914]	</s> [-0.910015]	hundred [-13.336102]	bucket [-9.810349]	</s> [-0.866944]	the [-13.735113]	bucket [-10.734259]	</s> [-0.813693]	an [-14.452624]	rustle [-10.946997]	</s> [-0.977309]	the [-13.583843]	rustle [-11.677122]	</s> [-0.973690]	hundred [-13.275580]	backtrack [-9.509351]	</s> [-1.108868]	the [-13.721121]	backtrack [-10.250118]	</s> [-1.264240]
2024-12-19 10:24:25 | INFO | fairseq_cli.eval_lm | 0 one [-14.563943]	crosscheck [-9.654770]	</s> [-1.288814]	an [-15.020327]	crosscheck [-8.682250]	</s> [-1.251427]	my [-13.810671]	decentralize [-9.659096]	</s> [-1.144886]	your [-14.189473]	decentralize [-9.309911]	</s> [-1.256023]	an [-14.960311]	decentralize [-9.473587]	</s> [-1.192471]	hundred [-13.847745]	keep [-9.522986]	</s> [-2.201461]	a [-14.261395]	keep [-9.634924]	</s> [-2.243252]	my [-13.882169]	keep [-9.999793]	</s> [-2.120302]	a [-14.185608]	soft [-11.154020]	</s> [-1.530320]	one [-14.046620]	soft [-11.146481]	</s> [-1.634179]	your [-13.582367]	pitchfork [-10.367472]	</s> [-1.388566]	hundred [-13.525650]	pitchfork [-9.733581]	</s> [-1.304317]	one [-14.059951]	pitchfork [-9.953766]	</s> [-1.344342]	the [-13.786594]	pitchfork [-10.743638]	</s> [-1.324908]	hundred [-13.508706]	bump [-10.776696]	</s> [-1.727427]	a [-14.151601]	bump [-10.892105]	</s> [-1.842921]	the [-14.027213]	give [-8.128799]	</s> [-1.287461]	one [-14.345027]	give [-7.610958]	</s> [-1.300051]	my [-14.197255]	give [-7.302813]	</s> [-1.316630]	one [-14.564899]	sneeze [-9.796658]	</s> [-1.415067]	my [-13.983350]	sneeze [-10.083209]	</s> [-1.502427]	a [-14.233017]	distemper [-9.551230]	</s> [-0.946029]	your [-13.818125]	disrespect [-9.831438]	</s> [-0.926686]	a [-14.242129]	disrespect [-9.726685]	</s> [-1.033671]	hundred [-13.675965]	disrespect [-10.417393]	</s> [-1.102941]	an [-14.745202]	dillydally [-9.583467]	</s> [-1.463645]	your [-13.810814]	dillydally [-9.857441]	</s> [-1.437560]	the [-14.175665]	dillydally [-9.842587]	</s> [-1.566523]	your [-13.723516]	still [-10.053837]	</s> [-1.526928]	an [-14.831759]	square [-9.917130]	</s> [-1.670233]	my [-13.961246]	square [-9.258463]	</s> [-1.791033]	the [-14.043851]	choke [-10.695461]	</s> [-1.412984]	an [-14.506612]	choke [-10.781274]	</s> [-1.346448]	my [-13.683300]	choke [-9.971767]	</s> [-1.403736]	a [-14.003790]	carry [-9.638020]	</s> [-1.235357]	your [-13.408395]	carry [-9.346591]	</s> [-1.081716]	my [-13.669154]	carry [-9.294332]	</s> [-1.000792]	the [-14.022502]	carry [-10.000588]	</s> [-0.990395]	hundred [-13.322586]	jeopardize [-10.460644]	</s> [-1.038140]	your [-13.529661]	jeopardize [-10.133265]	</s> [-1.115127]	my [-13.881775]	jeopardize [-9.973113]	</s> [-1.063192]	a [-13.732153]	jeopardize [-10.318617]	</s> [-1.023699]	one [-14.270051]	blackguard [-10.664202]	</s> [-1.362217]	my [-13.719723]	blackguard [-10.324474]	</s> [-1.367983]	an [-14.682985]	brutalize [-9.271018]	</s> [-0.925551]	a [-13.887314]	brutalize [-10.029680]	</s> [-0.899536]	hundred [-13.490116]	brutalize [-9.566393]	</s> [-0.838591]	a [-13.890878]	introvert [-9.802370]	</s> [-0.932445]	an [-14.640108]	introvert [-10.706926]	</s> [-0.889907]	one [-14.206170]	introvert [-9.948418]	</s> [-0.952060]	your [-13.370471]	prejudice [-9.228569]	</s> [-1.032377]	a [-13.788520]	undervalue [-10.447660]	</s> [-1.030409]	hundred [-13.359276]	undervalue [-10.198190]	</s> [-1.068719]	the [-13.658406]	undervalue [-10.799091]	</s> [-1.093659]	my [-13.473260]	overspecialize [-10.670099]	</s> [-0.881617]	an [-14.582067]	slaughter [-10.445046]	</s> [-1.044863]	one [-13.980289]	slaughter [-10.005500]	</s> [-1.086340]	hundred [-13.294597]	back [-6.761357]	</s> [-1.918159]	an [-14.439421]	back [-6.553377]	</s> [-1.971451]	an [-14.526227]	break [-11.122204]	</s> [-1.499968]	one [-13.927619]	break [-10.680206]	</s> [-1.425885]	hundred [-13.366219]	interfere [-9.804812]	</s> [-1.330267]	a [-13.770829]	get [-10.601723]	</s> [-1.013644]	an [-14.384117]	get [-11.109184]	</s> [-0.952900]	my [-13.649192]	get [-9.919196]	</s> [-0.894330]	the [-14.179426]	criticise [-10.734920]	</s> [-1.287870]	hundred [-13.331361]	criticise [-9.926557]	</s> [-1.199544]	the [-14.038893]	crash [-10.331652]	</s> [-1.073640]	a [-13.862701]	crash [-10.177814]	</s> [-1.101082]	an [-14.795893]	number [-10.538090]	</s> [-0.734708]	my [-13.713913]	number [-9.848885]	</s> [-0.775870]	the [-13.875197]	number [-10.571421]	</s> [-0.832987]	hundred [-13.147591]	blind [-5.293276]	</s> [-2.836718]	the [-13.727821]	blind [-5.945431]	</s> [-2.852046]	your [-13.421616]	blind [-5.766700]	</s> [-2.701108]	my [-13.491784]	disembowel [-9.283310]	</s> [-0.819679]	the [-13.848390]	disembowel [-9.804995]	</s> [-0.774641]	hundred [-13.266701]	slap [-10.390544]	</s> [-1.049347]	your [-13.412710]	sweet [-6.586763]	</s> [-1.944012]	one [-14.167282]	sweet [-6.472008]	</s> [-1.967508]	a [-13.855194]	intermarry [-11.339716]	</s> [-0.873658]	your [-13.488187]	misbehave [-10.504907]	</s> [-1.253288]	my [-13.384752]	misbehave [-10.505219]	</s> [-1.257009]	your [-13.569721]	splash [-10.059133]	</s> [-1.143257]	hundred [-13.347136]	splash [-10.128736]	</s> [-1.128622]	an [-14.555524]	splash [-10.251691]	</s> [-1.297158]	hundred [-13.261967]	mispronounce [-10.528337]	</s> [-1.347525]	a [-13.826221]	mispronounce [-10.606285]	</s> [-1.263688]	your [-13.619708]	folk [-10.497420]	</s> [-1.724392]	the [-13.964915]	folk [-10.696568]	</s> [-1.778758]	my [-13.849622]	folk [-10.000208]	</s> [-1.798852]	an [-14.705665]	stanchion [-10.157864]	</s> [-1.160576]	the [-13.813542]	stanchion [-10.739099]	</s> [-1.093080]	my [-13.644345]	stanchion [-10.227981]	</s> [-1.139984]	an [-14.616193]	secularize [-10.923023]	</s> [-1.106980]	a [-13.795129]	revitalise [-9.939172]	</s> [-1.142966]	one [-14.281398]	revitalise [-10.126108]	</s> [-1.120048]	an [-14.817404]	shove [-10.101972]	</s> [-1.166997]	one [-14.441888]	shove [-9.688310]	</s> [-1.245639]	your [-13.643617]	shove [-9.857999]	</s> [-1.258619]	your [-13.606350]	sound [-10.488718]	</s> [-1.513687]	the [-13.783506]	sound [-11.908248]	</s> [-1.377374]	hundred [-13.287941]	booby [-10.704244]	</s> [-1.549941]	one [-14.300350]	booby [-10.410448]	</s> [-1.464677]	an [-14.713502]	booby [-10.491361]	</s> [-1.464051]	hundred [-13.360989]	overemphasize [-10.126652]	</s> [-0.959507]	one [-14.330832]	overemphasize [-9.952489]	</s> [-0.978499]	an [-14.777081]	blaspheme [-10.396606]	</s> [-1.156399]	hundred [-13.107813]	blaspheme [-9.776028]	</s> [-1.024074]	your [-13.612752]	blaspheme [-9.423799]	</s> [-1.034992]	my [-13.447814]	blaspheme [-9.858517]	</s> [-1.071956]	the [-13.662407]	privilege [-9.762053]	</s> [-1.126813]	your [-13.555509]	privilege [-9.643797]	</s> [-1.059354]	an [-14.462408]	bring [-9.801361]	</s> [-1.098752]	the [-13.750742]	bring [-10.267370]	</s> [-1.215638]	my [-13.884191]	rediscover [-9.645182]	</s> [-1.049319]	your [-13.744856]	follow [-9.650318]	</s> [-0.722159]	hundred [-13.106882]	follow [-9.451609]	</s> [-0.747679]	one [-14.199349]	mountebank [-10.792753]	</s> [-1.143553]	a [-13.704065]	mountebank [-9.995928]	</s> [-1.080809]	the [-13.876471]	mountebank [-10.484035]	</s> [-1.291825]	my [-13.727576]	predetermine [-9.464981]	</s> [-1.030392]	one [-13.909679]	predetermine [-10.121513]	</s> [-1.045927]	your [-13.523833]	medium [-5.244458]	</s> [-2.649945]	hundred [-13.288857]	buttonhole [-9.953555]	</s> [-1.465701]	one [-14.249555]	run [-6.520792]	</s> [-1.179296]	a [-13.937964]	run [-6.363721]	</s> [-1.175890]	one [-14.379270]	hydroplane [-10.659861]	</s> [-1.264525]	hundred [-13.052869]	hydroplane [-10.773778]	</s> [-1.389619]	a [-13.889021]	hydroplane [-10.617368]	</s> [-1.175590]	hundred [-13.180965]	rich [-5.580940]	</s> [-3.128734]	your [-13.565302]	commercialize [-9.567167]	</s> [-1.113988]	your [-13.433138]	fall [-10.276531]	</s> [-0.909765]	hundred [-12.963958]	fall [-10.290623]	</s> [-0.872011]	my [-13.936348]	fall [-9.931433]	</s> [-0.891675]	hundred [-13.275234]	come [-10.434773]	</s> [-1.027200]	one [-14.403295]	come [-10.034973]	</s> [-1.065993]	one [-14.490387]	commingle [-10.702857]	</s> [-1.353837]	the [-14.013037]	dehumanize [-10.302924]	</s> [-0.726623]	hundred [-13.246685]	dehumanize [-10.079286]	</s> [-0.782683]	your [-13.501760]	dehumanize [-9.845573]	</s> [-0.805866]	an [-14.588966]	smooth [-10.767986]	</s> [-1.092494]	one [-14.313625]	smooth [-10.880755]	</s> [-1.101002]	a [-14.054676]	power [-10.554209]	</s> [-1.206995]	my [-13.824691]	power [-10.658970]	</s> [-1.145649]	the [-13.919088]	power [-10.854122]	</s> [-1.107017]	a [-14.035942]	wheelbarrow [-10.124807]	</s> [-1.643286]	the [-13.748747]	wheelbarrow [-10.222146]	</s> [-1.490122]	a [-14.109766]	vault [-10.751571]	</s> [-1.634313]	one [-14.383703]	vault [-11.216696]	</s> [-1.719697]	an [-14.690382]	close [-8.745736]	</s> [-2.203193]	one [-14.632760]	close [-9.084529]	</s> [-2.102785]	an [-14.952971]	intercede [-10.737456]	</s> [-1.386874]	a [-14.165912]	intercede [-9.811306]	</s> [-1.339720]	my [-13.964915]	intercede [-9.727392]	</s> [-1.255911]	your [-14.132935]	ginger [-9.658722]	</s> [-1.076569]	the [-14.404310]	ginger [-10.325583]	</s> [-1.147027]	a [-14.384022]	glamorize [-11.038037]	</s> [-1.083781]	the [-14.251896]	glamorize [-11.471006]	</s> [-1.205101]	one [-14.686551]	glamorize [-11.156689]	</s> [-1.230870]	my [-13.808777]	replenish [-9.379079]	</s> [-1.279250]	a [-14.142537]	replenish [-10.053742]	</s> [-1.096734]	your [-13.717795]	influence [-10.350881]	</s> [-1.328279]	one [-14.563258]	racketeer [-10.641539]	</s> [-1.184314]	my [-13.994269]	racketeer [-9.432673]	</s> [-1.208910]	a [-14.223219]	racketeer [-9.590979]	</s> [-1.266195]	the [-14.053705]	overcharge [-10.892514]	</s> [-1.338733]	your [-13.768252]	overcharge [-9.770360]	</s> [-1.317086]	a [-14.052925]	overcharge [-10.600416]	</s> [-1.461346]	hundred [-13.275572]	track [-10.738670]	</s> [-1.673517]	my [-13.778904]	track [-10.729638]
2024-12-19 10:24:25 | INFO | fairseq_cli.eval_lm | Evaluated 681 tokens in 0.8s (833.97 tokens/s)
2024-12-19 10:24:25 | INFO | fairseq_cli.eval_lm | Loss (base 2): 12.1314, Perplexity: 4486.69
