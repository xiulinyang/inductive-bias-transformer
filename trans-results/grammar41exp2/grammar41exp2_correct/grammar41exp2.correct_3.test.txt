2024-12-19 08:16:12 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX
2024-12-19 08:16:15 | INFO | fairseq_cli.eval_lm | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/grammar41exp2/3-transformer/checkpoint_best.pt', 'post_process': None, 'quiet': True, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': True, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': None, 'task': {'_name': 'language_modeling', 'data': 'data-bin/grammar41exp2/correct_3-dataset', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2024-12-19 08:16:15 | INFO | fairseq.tasks.language_modeling | dictionary: 1136 types
2024-12-19 08:16:15 | INFO | fairseq_cli.eval_lm | loading model(s) from checkpoints/grammar41exp2/3-transformer/checkpoint_best.pt
/workspace/artificial-languages/fairseq/fairseq/checkpoint_utils.py:340: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(f, map_location=torch.device("cpu"))
2024-12-19 08:16:18 | INFO | fairseq_cli.eval_lm | num. model params: 10,038,784
2024-12-19 08:16:18 | INFO | fairseq.data.data_utils | loaded 236 examples from: data-bin/grammar41exp2/correct_3-dataset/test
2024-12-19 08:16:18 | INFO | fairseq_cli.eval_lm | data-bin/grammar41exp2/correct_3-dataset test 2 examples
2024-12-19 08:16:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-12-19 08:16:18 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2024-12-19 08:16:18 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2024-12-19 08:16:18 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1
/opt/conda/envs/art/lib/python3.9/site-packages/torch/nn/functional.py:5849: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
2024-12-19 08:16:20 | INFO | fairseq_cli.eval_lm | 1 </s> [-0.275285]	hundred [-11.851758]	blackguard [-8.587673]	</s> [-0.549185]	an [-10.815581]	buttonhole [-9.943413]	</s> [-0.845407]	my [-9.869707]	buttonhole [-9.924921]	</s> [-1.017595]	the [-11.443166]	buttonhole [-10.040872]	</s> [-1.131457]	my [-10.606602]	privilege [-10.814112]	</s> [-0.905247]	an [-11.194257]	privilege [-10.636650]	</s> [-0.872711]	your [-11.975721]	racketeer [-9.726892]	</s> [-1.123680]	hundred [-10.978859]	racketeer [-9.952999]	</s> [-1.125883]	your [-11.428661]	shove [-9.665649]	</s> [-0.893418]	an [-10.621964]	shove [-10.795186]	</s> [-0.979306]	a [-11.227255]	shove [-10.566928]	</s> [-0.870522]	hundred [-10.555323]	disembowel [-9.964972]	</s> [-0.581388]	the [-11.377168]	disembowel [-9.976219]	</s> [-0.628624]	a [-11.352637]	disembowel [-10.478203]	</s> [-0.687195]	an [-10.764164]	disembowel [-10.223264]	</s> [-0.714164]	your [-11.413692]	decentralize [-9.778368]	</s> [-0.887899]	an [-11.176079]	decentralize [-9.511086]	</s> [-0.847685]	my [-10.707472]	decentralize [-9.701998]	</s> [-0.902765]	hundred [-10.878729]	dress [-11.185617]	</s> [-1.030373]	the [-11.393279]	dress [-10.772567]	</s> [-1.151797]	your [-12.051109]	dress [-10.469635]	</s> [-1.292583]	one [-11.619487]	dress [-11.384426]	</s> [-1.225857]	your [-12.048212]	intellectualize [-9.044790]	</s> [-0.735944]	hundred [-11.099909]	intellectualize [-9.611902]	</s> [-0.808107]	a [-11.658114]	dehumanize [-10.254040]	</s> [-0.923903]	one [-11.539455]	dehumanize [-10.125656]	</s> [-1.026466]	the [-11.524192]	jeopardize [-10.827860]	</s> [-1.179961]	my [-11.205833]	intermarry [-9.613249]	</s> [-0.767747]	a [-11.846545]	intermarry [-10.185292]	</s> [-0.731797]	an [-11.385483]	intermarry [-10.340854]	</s> [-0.665789]	your [-11.867505]	weird [-6.166138]	</s> [-1.440343]	a [-11.696152]	booby [-10.930513]	</s> [-1.125263]	hundred [-10.942594]	booby [-10.613559]	</s> [-1.272421]	one [-11.187090]	booby [-10.965910]	</s> [-1.145980]	an [-11.203881]	undervalue [-10.531574]	</s> [-1.135301]	one [-11.313228]	undervalue [-10.637944]	</s> [-1.104392]	my [-10.861161]	pitchfork [-9.347889]	</s> [-0.907852]	an [-11.307604]	pitchfork [-9.281805]	</s> [-0.805425]	hundred [-10.474031]	ginger [-9.519003]	</s> [-0.952910]	my [-10.721951]	ginger [-9.835648]	</s> [-0.996617]	one [-11.129384]	ginger [-9.675163]	</s> [-0.947458]	a [-11.154680]	crosscheck [-9.376322]	</s> [-0.905855]	a [-11.232532]	power [-10.475465]	</s> [-0.925406]	the [-11.085653]	power [-9.790028]	</s> [-0.911885]	hundred [-10.889175]	power [-10.409840]	</s> [-1.039930]	an [-10.958694]	folk [-9.908953]	</s> [-1.118322]	my [-10.714264]	mountebank [-11.504841]	</s> [-0.832572]	one [-11.203032]	mountebank [-11.321482]	</s> [-0.839657]	the [-11.192569]	mountebank [-11.415926]	</s> [-0.891349]	your [-11.616100]	smooth [-9.728956]	</s> [-1.053069]	one [-11.231906]	blaspheme [-10.309278]	</s> [-0.929263]	the [-11.045020]	blaspheme [-10.413672]	</s> [-0.941760]	my [-10.621379]	blaspheme [-10.175852]	</s> [-0.937853]	hundred [-10.718229]	demagnetise [-9.937866]	</s> [-1.194367]	your [-11.376805]	demagnetise [-9.769170]	</s> [-1.081285]	hundred [-10.893366]	come [-8.576845]	</s> [-0.729220]	one [-11.052896]	come [-9.288861]	</s> [-0.726103]	one [-11.003373]	prejudice [-10.283882]	</s> [-0.904519]	a [-11.350422]	prejudice [-10.378986]	</s> [-1.002321]	my [-10.602140]	prejudice [-9.973133]	</s> [-1.037714]	my [-10.425353]	brutalize [-10.162477]	</s> [-0.916446]	the [-11.008076]	square [-9.753234]	</s> [-1.312545]	your [-11.405287]	square [-9.767066]	</s> [-1.300017]	hundred [-10.661301]	still [-10.731661]	</s> [-0.874382]	your [-11.516166]	still [-10.162991]	</s> [-0.781132]
2024-12-19 08:16:20 | INFO | fairseq_cli.eval_lm | 0 your [-14.001026]	influence [-8.972481]	</s> [-0.918785]	my [-10.640991]	influence [-9.094303]	</s> [-0.863259]	an [-11.552104]	influence [-9.125522]	</s> [-0.956790]	my [-11.106221]	sound [-9.493253]	</s> [-1.047270]	your [-12.014091]	sound [-9.951475]	</s> [-1.084280]	hundred [-11.283508]	interchange [-9.652725]	</s> [-0.813596]	my [-11.211094]	interchange [-9.564548]	</s> [-0.783449]	a [-12.032634]	interchange [-10.358083]	</s> [-0.850708]	an [-11.259863]	rich [-5.136985]	</s> [-1.008301]	a [-11.799615]	rich [-5.287444]	</s> [-1.057905]	one [-11.611798]	rich [-4.955858]	</s> [-0.940389]	an [-11.332329]	reemphasize [-8.918415]	</s> [-0.683510]	one [-11.516994]	commingle [-9.759426]	</s> [-0.617394]	the [-11.985351]	inconvenience [-9.597984]	</s> [-0.662003]	a [-11.799558]	slaughter [-9.659871]	</s> [-1.046843]	the [-11.669439]	slaughter [-9.664911]	</s> [-1.021001]	hundred [-11.441663]	slaughter [-9.804008]	</s> [-0.907611]	one [-11.623114]	slaughter [-10.046122]	</s> [-0.984102]	your [-12.049723]	break [-10.245927]	</s> [-0.621384]	the [-11.650234]	pressurize [-10.082469]	</s> [-0.913556]	one [-11.736735]	pressurize [-10.598749]	</s> [-1.027741]	a [-12.168200]	pressurize [-10.461555]	</s> [-0.960010]	an [-11.881132]	pressurize [-10.605148]	</s> [-0.869013]	my [-11.453405]	pressurize [-9.930945]	</s> [-0.885490]	your [-12.112671]	choke [-10.900364]	</s> [-0.900866]	my [-11.446959]	choke [-10.898336]	</s> [-1.021566]	a [-11.952431]	choke [-11.003931]	</s> [-1.110675]	the [-11.899000]	distemper [-10.749893]	</s> [-1.273541]	a [-12.203014]	distemper [-10.955387]	</s> [-1.364954]	one [-11.438951]	gallivant [-10.828015]	</s> [-0.857694]	the [-11.615194]	apologize [-10.632099]	</s> [-1.259361]	a [-12.018469]	apologize [-10.373136]	</s> [-1.262314]	one [-11.324038]	apologize [-10.672997]	</s> [-1.483925]	a [-11.686756]	blind [-5.428981]	</s> [-1.701519]	an [-11.443001]	wheelbarrow [-9.650618]	</s> [-0.830076]	hundred [-11.201075]	wheelbarrow [-10.050848]	</s> [-0.835585]	my [-11.049140]	wheelbarrow [-9.288332]	</s> [-0.836047]	the [-11.548215]	headquarter [-9.515979]	</s> [-1.143998]	an [-11.398137]	predetermine [-9.825180]	</s> [-0.839312]	my [-10.884313]	interfere [-9.998358]	</s> [-0.874291]	one [-11.330630]	interfere [-10.332254]	</s> [-0.837777]	the [-11.062967]	dillydally [-10.679272]	</s> [-0.709573]	my [-11.065301]	dillydally [-10.721153]	</s> [-0.764302]	an [-11.324408]	bucket [-9.554678]	</s> [-0.903297]	a [-11.596699]	bucket [-9.614541]	</s> [-1.017518]	hundred [-11.086677]	hero [-9.692435]	</s> [-1.142583]	my [-10.959396]	hero [-9.993772]	</s> [-1.020394]	your [-11.756616]	hero [-10.233020]	</s> [-1.045885]	my [-11.157981]	outbalance [-10.197481]	</s> [-0.670320]	your [-11.756492]	outbalance [-10.190572]	</s> [-0.728647]	one [-11.246607]	outbalance [-10.634098]	</s> [-0.724260]	an [-11.313951]	weasel [-10.894212]	</s> [-1.085529]	a [-11.427261]	weasel [-10.783579]	</s> [-1.058669]	one [-11.131990]	overcharge [-10.508758]	</s> [-0.886115]	your [-11.528149]	overcharge [-9.989580]	</s> [-0.809810]	a [-11.638114]	overcharge [-9.855755]	</s> [-0.838929]	hundred [-11.018637]	misconduct [-9.290041]	</s> [-1.026576]	a [-11.651546]	misconduct [-9.489507]	</s> [-0.951166]	an [-11.211238]	misconduct [-9.518538]	</s> [-1.036715]	a [-11.610720]	spearhead [-11.086187]	</s> [-0.974867]	hundred [-10.972791]	spearhead [-10.651929]	</s> [-0.976389]	a [-11.507071]	follow [-9.993792]	</s> [-1.578202]	an [-11.330161]	follow [-9.636879]	</s> [-1.652946]	a [-11.499194]	vault [-9.897335]	</s> [-1.115410]	an [-11.276454]	vault [-10.165331]	</s> [-0.933470]	my [-10.929107]	vault [-9.789140]	</s> [-1.084998]	hundred [-11.152928]	listen [-10.057105]	</s> [-0.906440]	one [-11.194018]	listen [-10.571645]	</s> [-0.818424]	my [-10.975554]	overspecialize [-10.655151]	</s> [-0.868977]	the [-11.444146]	overspecialize [-10.354043]	</s> [-0.878431]	an [-11.370357]	overspecialize [-10.763515]	</s> [-1.025396]	my [-10.888880]	slap [-11.118620]	</s> [-1.102612]	one [-10.800279]	slap [-10.761845]	</s> [-1.170276]	hundred [-10.929540]	slap [-11.326876]	</s> [-1.037705]	a [-11.678169]	slap [-11.144152]	</s> [-1.058343]	the [-11.243979]	sneeze [-11.261225]	</s> [-0.987106]	a [-11.698220]	sneeze [-11.270273]	</s> [-0.991935]	my [-11.039525]	sneeze [-11.488621]	</s> [-0.943328]	one [-10.918247]	sneeze [-11.287638]	</s> [-0.902059]	the [-11.014265]	bring [-10.223700]	</s> [-0.787288]	a [-11.697029]	bring [-10.947184]	</s> [-0.849813]	hundred [-11.129524]	run [-7.075537]	</s> [-0.741383]	an [-10.947614]	run [-6.595883]	</s> [-0.831235]	the [-11.467833]	run [-7.202321]	</s> [-0.709167]	one [-11.137173]	run [-6.379488]	</s> [-0.687760]	a [-11.666455]	run [-7.429688]	</s> [-0.831128]	one [-11.044111]	circumcise [-9.623287]	</s> [-1.012012]	my [-10.942677]	stand [-10.464386]	</s> [-1.057545]	your [-11.706621]	key [-10.006777]	</s> [-1.052088]	the [-11.265616]	introvert [-10.451053]	</s> [-1.403354]	hundred [-11.088741]	revitalise [-9.163335]	</s> [-0.870908]	one [-11.164874]	revitalise [-9.536270]	</s> [-0.828621]	an [-11.192114]	revitalise [-9.715283]	</s> [-0.834983]	a [-11.745056]	revitalise [-9.436373]	</s> [-0.886828]	the [-11.306738]	revitalise [-9.280865]	</s> [-0.794111]	a [-11.721334]	bump [-9.852442]	</s> [-1.065506]	a [-11.934977]	rediscover [-9.963391]	</s> [-0.992859]	one [-11.385568]	rediscover [-10.427879]	</s> [-1.027571]	an [-11.446045]	back [-6.848250]	</s> [-0.679115]	the [-11.139118]	back [-6.921137]	</s> [-0.685697]	the [-11.223764]	promenade [-10.722830]	</s> [-0.815063]	a [-11.570707]	promenade [-10.499626]	</s> [-0.782795]	an [-11.241486]	promenade [-10.752291]	</s> [-0.817781]	the [-11.369400]	misbehave [-9.934147]	</s> [-0.855541]	your [-11.568931]	misbehave [-9.849739]	</s> [-0.905409]	one [-10.988211]	fall [-9.585306]	</s> [-0.897289]	hundred [-11.223551]	fall [-9.193048]	</s> [-0.853020]	my [-10.998156]	fall [-9.808114]	</s> [-1.041547]	an [-10.802785]	replenish [-10.830015]	</s> [-1.020666]	the [-10.759219]	replenish [-10.308392]	</s> [-1.029797]	a [-11.182313]	carry [-10.254692]	</s> [-1.344531]	my [-10.672727]	crash [-9.786584]	</s> [-1.191699]	one [-10.888933]	crash [-10.281384]	</s> [-1.277587]	a [-11.291848]	crash [-9.708166]	</s> [-1.108040]	an [-11.017866]	crash [-9.659792]	</s> [-1.126256]	an [-11.341073]	provision [-10.314820]	</s> [-0.818108]	one [-11.199092]	provision [-9.838388]	</s> [-0.898813]	my [-10.925490]	provision [-10.451102]	</s> [-0.956591]	the [-11.149285]	provision [-10.420392]	</s> [-0.933696]	a [-11.403691]	provision [-10.674927]	</s> [-0.863723]	one [-10.978169]	track [-9.987701]	</s> [-1.109420]	my [-10.893538]	track [-10.387716]	</s> [-1.105780]	your [-11.220648]	splash [-11.338259]	</s> [-1.467224]	a [-11.377059]	splash [-10.788301]	</s> [-1.396483]	the [-10.939144]	intercede [-10.256859]	</s> [-0.980535]	one [-11.242506]	intercede [-10.487216]	</s> [-0.960698]	an [-11.045462]	mispronounce [-11.101620]	</s> [-1.081298]	one [-10.998236]	mispronounce [-10.122118]	</s> [-0.971140]	hundred [-10.766136]	mispronounce [-10.534038]	</s> [-1.125958]	hundred [-10.757474]	fertilize [-10.420374]	</s> [-0.878695]	a [-11.286554]	fertilize [-10.695591]	</s> [-0.861578]	a [-11.332665]	number [-11.141684]	</s> [-0.812008]	an [-11.065993]	number [-11.029439]	</s> [-0.780550]	hundred [-10.902843]	hydroplane [-10.363026]	</s> [-1.020595]	one [-11.149989]	hydroplane [-10.286969]	</s> [-0.901586]	the [-11.173090]	hydroplane [-10.268457]	</s> [-0.884451]	one [-11.153463]	disrespect [-9.711853]	</s> [-1.109089]	hundred [-11.248927]	disrespect [-10.339235]	</s> [-1.213161]	the [-10.987234]	disrespect [-9.909644]	</s> [-1.216680]	hundred [-10.900176]	overemphasize [-9.776257]	</s> [-0.863943]	an [-11.203912]	backtrack [-10.336838]	</s> [-1.334658]	a [-11.716028]	backtrack [-10.076447]	</s> [-1.478768]	my [-10.919757]	backtrack [-10.296089]	</s> [-1.403394]	a [-11.640306]	prescribe [-9.954574]	</s> [-0.805620]	an [-11.192632]	criticise [-10.190065]	</s> [-0.961494]	one [-11.066095]	criticise [-10.154179]	</s> [-0.974184]	my [-11.005416]	burlesque [-10.978642]	</s> [-0.743590]	your [-11.568078]	burlesque [-10.865917]	</s> [-0.781547]	the [-10.989806]	commercialize [-10.360742]	</s> [-1.118317]	your [-11.351817]	commercialize [-11.007411]	</s> [-1.266353]	hundred [-10.827665]	rustle [-10.218616]	</s> [-1.033340]	my [-10.845637]	rustle [-10.331642]	</s> [-0.930278]	the [-11.045746]	soft [-10.533791]	</s> [-1.377141]	your [-11.518041]	soft [-10.383961]	</s> [-1.147037]	your [-11.527404]	keep [-9.584870]	</s> [-0.719566]	a [-12.071502]	keep [-9.667764]	</s> [-0.682859]	hundred [-11.480293]	keep [-9.855272]	</s> [-0.684562]	my [-11.177935]	rubber [-10.335844]	</s> [-0.949436]	your [-11.636444]	rubber [-10.360713]	</s> [-0.954349]	the [-11.214207]	rubber [-10.662878]	</s> [-0.928216]	your [-11.311753]	glamorize [-9.141706]	</s> [-0.825903]	one [-11.306510]	glamorize [-9.880407]	</s> [-0.770288]	one [-11.081402]	close [-9.198351]	</s> [-1.074276]	hundred [-11.209208]	close [-9.960281]	</s> [-1.063077]	a [-11.735205]	close [-9.564302]	</s> [-0.961586]	the [-11.070017]	stanchion [-10.940084]	</s> [-1.479745]	hundred [-11.295811]	stanchion [-11.128395]	</s> [-1.608335]	the [-11.213024]	get [-9.427106]	</s> [-0.827756]	a [-11.591380]	blackguard [-10.172242]	</s> [-0.661370]	an [-10.989512]	blackguard [-10.497025]	</s> [-0.676499]	one [-11.330431]	blackguard [-10.055658]
2024-12-19 08:16:20 | INFO | fairseq_cli.eval_lm | Evaluated 708 tokens in 1.1s (661.31 tokens/s)
2024-12-19 08:16:20 | INFO | fairseq_cli.eval_lm | Loss (base 2): 10.6987, Perplexity: 1662.02
