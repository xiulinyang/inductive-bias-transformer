2024-12-19 06:33:27 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX
2024-12-19 06:33:30 | INFO | fairseq_cli.eval_lm | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/grammar41exp2/0-transformer/checkpoint_best.pt', 'post_process': None, 'quiet': True, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': True, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': None, 'task': {'_name': 'language_modeling', 'data': 'data-bin/grammar41exp2/correct_0-dataset', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2024-12-19 06:33:30 | INFO | fairseq.tasks.language_modeling | dictionary: 1136 types
2024-12-19 06:33:30 | INFO | fairseq_cli.eval_lm | loading model(s) from checkpoints/grammar41exp2/0-transformer/checkpoint_best.pt
/workspace/artificial-languages/fairseq/fairseq/checkpoint_utils.py:340: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(f, map_location=torch.device("cpu"))
2024-12-19 06:33:33 | INFO | fairseq_cli.eval_lm | num. model params: 10,038,784
2024-12-19 06:33:33 | INFO | fairseq.data.data_utils | loaded 236 examples from: data-bin/grammar41exp2/correct_0-dataset/test
2024-12-19 06:33:33 | INFO | fairseq_cli.eval_lm | data-bin/grammar41exp2/correct_0-dataset test 2 examples
2024-12-19 06:33:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-12-19 06:33:33 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2024-12-19 06:33:33 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2024-12-19 06:33:33 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1
/opt/conda/envs/art/lib/python3.9/site-packages/torch/nn/functional.py:5849: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
2024-12-19 06:33:35 | INFO | fairseq_cli.eval_lm | 1 </s> [-0.752744]	my [-14.206366]	pitchfork [-7.643249]	</s> [-0.757013]	the [-12.500536]	pitchfork [-8.890586]	</s> [-0.870675]	your [-12.872867]	pitchfork [-9.434734]	</s> [-0.997015]	a [-12.754955]	sneeze [-9.059730]	</s> [-0.985303]	my [-12.329492]	sneeze [-9.049304]	</s> [-0.963240]	an [-13.544254]	mountebank [-8.499749]	</s> [-0.998191]	your [-12.874771]	mountebank [-9.904120]	</s> [-1.010906]	one [-12.186100]	key [-10.251289]	</s> [-0.819209]	the [-11.986331]	close [-9.281199]	</s> [-1.186316]	your [-12.530606]	close [-9.348215]	</s> [-1.250961]	a [-12.679442]	number [-9.238155]	</s> [-0.895576]	your [-12.305583]	number [-9.427074]	</s> [-0.844518]	hundred [-12.450694]	rediscover [-9.869985]	</s> [-0.871558]	an [-13.263407]	rediscover [-9.039089]	</s> [-0.921610]	your [-12.431950]	rediscover [-9.737474]	</s> [-0.917397]	hundred [-12.770721]	outbalance [-9.854395]	</s> [-1.042680]	one [-12.317363]	outbalance [-10.083374]	</s> [-1.130705]	a [-12.380351]	circumcise [-10.217727]	</s> [-0.895591]	your [-12.436158]	power [-9.659641]	</s> [-1.010108]	hundred [-12.779322]	decentralize [-10.388115]	</s> [-0.991076]	your [-12.451144]	decentralize [-10.069203]	</s> [-1.078582]	my [-11.975908]	decentralize [-9.276962]	</s> [-1.000978]	one [-12.207002]	get [-9.821917]	</s> [-1.051100]	hundred [-12.901561]	get [-9.465732]	</s> [-1.075217]	a [-12.172471]	wheelbarrow [-10.893632]	</s> [-0.903817]	my [-12.174509]	wheelbarrow [-10.031466]	</s> [-0.997733]	an [-13.459101]	booby [-9.356117]	</s> [-1.036382]	my [-12.086333]	booby [-9.603582]	</s> [-1.134869]	hundred [-12.674761]	booby [-9.716061]	</s> [-1.150752]	your [-12.439217]	interchange [-9.811422]	</s> [-0.970187]	my [-12.357320]	interchange [-9.560397]	</s> [-0.930278]	the [-11.906862]	keep [-9.652974]	</s> [-0.988586]	an [-13.059551]	keep [-8.977580]	</s> [-1.025901]	my [-12.088484]	keep [-9.630646]	</s> [-1.012352]	an [-13.188192]	hero [-9.447618]	</s> [-1.202219]	one [-12.251638]	hero [-9.490942]	</s> [-1.222003]	one [-12.329123]	constrict [-10.467196]	</s> [-1.020318]	the [-12.234795]	constrict [-11.244492]	</s> [-0.976422]	the [-11.945506]	weasel [-10.351504]	</s> [-1.121488]	one [-12.339609]	intermarry [-10.025229]	</s> [-0.997186]	an [-13.003986]	intermarry [-9.421805]	</s> [-0.938752]	hundred [-12.659965]	intermarry [-9.638347]	</s> [-1.010191]	a [-12.062666]	intermarry [-10.211284]	</s> [-0.915810]	your [-12.766260]	prejudice [-9.172489]	</s> [-1.024851]	hundred [-13.034760]	secularize [-9.099310]	</s> [-1.068714]	an [-13.292272]	rustle [-10.158436]	</s> [-0.997898]	your [-12.531533]	rustle [-10.687972]	</s> [-1.043859]	my [-12.229333]	rustle [-10.195004]	</s> [-1.074288]	hundred [-12.996128]	slap [-9.695798]	</s> [-0.967875]	one [-12.165209]	slap [-9.932946]	</s> [-1.005123]	your [-12.554412]	break [-10.081665]	</s> [-0.947277]	one [-12.545690]	break [-9.774336]	</s> [-0.914684]	my [-12.107793]	ginger [-9.561866]	</s> [-1.188468]	the [-11.908679]	ginger [-10.529142]	</s> [-1.250321]	a [-12.231173]	ginger [-10.498141]	</s> [-1.340385]	hundred [-12.892215]	soft [-9.214266]	</s> [-0.899278]	your [-12.333152]	jeopardize [-9.639983]	</s> [-0.997940]	my [-11.819952]	jeopardize [-8.647758]	</s> [-0.948005]	my [-11.981980]	influence [-10.218808]	</s> [-1.085273]	an [-13.007581]	influence [-9.843979]	</s> [-1.086956]	one [-12.304284]	intercede [-9.821588]	</s> [-1.189887]	an [-12.886106]	intercede [-8.610581]	</s> [-1.183089]	the [-11.897999]	intercede [-10.188498]	</s> [-1.198624]	hundred [-12.685453]	intercede [-9.316751]	</s> [-1.204665]	your [-12.432075]	intercede [-9.268553]	</s> [-1.184009]
2024-12-19 06:33:35 | INFO | fairseq_cli.eval_lm | 0 one [-12.599641]	come [-8.600909]	</s> [-0.876189]	an [-12.670422]	come [-7.256138]	</s> [-0.866430]	an [-12.419661]	dehumanize [-8.944160]	</s> [-0.858014]	the [-11.875858]	dehumanize [-10.463121]	</s> [-1.019922]	hundred [-12.677917]	dehumanize [-10.225471]	</s> [-0.984313]	a [-12.334119]	dehumanize [-9.868657]	</s> [-0.906675]	one [-12.022393]	dehumanize [-10.055145]	</s> [-0.930231]	hundred [-12.682292]	vault [-11.041548]	</s> [-0.944820]	the [-11.642553]	square [-9.138966]	</s> [-1.388816]	hundred [-12.394800]	privilege [-8.996033]	</s> [-0.929575]	one [-11.909401]	privilege [-9.188140]	</s> [-0.939207]	your [-11.868979]	racketeer [-10.360710]	</s> [-0.963694]	hundred [-12.212147]	racketeer [-10.356932]	</s> [-0.997511]	my [-11.616995]	racketeer [-10.466627]	</s> [-1.141010]	the [-11.537392]	reemphasize [-9.145962]	</s> [-1.330909]	an [-12.767796]	rich [-5.900301]	</s> [-2.370848]	a [-12.290874]	rich [-5.768650]	</s> [-2.350910]	a [-12.237409]	pressurize [-10.356327]	</s> [-1.207417]	your [-12.017577]	pressurize [-10.193564]	</s> [-1.162139]	the [-12.017961]	blackguard [-9.666340]	</s> [-1.004989]	my [-11.767807]	blackguard [-8.502039]	</s> [-1.092235]	your [-12.102685]	give [-6.279584]	</s> [-0.899838]	the [-11.755077]	give [-6.190181]	</s> [-0.861031]	an [-13.208744]	give [-6.245016]	</s> [-0.961923]	your [-11.918960]	prescribe [-9.370866]	</s> [-0.923655]	my [-11.676764]	prescribe [-8.540745]	</s> [-0.986446]	a [-12.246748]	prescribe [-9.471077]	</s> [-0.993598]	hundred [-12.522771]	prescribe [-9.372388]	</s> [-1.023025]	my [-11.748367]	backtrack [-10.130076]	</s> [-1.180654]	a [-11.982536]	backtrack [-10.922141]	</s> [-1.118984]	my [-11.938395]	fall [-9.240623]	</s> [-1.219895]	a [-12.125521]	fall [-9.476217]	</s> [-1.193501]	an [-12.667511]	fall [-9.484802]	</s> [-1.229226]	one [-12.112453]	shove [-9.709785]	</s> [-1.041110]	an [-12.894168]	shove [-9.537786]	</s> [-0.998621]	an [-12.858463]	sweet [-6.481142]	</s> [-2.538208]	an [-12.845810]	sound [-9.485905]	</s> [-1.439901]	a [-12.239370]	sound [-10.965083]	</s> [-1.410864]	one [-12.237218]	sound [-10.179353]	</s> [-1.323966]	a [-11.904070]	hydroplane [-10.104819]	</s> [-0.968464]	the [-11.850791]	hydroplane [-10.409647]	</s> [-0.928658]	one [-11.897844]	hydroplane [-10.554275]	</s> [-0.969520]	an [-12.849244]	crosscheck [-8.883980]	</s> [-1.123659]	a [-12.159025]	crosscheck [-9.983595]	</s> [-1.021225]	my [-12.100875]	back [-4.644739]	</s> [-1.383695]	a [-12.094148]	back [-5.398345]	</s> [-1.180504]	your [-12.409188]	commercialize [-10.189083]	</s> [-0.913482]	a [-12.137980]	commercialize [-10.637740]	</s> [-1.000089]	my [-12.171755]	promenade [-9.509602]	</s> [-0.929699]	your [-12.051114]	promenade [-9.894732]	</s> [-0.928068]	one [-12.429438]	promenade [-9.840483]	</s> [-0.917742]	a [-12.240727]	promenade [-9.574885]	</s> [-0.865733]	one [-12.531662]	blind [-4.844481]	</s> [-2.371720]	hundred [-12.832346]	headquarter [-9.006670]	</s> [-0.918835]	one [-12.197204]	headquarter [-9.198990]	</s> [-0.947528]	the [-12.150019]	follow [-9.294621]	</s> [-1.352815]	your [-12.253247]	bump [-9.914812]	</s> [-0.795173]	an [-12.865172]	bump [-8.439237]	</s> [-0.759947]	hundred [-12.953739]	run [-5.130394]	</s> [-1.327260]	your [-12.330078]	run [-5.732143]	</s> [-1.342330]	the [-12.051995]	run [-5.843006]	</s> [-1.287940]	the [-11.922944]	commingle [-9.590189]	</s> [-1.528563]	my [-11.756408]	commingle [-9.595025]	</s> [-1.632825]	an [-12.922729]	choke [-10.082066]	</s> [-1.042887]	the [-12.090139]	choke [-9.966477]	</s> [-1.018673]	one [-12.666936]	choke [-10.094240]	</s> [-0.986951]	hundred [-12.919990]	choke [-10.566145]	</s> [-1.020547]	your [-12.267348]	medium [-5.033935]	</s> [-2.514625]	my [-11.797400]	medium [-5.234536]	</s> [-2.505940]	a [-12.240927]	medium [-4.796976]	</s> [-2.403394]	one [-12.571287]	intellectualize [-9.654933]	</s> [-1.174354]	a [-12.023734]	intellectualize [-9.769506]	</s> [-1.173791]	hundred [-12.572368]	folk [-10.204287]	</s> [-1.010672]	the [-11.869446]	folk [-10.118102]	</s> [-1.067326]	one [-12.396631]	folk [-9.480769]	</s> [-0.956274]	my [-11.975547]	folk [-9.470198]	</s> [-0.906046]	my [-11.776752]	dillydally [-8.794014]	</s> [-1.007507]	your [-12.491324]	dillydally [-10.046414]	</s> [-1.037395]	the [-11.722080]	crash [-10.352213]	</s> [-1.101300]	your [-12.113735]	crash [-10.050309]	</s> [-1.068613]	my [-11.937576]	crash [-9.179037]	</s> [-1.025780]	hundred [-12.823736]	crash [-10.127102]	</s> [-1.167220]	an [-12.916163]	listen [-10.299072]	</s> [-1.320212]	hundred [-13.032475]	listen [-10.094656]	</s> [-1.195169]	an [-13.084153]	provision [-8.789948]	</s> [-1.040773]	a [-11.858644]	provision [-9.867611]	</s> [-1.078897]	hundred [-12.664076]	provision [-10.600947]	</s> [-1.112369]	hundred [-12.694873]	criticise [-10.358089]	</s> [-1.116225]	my [-11.847579]	criticise [-9.251955]	</s> [-1.107428]	my [-11.932838]	track [-9.601917]	</s> [-1.117855]	a [-12.049726]	track [-10.236973]	</s> [-1.207736]	an [-13.207203]	track [-9.384743]	</s> [-1.186967]	an [-13.122990]	splash [-9.040138]	</s> [-1.065113]	your [-12.639250]	stanchion [-10.011656]	</s> [-1.502038]	an [-13.170732]	stanchion [-9.372244]	</s> [-1.283283]	one [-12.361671]	stanchion [-10.091915]	</s> [-1.408425]	my [-12.113764]	stanchion [-10.054702]	</s> [-1.379593]	my [-12.052042]	still [-9.771442]	</s> [-1.076007]	your [-12.678720]	still [-10.169859]	</s> [-1.129677]	my [-11.796341]	bucket [-9.874425]	</s> [-0.904660]	your [-12.400485]	carry [-10.058964]	</s> [-1.020667]	an [-12.764139]	carry [-9.512499]	</s> [-1.030666]	the [-11.833976]	gallivant [-9.857815]	</s> [-0.954298]	one [-12.339287]	gallivant [-10.120090]	</s> [-0.956718]	one [-12.335525]	interfere [-10.283773]	</s> [-1.346141]	the [-11.807709]	interfere [-10.578142]	</s> [-1.232949]	your [-12.524447]	interfere [-10.369162]	</s> [-1.296927]	your [-12.657954]	inconvenience [-10.370787]	</s> [-0.910686]	a [-11.827142]	replenish [-9.457035]	</s> [-1.309274]	an [-12.514889]	replenish [-8.475092]	</s> [-1.305110]	one [-12.072184]	replenish [-10.080638]	</s> [-1.328449]	my [-11.693666]	replenish [-8.854008]	</s> [-1.276457]	hundred [-12.477990]	disrespect [-10.201715]	</s> [-1.172202]	your [-12.308339]	disrespect [-9.729660]	</s> [-1.190292]	a [-12.137363]	disrespect [-9.542978]	</s> [-1.146764]	an [-13.048493]	disrespect [-9.848624]	</s> [-1.080854]	an [-12.908210]	rubber [-8.941259]	</s> [-1.159189]	your [-12.190727]	rubber [-9.903784]	</s> [-1.262047]	your [-12.149214]	undervalue [-10.376260]	</s> [-1.089790]	a [-11.666118]	undervalue [-10.964224]	</s> [-1.121868]	your [-11.854202]	apologize [-10.058937]	</s> [-1.015293]	an [-12.548004]	apologize [-9.241972]	</s> [-0.977624]	your [-11.797001]	mispronounce [-11.291841]	</s> [-1.342733]	an [-12.546078]	blaspheme [-8.195130]	</s> [-1.043755]	the [-11.648028]	spearhead [-10.083213]	</s> [-0.874114]	a [-11.992517]	overcharge [-10.215114]	</s> [-1.084586]	my [-11.909802]	demagnetise [-8.446239]	</s> [-1.263172]	the [-11.860605]	demagnetise [-9.371472]	</s> [-1.228140]	an [-12.672688]	demagnetise [-9.024507]	</s> [-1.270447]	the [-11.576241]	distemper [-10.243500]	</s> [-0.947681]	your [-12.387143]	distemper [-9.910151]	</s> [-0.913016]	my [-11.793070]	distemper [-9.754655]	</s> [-1.006365]	a [-11.878385]	distemper [-9.514301]	</s> [-1.003069]	my [-11.783765]	overemphasize [-10.025135]	</s> [-1.080698]	one [-12.298514]	overemphasize [-9.298989]	</s> [-1.029263]	an [-13.035346]	overemphasize [-8.780588]	</s> [-0.981280]	the [-11.835305]	overemphasize [-9.794798]	</s> [-0.982710]	your [-12.509266]	brutalize [-9.803486]	</s> [-1.302857]	the [-12.172957]	brutalize [-10.184814]	</s> [-1.331347]	the [-11.995774]	glamorize [-9.751232]	</s> [-0.965232]	one [-12.355467]	glamorize [-9.086421]	</s> [-0.977086]	a [-12.257699]	glamorize [-9.393618]	</s> [-0.981866]	my [-11.907747]	glamorize [-9.355581]	</s> [-1.023722]	my [-11.863682]	introvert [-10.402367]	</s> [-1.031007]	a [-12.150616]	introvert [-10.513810]	</s> [-1.049512]	an [-13.014986]	introvert [-10.143946]	</s> [-1.065614]	one [-12.065144]	revitalise [-9.360243]	</s> [-1.263699]	hundred [-12.346899]	revitalise [-8.977297]	</s> [-1.217834]	the [-11.666445]	revitalise [-9.139127]	</s> [-1.269862]	an [-12.705415]	revitalise [-8.430601]	</s> [-1.240319]	a [-11.930577]	bring [-9.867576]	</s> [-0.927195]	my [-11.878891]	bring [-9.461185]	</s> [-0.994629]	one [-12.344435]	overspecialize [-9.722321]	</s> [-0.812040]	a [-12.037930]	misbehave [-10.722806]	</s> [-1.064834]	my [-11.939324]	disembowel [-10.046549]	</s> [-1.129805]	the [-12.053520]	disembowel [-10.246338]	</s> [-1.104628]	your [-12.309606]	smooth [-10.504188]	</s> [-1.006231]	a [-12.021017]	smooth [-9.949314]	</s> [-1.027477]	the [-11.669496]	smooth [-10.241728]	</s> [-1.026455]	the [-11.904179]	misconduct [-9.803835]	</s> [-1.000003]	my [-11.472380]	misconduct [-9.249756]	</s> [-1.016091]	an [-12.912731]	misconduct [-8.573775]	</s> [-0.997752]	your [-12.042606]	misconduct [-9.248149]	</s> [-0.961017]	hundred [-12.853039]	predetermine [-10.555003]	</s> [-1.086531]	a [-12.332445]	predetermine [-10.609541]	</s> [-0.992344]	an [-12.966973]	predetermine [-10.267625]	</s> [-1.003299]	hundred [-12.742739]	burlesque [-9.436522]	</s> [-1.200180]	an [-13.018653]	burlesque [-8.659538]	</s> [-1.130642]	a [-12.210240]	dress [-11.010222]	</s> [-1.206381]	your [-12.249561]	dress [-10.882034]	</s> [-1.150898]	my [-11.913795]	dress [-9.565075]
2024-12-19 06:33:35 | INFO | fairseq_cli.eval_lm | Evaluated 708 tokens in 0.9s (745.43 tokens/s)
2024-12-19 06:33:35 | INFO | fairseq_cli.eval_lm | Loss (base 2): 11.0199, Perplexity: 2076.41
