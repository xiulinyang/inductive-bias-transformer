2024-12-19 07:41:36 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX
2024-12-19 07:41:39 | INFO | fairseq_cli.eval_lm | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/grammar41exp2/2-transformer/checkpoint_best.pt', 'post_process': None, 'quiet': True, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': True, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': None, 'task': {'_name': 'language_modeling', 'data': 'data-bin/grammar41exp2/correct_2-dataset', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2024-12-19 07:41:39 | INFO | fairseq.tasks.language_modeling | dictionary: 1136 types
2024-12-19 07:41:39 | INFO | fairseq_cli.eval_lm | loading model(s) from checkpoints/grammar41exp2/2-transformer/checkpoint_best.pt
/workspace/artificial-languages/fairseq/fairseq/checkpoint_utils.py:340: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(f, map_location=torch.device("cpu"))
2024-12-19 07:41:42 | INFO | fairseq_cli.eval_lm | num. model params: 10,038,784
2024-12-19 07:41:42 | INFO | fairseq.data.data_utils | loaded 222 examples from: data-bin/grammar41exp2/correct_2-dataset/test
2024-12-19 07:41:42 | INFO | fairseq_cli.eval_lm | data-bin/grammar41exp2/correct_2-dataset test 2 examples
2024-12-19 07:41:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-12-19 07:41:42 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2024-12-19 07:41:42 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2024-12-19 07:41:42 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1
/opt/conda/envs/art/lib/python3.9/site-packages/torch/nn/functional.py:5849: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
2024-12-19 07:41:44 | INFO | fairseq_cli.eval_lm | 1 </s> [-1.493774]	hundred [-15.593929]	misbehave [-7.204427]	</s> [-0.712401]	the [-13.872417]	run [-5.913922]	</s> [-0.493089]	a [-14.252903]	run [-5.819456]	</s> [-0.546787]	hundred [-15.110225]	run [-5.197352]	</s> [-0.543247]	your [-13.976965]	run [-6.435639]	</s> [-0.549495]	an [-13.649362]	blind [-5.451464]	</s> [-1.635947]	the [-13.582547]	blind [-5.927959]	</s> [-1.583317]	the [-13.504307]	intermarry [-8.490826]	</s> [-0.624560]	an [-13.202148]	intermarry [-6.887639]	</s> [-0.548234]	my [-13.859538]	criticise [-7.969440]	</s> [-0.335327]	an [-13.118245]	criticise [-7.794278]	</s> [-0.267204]	one [-13.791490]	criticise [-9.578865]	</s> [-0.279937]	a [-13.445518]	criticise [-9.420580]	</s> [-0.288566]	one [-13.914558]	stanchion [-9.036747]	</s> [-0.599276]	hundred [-14.342460]	stanchion [-8.559069]	</s> [-0.540676]	hundred [-14.463409]	disembowel [-8.213783]	</s> [-1.206159]	my [-13.551317]	disembowel [-9.137876]	</s> [-1.115391]	a [-13.177647]	get [-8.552951]	</s> [-0.533092]	hundred [-14.452575]	get [-7.499172]	</s> [-0.501871]	my [-13.935858]	headquarter [-8.695293]	</s> [-0.647212]	one [-14.201124]	headquarter [-9.961626]	</s> [-0.743937]	an [-13.120098]	headquarter [-9.203101]	</s> [-0.698594]	an [-13.205697]	outbalance [-7.383083]	</s> [-0.574340]	the [-13.487814]	brutalize [-8.289233]	</s> [-0.608315]	one [-13.915033]	brutalize [-9.340946]	</s> [-0.637827]	hundred [-14.272387]	still [-8.308895]	</s> [-0.669473]	an [-12.959517]	still [-7.903196]	</s> [-0.674125]	your [-13.004688]	still [-10.027895]	</s> [-0.563013]	my [-13.269180]	crash [-8.679759]	</s> [-0.753487]	the [-12.924358]	crash [-7.769770]	</s> [-0.694978]	a [-13.268180]	interchange [-9.234270]	</s> [-0.544996]	one [-13.522406]	interchange [-9.022181]	</s> [-0.625620]	hundred [-13.961060]	overcharge [-8.032525]	</s> [-0.756838]	one [-13.657082]	overcharge [-9.496864]	</s> [-0.766226]	the [-12.880420]	come [-7.950197]	</s> [-0.730673]	a [-13.112917]	listen [-9.273086]	</s> [-0.498126]	your [-13.064253]	weasel [-10.414666]	</s> [-0.628119]	an [-12.894323]	weasel [-8.821265]	</s> [-0.564935]	my [-13.398284]	bring [-8.151479]	</s> [-0.499575]	hundred [-14.066489]	bring [-8.160738]	</s> [-0.574754]	a [-13.105914]	backtrack [-10.072851]	</s> [-0.859393]	your [-12.879975]	backtrack [-10.662060]	</s> [-0.809395]	one [-13.570528]	backtrack [-10.567974]	</s> [-0.831039]	hundred [-13.916817]	carry [-9.532421]	</s> [-0.751280]	one [-13.537491]	carry [-10.547219]	</s> [-0.921839]	your [-13.080602]	decentralize [-9.665520]	</s> [-0.433761]	my [-13.441758]	circumcise [-8.360416]	</s> [-0.634661]	one [-13.815544]	circumcise [-9.067462]	</s> [-0.617421]	one [-13.705646]	inconvenience [-10.162060]	</s> [-0.487686]	hundred [-14.048775]	inconvenience [-8.842169]	</s> [-0.488820]	my [-13.392225]	inconvenience [-8.075266]	</s> [-0.499743]
2024-12-19 07:41:45 | INFO | fairseq_cli.eval_lm | 0 an [-14.338367]	prescribe [-6.938434]	</s> [-0.421166]	one [-13.061315]	prescribe [-8.164733]	</s> [-0.475225]	one [-13.477489]	promenade [-8.493650]	</s> [-1.054373]	hundred [-14.182284]	promenade [-8.018839]	</s> [-1.113288]	your [-13.413557]	promenade [-8.975998]	</s> [-1.007040]	the [-13.006888]	promenade [-8.275307]	</s> [-0.958366]	your [-13.291635]	pitchfork [-10.021498]	</s> [-0.682710]	hundred [-14.268145]	pitchfork [-9.012941]	</s> [-0.753639]	hundred [-14.084938]	slaughter [-7.848296]	</s> [-0.824403]	the [-12.837606]	slaughter [-7.582468]	</s> [-0.853663]	your [-12.920209]	choke [-8.977057]	</s> [-0.757578]	a [-12.982645]	choke [-8.433823]	</s> [-0.666574]	an [-12.639545]	follow [-7.445314]	</s> [-1.116704]	an [-12.690207]	shove [-8.258131]	</s> [-0.659712]	hundred [-13.881907]	shove [-9.156552]	</s> [-0.650205]	my [-13.392300]	shove [-9.288336]	</s> [-0.675360]	a [-12.799631]	track [-9.477969]	</s> [-0.600097]	a [-12.580286]	influence [-9.282768]	</s> [-0.646954]	an [-12.838392]	apologize [-7.548297]	</s> [-0.751237]	one [-13.755650]	apologize [-8.205961]	</s> [-0.636808]	hundred [-14.200789]	apologize [-7.916962]	</s> [-0.668055]	the [-12.847970]	ginger [-8.787809]	</s> [-0.624905]	a [-12.916515]	fall [-8.460927]	</s> [-0.475738]	the [-13.263075]	fall [-6.514135]	</s> [-0.486998]	a [-13.224566]	booby [-9.031525]	</s> [-0.471494]	the [-12.908981]	booby [-8.366838]	</s> [-0.481711]	hundred [-14.233692]	booby [-9.123327]	</s> [-0.518625]	your [-13.054772]	booby [-10.436782]	</s> [-0.445631]	one [-13.588454]	privilege [-9.177502]	</s> [-0.576677]	your [-13.006216]	privilege [-9.785480]	</s> [-0.535869]	hundred [-14.245527]	privilege [-7.762972]	</s> [-0.563633]	my [-13.403650]	stand [-8.478286]	</s> [-0.953411]	my [-13.243912]	bump [-9.715370]	</s> [-0.766213]	one [-13.490944]	bump [-10.417694]	</s> [-0.762385]	hundred [-14.011239]	bump [-9.104154]	</s> [-0.723346]	your [-13.035901]	bump [-10.684836]	</s> [-0.664732]	one [-13.490812]	rediscover [-9.010205]	</s> [-0.824628]	my [-13.461142]	rediscover [-8.937238]	</s> [-0.807424]	one [-13.482324]	give [-6.197863]	</s> [-0.562348]	hundred [-13.924462]	give [-5.988783]	</s> [-0.621629]	the [-12.807408]	give [-5.486016]	</s> [-0.614910]	your [-12.953150]	sneeze [-9.610336]	</s> [-0.589531]	a [-12.966132]	sneeze [-9.794651]	</s> [-0.557798]	hundred [-13.864277]	sneeze [-8.461925]	</s> [-0.581899]	one [-13.629230]	sneeze [-9.344486]	</s> [-0.740290]	a [-13.208248]	blackguard [-9.705050]	</s> [-0.724589]	your [-13.247011]	blackguard [-10.427953]	</s> [-0.582331]	a [-13.206062]	wheelbarrow [-9.080689]	</s> [-0.557770]	hundred [-14.098454]	prejudice [-7.955763]	</s> [-0.552491]	hundred [-13.984878]	intercede [-8.216263]	</s> [-0.772222]	one [-13.810226]	intercede [-8.788239]	</s> [-0.788543]	one [-13.702081]	fertilize [-9.306824]	</s> [-0.819908]	your [-13.176080]	fertilize [-9.976328]	</s> [-0.747772]	my [-13.168210]	fertilize [-8.835188]	</s> [-0.828330]	one [-14.003471]	hydroplane [-8.947016]	</s> [-0.842673]	your [-13.461813]	hydroplane [-9.427593]	</s> [-0.841541]	hundred [-14.124510]	hydroplane [-8.773328]	</s> [-0.832306]	the [-13.072371]	revitalise [-7.670108]	</s> [-0.504272]	your [-13.546295]	rustle [-10.285086]	</s> [-0.828213]	a [-13.502716]	rustle [-9.061493]	</s> [-0.946967]	my [-13.317832]	burlesque [-8.790180]	</s> [-0.586630]	a [-13.632551]	burlesque [-9.066206]	</s> [-0.616166]	the [-13.019846]	burlesque [-9.251245]	</s> [-0.583318]	an [-12.778643]	dillydally [-6.914382]	</s> [-0.535507]	hundred [-14.167013]	dillydally [-6.853739]	</s> [-0.514076]	a [-13.460206]	dillydally [-8.018546]	</s> [-0.518563]	the [-13.028424]	introvert [-7.671453]	</s> [-0.601931]	one [-14.017095]	introvert [-8.843453]	</s> [-0.618981]	an [-13.077394]	introvert [-6.897418]	</s> [-0.588964]	one [-14.007648]	bucket [-8.251493]	</s> [-0.561874]	your [-13.434708]	dress [-10.195975]	</s> [-1.047841]	an [-12.793572]	folk [-8.409260]	</s> [-0.609190]	the [-12.709085]	folk [-9.525700]	</s> [-0.637862]	one [-13.443327]	provision [-9.327782]	</s> [-1.068855]	one [-13.790291]	glamorize [-8.643164]	</s> [-1.174071]	a [-13.273291]	glamorize [-9.158316]	</s> [-0.956376]	your [-13.357533]	glamorize [-9.110466]	</s> [-0.885819]	one [-13.936434]	rich [-5.562313]	</s> [-1.802096]	one [-13.765922]	undervalue [-9.638441]	</s> [-0.857627]	your [-13.269680]	undervalue [-10.010000]	</s> [-0.875447]	an [-12.839279]	square [-8.247074]	</s> [-0.724531]	your [-13.119167]	square [-9.747715]	</s> [-0.833007]	one [-13.350787]	square [-9.290481]	</s> [-0.884176]	my [-13.056201]	power [-9.274786]	</s> [-0.580127]	one [-13.610609]	pressurize [-9.684361]	</s> [-0.870394]	your [-13.126116]	pressurize [-10.324517]	</s> [-0.974371]	your [-13.241483]	medium [-5.878517]	</s> [-2.849342]	my [-13.301560]	hero [-8.542370]	</s> [-0.793530]	hundred [-14.134048]	hero [-7.992271]	</s> [-0.749013]	my [-13.288976]	racketeer [-9.791414]	</s> [-0.888101]	an [-13.145066]	racketeer [-8.274941]	</s> [-0.917659]	a [-13.105440]	racketeer [-9.870089]	</s> [-0.968171]	my [-13.095269]	number [-9.335601]	</s> [-0.921735]	your [-13.477852]	number [-10.213432]	</s> [-1.042928]	hundred [-14.088828]	dehumanize [-9.388809]	</s> [-0.650421]	an [-12.806753]	dehumanize [-8.797999]	</s> [-0.723057]	a [-12.906525]	dehumanize [-10.007523]	</s> [-0.795631]	my [-13.335588]	dehumanize [-8.467960]	</s> [-0.631759]	my [-13.492052]	intellectualize [-9.762396]	</s> [-0.640157]	a [-13.105762]	intellectualize [-10.244252]	</s> [-0.819919]	my [-12.777523]	blaspheme [-9.233870]	</s> [-0.763336]	an [-12.475522]	blaspheme [-9.427960]	</s> [-0.763439]	one [-13.380647]	blaspheme [-10.038713]	</s> [-0.728618]	an [-12.760419]	key [-8.056675]	</s> [-0.637809]	an [-12.746457]	mountebank [-8.474341]	</s> [-0.922118]	one [-13.645255]	mountebank [-9.562561]	</s> [-0.909093]	one [-13.706772]	smooth [-9.334543]	</s> [-0.794568]	the [-12.710448]	smooth [-9.424541]	</s> [-0.827231]	one [-13.387288]	spearhead [-9.767517]	</s> [-0.561161]	a [-12.830950]	spearhead [-9.427759]	</s> [-0.597762]	an [-12.756702]	spearhead [-9.136555]	</s> [-0.576151]	hundred [-13.390662]	spearhead [-9.651581]	</s> [-0.595067]	one [-13.125522]	mispronounce [-9.357993]	</s> [-0.649582]	one [-13.435985]	demagnetise [-9.326493]	</s> [-0.748749]	your [-13.033211]	demagnetise [-9.338955]	</s> [-0.826082]	a [-13.202843]	demagnetise [-8.931486]	</s> [-0.837942]	my [-13.256826]	demagnetise [-8.468779]	</s> [-0.847485]	your [-12.953651]	break [-9.854876]	</s> [-0.813849]	one [-13.295185]	break [-9.604955]	</s> [-0.766962]	a [-12.786764]	break [-9.255394]	</s> [-0.767047]	an [-12.877390]	break [-7.555263]	</s> [-0.948662]	an [-12.504240]	buttonhole [-8.931086]	</s> [-1.029858]	a [-12.591871]	buttonhole [-9.031601]	</s> [-1.049637]	the [-12.355504]	buttonhole [-8.308353]	</s> [-0.881949]	hundred [-13.511310]	constrict [-8.347061]	</s> [-0.679985]	a [-12.634775]	constrict [-9.112485]	</s> [-0.656542]	one [-13.324638]	reemphasize [-10.767288]	</s> [-0.839125]	one [-13.666743]	gallivant [-9.779746]	</s> [-0.684782]	your [-13.049733]	replenish [-9.973862]	</s> [-0.703956]	a [-13.024497]	replenish [-9.946838]	</s> [-0.735163]	hundred [-13.671085]	replenish [-9.254732]	</s> [-0.687817]	your [-12.869300]	crosscheck [-10.118286]	</s> [-0.691263]	a [-12.854521]	crosscheck [-10.113981]	</s> [-0.650842]	a [-12.795790]	commingle [-9.052143]	</s> [-0.616823]	one [-13.619088]	commingle [-8.708982]	</s> [-0.569872]	hundred [-13.805676]	commingle [-7.869407]	</s> [-0.535362]	the [-12.810913]	secularize [-7.272158]	</s> [-0.515495]	hundred [-14.047399]	rubber [-7.907332]	</s> [-0.757487]	the [-12.843773]	rubber [-8.822672]	</s> [-0.704998]	an [-13.204425]	sweet [-7.106311]	</s> [-2.110131]	your [-13.423220]	overemphasize [-10.186653]	</s> [-0.758276]	hundred [-13.728197]	overemphasize [-8.831788]	</s> [-0.805758]	the [-12.723297]	commercialize [-9.198818]	</s> [-0.849253]	a [-13.154410]	commercialize [-9.043612]	</s> [-0.770031]	my [-13.270948]	commercialize [-8.707550]	</s> [-0.744385]	my [-13.185947]	splash [-9.418916]	</s> [-0.539442]	an [-12.624019]	splash [-8.920710]	</s> [-0.506160]	a [-13.254227]	splash [-8.913308]	</s> [-0.546924]	hundred [-13.859713]	soft [-8.088415]	</s> [-0.722913]	one [-13.583673]	soft [-9.420475]	</s> [-0.708963]	your [-12.712334]	soft [-9.771585]	</s> [-0.719401]	an [-12.653486]	back [-5.414361]	</s> [-1.239616]	one [-13.366930]	back [-5.659315]	</s> [-1.210622]	my [-13.000755]	back [-5.201532]	</s> [-1.058891]	a [-13.247982]	back [-5.431571]	</s> [-1.066142]	an [-13.010883]	distemper [-7.934536]	</s> [-0.725356]	one [-13.622656]	distemper [-9.765821]	</s> [-0.789448]	the [-12.549753]	misconduct [-9.578583]	</s> [-0.652610]	a [-13.581676]	misconduct [-9.849874]	</s> [-0.604499]	my [-13.400174]	misconduct [-9.243958]	</s> [-0.610177]	an [-12.994145]	misconduct [-8.649137]	</s> [-0.603350]	a [-13.450702]	keep [-8.818079]	</s> [-0.535571]	my [-13.339483]	keep [-8.069796]	</s> [-0.448682]	one [-13.778109]	keep [-8.940620]	</s> [-0.466035]	your [-13.122884]	keep [-9.472410]	</s> [-0.536482]	your [-13.085467]	sound [-9.411318]	</s> [-0.397181]	a [-13.339819]	sound [-9.050960]	</s> [-0.454735]	one [-13.613847]	sound [-9.457884]	</s> [-0.448833]	my [-13.483466]	sound [-8.327901]	</s> [-0.450955]	my [-13.452147]	misbehave [-8.150637]	</s> [-0.584535]	a [-13.460560]	misbehave [-8.701835]
2024-12-19 07:41:45 | INFO | fairseq_cli.eval_lm | Evaluated 666 tokens in 0.8s (797.18 tokens/s)
2024-12-19 07:41:45 | INFO | fairseq_cli.eval_lm | Loss (base 2): 10.9820, Perplexity: 2022.66
