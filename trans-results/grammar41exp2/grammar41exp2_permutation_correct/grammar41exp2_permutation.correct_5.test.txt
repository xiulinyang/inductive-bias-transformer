2024-12-19 09:06:05 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX
2024-12-19 09:06:07 | INFO | fairseq_cli.eval_lm | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/grammar41exp2_permutation/5-transformer/checkpoint_best.pt', 'post_process': None, 'quiet': True, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': True, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': None, 'task': {'_name': 'language_modeling', 'data': 'data-bin/grammar41exp2_permutation/correct_5-dataset', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2024-12-19 09:06:07 | INFO | fairseq.tasks.language_modeling | dictionary: 1136 types
2024-12-19 09:06:07 | INFO | fairseq_cli.eval_lm | loading model(s) from checkpoints/grammar41exp2_permutation/5-transformer/checkpoint_best.pt
/workspace/artificial-languages/fairseq/fairseq/checkpoint_utils.py:340: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(f, map_location=torch.device("cpu"))
2024-12-19 09:06:10 | INFO | fairseq_cli.eval_lm | num. model params: 10,038,784
2024-12-19 09:06:10 | INFO | fairseq.data.data_utils | loaded 207 examples from: data-bin/grammar41exp2_permutation/correct_5-dataset/test
2024-12-19 09:06:10 | INFO | fairseq_cli.eval_lm | data-bin/grammar41exp2_permutation/correct_5-dataset test 2 examples
2024-12-19 09:06:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-12-19 09:06:10 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2024-12-19 09:06:10 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2024-12-19 09:06:10 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1
/opt/conda/envs/art/lib/python3.9/site-packages/torch/nn/functional.py:5849: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
2024-12-19 09:06:11 | INFO | fairseq_cli.eval_lm | 1 </s> [-1.669385]	promenade [-20.200575]	my [-6.526544]	</s> [-2.444966]	power [-17.814552]	an [-8.773756]	</s> [-1.570334]	power [-18.167521]	my [-8.077503]	</s> [-1.624964]	constrict [-17.745285]	your [-7.812144]	</s> [-1.222841]	constrict [-17.899450]	the [-8.158815]	</s> [-1.854287]	rediscover [-19.296701]	one [-9.593377]	</s> [-1.740781]	rediscover [-19.183136]	a [-8.350432]	</s> [-1.385321]	rediscover [-19.008123]	hundred [-8.776049]	</s> [-1.863772]	blind [-14.118125]	your [-2.874865]	</s> [-1.490217]	blind [-14.213245]	the [-2.642884]	</s> [-1.647639]	predetermine [-17.084356]	one [-8.551022]	</s> [-1.639259]	misbehave [-18.510735]	my [-8.169844]	</s> [-1.641118]	misbehave [-18.434408]	hundred [-8.442522]	</s> [-1.568839]	fertilize [-18.363159]	an [-9.718093]	</s> [-1.506879]	fertilize [-18.312372]	your [-8.753718]	</s> [-0.986271]	key [-19.205355]	the [-8.419159]	</s> [-1.537158]	key [-19.175924]	my [-7.998640]	</s> [-1.847605]	key [-18.965258]	a [-8.109421]	</s> [-1.323705]	key [-18.904875]	one [-8.972318]	</s> [-1.354330]	commingle [-18.145277]	your [-8.030457]	</s> [-0.653122]	commingle [-18.231407]	an [-8.788491]	</s> [-1.526545]	commingle [-18.250578]	one [-8.320949]	</s> [-1.495514]	misconduct [-16.975515]	my [-8.618679]	</s> [-1.278001]	misconduct [-17.022165]	a [-8.571408]	</s> [-1.122094]	wheelbarrow [-17.920918]	a [-8.282959]	</s> [-0.944893]	wheelbarrow [-17.999186]	the [-8.802757]	</s> [-1.309450]	rustle [-18.540394]	your [-8.765019]	</s> [-0.682595]	rustle [-18.558310]	hundred [-9.040707]	</s> [-1.671029]	rustle [-18.414150]	one [-9.272112]	</s> [-1.454111]	provision [-17.539623]	one [-9.234785]	</s> [-1.376746]	circumcise [-17.991625]	your [-7.891660]	</s> [-0.964041]	circumcise [-17.860847]	my [-8.123053]	</s> [-1.510263]	circumcise [-17.685452]	hundred [-8.664704]	</s> [-1.652266]	interchange [-17.942923]	the [-8.193925]	</s> [-1.483028]	interchange [-17.893816]	hundred [-8.559571]	</s> [-1.518211]	intermarry [-18.800898]	one [-8.823845]	</s> [-1.494605]
2024-12-19 09:06:12 | INFO | fairseq_cli.eval_lm | 0 ginger [-14.124004]	your [-7.111820]	</s> [-1.371697]	ginger [-17.327583]	a [-8.198043]	</s> [-1.135652]	give [-16.097790]	a [-5.794516]	</s> [-0.989668]	give [-16.177675]	hundred [-5.187079]	</s> [-1.689357]	give [-16.177046]	the [-6.059706]	</s> [-1.508380]	demagnetise [-18.437958]	one [-8.487974]	</s> [-1.592570]	racketeer [-18.085190]	a [-9.238326]	</s> [-1.184535]	slap [-18.172791]	my [-8.038753]	</s> [-1.751556]	slap [-18.066469]	your [-8.328991]	</s> [-1.341591]	slap [-18.134068]	one [-8.282413]	</s> [-1.628356]	pitchfork [-18.310125]	my [-8.456463]	</s> [-1.689031]	pitchfork [-18.590950]	an [-9.445552]	</s> [-1.550846]	carry [-16.889479]	the [-8.063612]	</s> [-1.345436]	carry [-16.824104]	your [-8.062505]	</s> [-0.762752]	rich [-13.298328]	my [-2.247035]	</s> [-1.620636]	follow [-18.245481]	an [-9.174783]	</s> [-1.421980]	follow [-18.367596]	one [-8.584315]	</s> [-1.582827]	disembowel [-18.854845]	a [-8.187024]	</s> [-1.293522]	disembowel [-18.715969]	the [-8.502490]	</s> [-1.381038]	disembowel [-18.607128]	your [-8.772087]	</s> [-0.654790]	folk [-17.914034]	your [-7.992844]	</s> [-0.937938]	apologize [-17.897432]	hundred [-9.137669]	</s> [-1.570077]	apologize [-17.834934]	the [-8.828396]	</s> [-1.444035]	apologize [-17.793900]	your [-8.150787]	</s> [-0.952902]	brutalize [-18.372438]	a [-8.370031]	</s> [-0.900063]	brutalize [-18.334995]	one [-9.227640]	</s> [-1.349683]	brutalize [-18.498917]	the [-8.507873]	</s> [-1.206953]	brutalize [-18.578739]	an [-8.919245]	</s> [-1.424517]	hero [-18.291771]	an [-9.365943]	</s> [-1.491784]	hero [-18.195597]	a [-8.595396]	</s> [-1.057773]	slaughter [-17.802959]	an [-9.728903]	</s> [-1.332613]	smooth [-18.684544]	an [-9.483829]	</s> [-1.489521]	run [-16.222113]	my [-5.742074]	</s> [-1.590044]	run [-16.182262]	hundred [-5.855221]	</s> [-1.562811]	glamorize [-17.445187]	an [-9.107724]	</s> [-1.386069]	glamorize [-17.514252]	one [-8.906511]	</s> [-1.483099]	burlesque [-17.107710]	the [-8.507751]	</s> [-1.564371]	burlesque [-17.157368]	hundred [-8.819266]	</s> [-1.494795]	disrespect [-18.078382]	my [-8.009848]	</s> [-1.647104]	reemphasize [-17.634706]	a [-8.686794]	</s> [-1.134809]	listen [-18.586519]	my [-8.166862]	</s> [-1.624756]	booby [-17.391153]	one [-8.475204]	</s> [-1.387948]	booby [-17.448545]	a [-8.146617]	</s> [-0.964150]	booby [-17.614485]	my [-8.212149]	</s> [-1.581509]	jeopardize [-17.137543]	your [-8.509521]	</s> [-0.938557]	jeopardize [-17.183167]	my [-8.734831]	</s> [-1.381543]	overcharge [-17.181133]	the [-8.477754]	</s> [-1.286297]	overcharge [-17.109671]	hundred [-8.971481]	</s> [-1.472622]	rubber [-17.247282]	hundred [-8.550392]	</s> [-1.697659]	rubber [-17.221722]	an [-9.342641]	</s> [-1.673893]	undervalue [-17.124323]	one [-8.808358]	</s> [-1.603620]	undervalue [-17.134129]	a [-8.484326]	</s> [-1.063268]	inconvenience [-17.524807]	one [-9.454457]	</s> [-1.500485]	hydroplane [-16.682274]	your [-8.200745]	</s> [-0.891276]	hydroplane [-16.682899]	a [-8.510510]	</s> [-1.119841]	sound [-18.089989]	hundred [-8.728897]	</s> [-1.737355]	dillydally [-17.285898]	hundred [-8.963829]	</s> [-1.526109]	come [-16.887579]	a [-8.572557]	</s> [-0.936040]	outbalance [-17.929848]	one [-9.080266]	</s> [-1.503031]	outbalance [-17.930855]	hundred [-8.653954]	</s> [-1.469510]	crash [-16.425428]	your [-8.504376]	</s> [-0.995872]	crash [-16.502392]	hundred [-8.419765]	</s> [-1.461900]	crash [-16.518784]	my [-8.311563]	</s> [-1.416766]	stand [-18.321819]	one [-8.804996]	</s> [-1.661492]	stand [-18.435366]	a [-8.539951]	</s> [-1.287647]	dehumanize [-19.393940]	my [-8.154456]	</s> [-1.792352]	dehumanize [-19.190163]	your [-8.028992]	</s> [-1.095498]	dehumanize [-18.998810]	hundred [-8.223860]	</s> [-1.437304]	sneeze [-18.482910]	my [-8.915483]	</s> [-1.547722]	sneeze [-18.363958]	the [-8.923181]	</s> [-1.343541]	back [-15.232765]	an [-4.378160]	</s> [-1.386583]	back [-15.178529]	a [-4.833417]	</s> [-1.100343]	back [-15.247257]	one [-4.464554]	</s> [-1.458686]	spearhead [-17.745226]	an [-9.831272]	</s> [-1.440274]	spearhead [-17.867445]	a [-8.730305]	</s> [-1.066385]	spearhead [-17.858990]	my [-8.715503]	</s> [-1.478802]	intercede [-17.655527]	my [-8.755050]	</s> [-1.341715]	intercede [-17.703726]	a [-8.889091]	</s> [-0.735875]	intercede [-17.583254]	your [-8.329987]	</s> [-0.846749]	crosscheck [-17.210562]	the [-8.137877]	</s> [-1.409102]	crosscheck [-17.192190]	your [-8.394271]	</s> [-0.824186]	crosscheck [-17.195734]	an [-9.554027]	</s> [-1.294847]	prescribe [-17.763268]	one [-8.784993]	</s> [-1.253891]	introvert [-17.426857]	the [-8.789613]	</s> [-1.170755]	mountebank [-18.050085]	your [-8.197947]	</s> [-0.965181]	mountebank [-17.930513]	an [-9.577427]	</s> [-1.410514]	close [-17.823322]	the [-8.925610]	</s> [-1.335513]	close [-17.711372]	an [-9.443932]	</s> [-1.097293]	backtrack [-16.824154]	the [-9.029159]	</s> [-1.195798]	backtrack [-16.981440]	my [-8.831489]	</s> [-1.478641]	blaspheme [-18.319368]	the [-8.705153]	</s> [-1.333266]	interfere [-18.051407]	your [-8.480328]	</s> [-0.962192]	bucket [-17.542070]	one [-8.826655]	</s> [-1.578155]	bucket [-17.618149]	the [-8.711721]	</s> [-1.438454]	bucket [-17.648830]	my [-8.096847]	</s> [-1.467601]	decentralize [-18.667290]	the [-8.721657]	</s> [-1.444643]	privilege [-17.852419]	hundred [-8.654446]	</s> [-1.383938]	keep [-18.836018]	the [-9.063247]	</s> [-1.295008]	keep [-18.799698]	your [-8.801334]	</s> [-1.048884]	keep [-18.723957]	an [-10.088801]	</s> [-1.260441]	soft [-16.411213]	your [-8.600492]	</s> [-0.932982]	break [-18.663885]	a [-8.161001]	</s> [-1.229845]	break [-18.708281]	your [-8.380661]	</s> [-1.098769]	break [-18.622358]	the [-8.522421]	</s> [-1.557419]	influence [-18.390789]	hundred [-8.051579]	</s> [-1.591368]	influence [-18.439007]	my [-8.288999]	</s> [-1.721518]	influence [-18.450129]	an [-9.019366]	</s> [-1.598366]	headquarter [-18.357246]	the [-8.135795]	</s> [-1.570470]	headquarter [-18.107216]	one [-8.079112]	</s> [-1.607348]	headquarter [-17.976284]	my [-8.066560]	</s> [-1.557389]	sweet [-14.169618]	an [-3.289574]	</s> [-1.473013]	get [-18.490391]	hundred [-8.283003]	</s> [-1.519046]	get [-18.499697]	my [-8.043909]	</s> [-1.365315]	splash [-16.134987]	one [-9.281747]	</s> [-1.327564]	splash [-16.039961]	your [-8.655853]	</s> [-0.840719]	commercialize [-17.811272]	a [-8.775230]	</s> [-1.085871]	overspecialize [-18.736506]	a [-8.384928]	</s> [-1.183147]	overspecialize [-18.790981]	an [-9.269054]	</s> [-1.550893]	overspecialize [-18.742996]	hundred [-8.186889]	</s> [-1.466157]	criticise [-17.365383]	a [-8.563686]	</s> [-1.002447]	criticise [-17.465240]	hundred [-8.538890]	</s> [-1.462412]	criticise [-17.573669]	one [-8.945208]	</s> [-1.531531]	gallivant [-17.915461]	an [-9.655530]	</s> [-1.518739]	gallivant [-17.790585]	a [-8.644820]	</s> [-1.013367]	gallivant [-18.115643]	your [-8.450562]	</s> [-0.797662]	gallivant [-17.889849]	one [-9.514669]	</s> [-1.189410]	intellectualize [-18.128809]	a [-8.464931]	</s> [-0.659332]	intellectualize [-18.129496]	the [-8.162370]	</s> [-1.034371]	number [-16.961945]	a [-8.442276]	</s> [-1.152864]	number [-17.042593]	one [-9.294357]	</s> [-1.441535]	prejudice [-17.523241]	my [-8.933595]	</s> [-1.536323]	prejudice [-17.467861]	the [-9.012436]	</s> [-1.517462]	choke [-18.461044]	hundred [-8.680536]	</s> [-1.484182]	secularize [-18.876410]	one [-8.839581]	</s> [-1.628009]	secularize [-18.865921]	a [-8.560322]	</s> [-1.016370]	secularize [-18.878258]	your [-8.094604]	</s> [-0.828124]	revitalise [-17.103371]	hundred [-8.119668]	</s> [-1.656620]	revitalise [-17.016613]	my [-7.922342]	</s> [-1.604472]	revitalise [-17.008184]	the [-8.104227]	</s> [-1.537444]	stanchion [-17.655848]	my [-8.748140]	</s> [-1.585616]	fall [-18.312574]	the [-8.490745]	</s> [-1.308393]	fall [-18.349432]	your [-8.894266]	</s> [-0.689717]	fall [-18.419355]	one [-9.043959]	</s> [-1.374637]	track [-17.314323]	an [-9.619750]	</s> [-1.210103]	track [-17.495846]	your [-8.325981]	</s> [-0.686802]	track [-17.548283]	the [-8.514933]	</s> [-1.393736]	overemphasize [-17.046474]	a [-8.740536]	</s> [-1.071881]	overemphasize [-16.764490]	an [-9.356677]	</s> [-1.420195]	overemphasize [-16.936829]	your [-8.732516]	</s> [-1.058425]	medium [-14.173743]	hundred [-3.714528]	</s> [-1.361211]	weasel [-17.090166]	hundred [-8.421690]	</s> [-1.248895]	weasel [-17.212233]	my [-8.757822]	</s> [-1.571037]	weasel [-17.140697]	a [-8.234982]	</s> [-1.010860]	weasel [-17.051800]	an [-9.110925]	</s> [-1.347461]	buttonhole [-18.049244]	your [-8.141181]	</s> [-0.673157]	square [-16.688231]	one [-9.361738]	</s> [-1.192300]	pressurize [-17.808590]	my [-8.277388]	</s> [-0.829503]	pressurize [-17.501076]	your [-7.973696]	</s> [-0.544106]	mispronounce [-18.938189]	a [-8.828696]	</s> [-0.764040]	replenish [-16.784250]	your [-8.295177]	</s> [-0.767161]	replenish [-16.756037]	my [-8.926163]	</s> [-0.915366]	blackguard [-17.251944]	the [-8.918617]	</s> [-1.308960]	blackguard [-17.156456]	hundred [-8.425713]	</s> [-1.377545]	blackguard [-17.131102]	a [-8.905644]	</s> [-0.952111]	blackguard [-17.325493]	an [-9.756588]	</s> [-1.340475]	bring [-18.538633]	your [-7.805923]	</s> [-0.877716]	bring [-18.609293]	hundred [-7.856837]	</s> [-1.278218]	still [-17.934742]	a [-8.244102]	</s> [-0.845374]	still [-17.799456]	hundred [-8.636986]	</s> [-1.257600]	still [-17.758371]	your [-8.063418]	</s> [-0.621502]	promenade [-19.033955]	an [-9.714287]
2024-12-19 09:06:12 | INFO | fairseq_cli.eval_lm | Evaluated 621 tokens in 0.8s (803.19 tokens/s)
2024-12-19 09:06:12 | INFO | fairseq_cli.eval_lm | Loss (base 2): 13.1641, Perplexity: 9178.55
