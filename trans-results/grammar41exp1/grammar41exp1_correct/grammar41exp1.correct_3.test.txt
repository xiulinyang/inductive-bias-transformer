2024-12-19 08:07:09 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX
2024-12-19 08:07:12 | INFO | fairseq_cli.eval_lm | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/grammar41exp1/3-transformer/checkpoint_best.pt', 'post_process': None, 'quiet': True, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': True, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': None, 'task': {'_name': 'language_modeling', 'data': 'data-bin/grammar41exp1/correct_3-dataset', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2024-12-19 08:07:12 | INFO | fairseq.tasks.language_modeling | dictionary: 1136 types
2024-12-19 08:07:12 | INFO | fairseq_cli.eval_lm | loading model(s) from checkpoints/grammar41exp1/3-transformer/checkpoint_best.pt
/workspace/artificial-languages/fairseq/fairseq/checkpoint_utils.py:340: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(f, map_location=torch.device("cpu"))
2024-12-19 08:07:15 | INFO | fairseq_cli.eval_lm | num. model params: 10,038,784
2024-12-19 08:07:15 | INFO | fairseq.data.data_utils | loaded 142 examples from: data-bin/grammar41exp1/correct_3-dataset/test
2024-12-19 08:07:15 | INFO | fairseq_cli.eval_lm | data-bin/grammar41exp1/correct_3-dataset test 1 examples
2024-12-19 08:07:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-12-19 08:07:15 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2024-12-19 08:07:15 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2024-12-19 08:07:15 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1
2024-12-19 08:07:17 | INFO | fairseq_cli.eval_lm | 0 your [-13.997926]	influence [-8.983291]	</s> [-0.918099]	an [-11.217687]	influence [-8.860063]	</s> [-0.915147]	your [-11.100252]	sound [-10.152301]	</s> [-0.872080]	hundred [-10.551703]	interchange [-9.596370]	</s> [-0.820400]	your [-11.471663]	interchange [-10.149383]	</s> [-0.838663]	an [-10.938313]	rich [-5.037083]	</s> [-0.957336]	a [-11.387985]	rich [-5.233106]	</s> [-0.938878]	one [-10.921712]	rich [-4.937193]	</s> [-1.037644]	hundred [-10.650303]	commingle [-9.314164]	</s> [-0.714995]	hundred [-10.534892]	inconvenience [-9.665306]	</s> [-0.701380]	the [-11.098679]	slaughter [-9.540663]	</s> [-0.988272]	hundred [-10.592566]	slaughter [-9.489775]	</s> [-0.895774]	a [-11.235730]	slaughter [-9.576529]	</s> [-0.915286]	your [-11.362236]	break [-9.965050]	</s> [-0.715539]	one [-10.799339]	pressurize [-10.863596]	</s> [-1.191667]	your [-11.020464]	choke [-11.158472]	</s> [-0.953807]	the [-11.110522]	distemper [-10.723946]	</s> [-0.999509]	a [-11.219647]	blind [-5.366555]	</s> [-1.350855]	hundred [-10.696729]	wheelbarrow [-10.213373]	</s> [-0.641092]	your [-11.306435]	wheelbarrow [-9.463638]	</s> [-0.588181]	an [-11.316034]	headquarter [-9.738934]	</s> [-0.871207]	hundred [-10.988775]	predetermine [-9.611825]	</s> [-0.765522]	your [-11.721173]	interfere [-9.635948]	</s> [-0.709546]	an [-11.470994]	bucket [-9.550641]	</s> [-0.869081]	hundred [-10.775168]	hero [-9.167330]	</s> [-1.032281]	your [-11.664772]	outbalance [-10.378157]	</s> [-0.595773]	a [-11.549145]	outbalance [-11.202228]	</s> [-0.660334]	the [-11.483271]	weasel [-11.499480]	</s> [-1.090628]	the [-11.568439]	overcharge [-10.021599]	</s> [-0.779184]	your [-11.679429]	overcharge [-10.029407]	</s> [-0.719593]	hundred [-10.767575]	misconduct [-9.586466]	</s> [-0.912098]	your [-11.724210]	spearhead [-10.680614]	</s> [-0.831333]	hundred [-10.759008]	spearhead [-10.620738]	</s> [-0.956689]	hundred [-10.744104]	follow [-9.190625]	</s> [-1.337248]	hundred [-10.683473]	listen [-10.226212]	</s> [-0.855752]	your [-11.398536]	listen [-9.895777]	</s> [-0.856036]	my [-10.641542]	overspecialize [-10.176759]	</s> [-0.864443]	my [-11.093759]	slap [-11.105110]	</s> [-0.908974]	hundred [-10.789263]	slap [-11.276645]	</s> [-0.776049]	the [-11.044474]	slap [-10.745483]	</s> [-0.809015]	the [-11.141105]	sneeze [-11.268791]	</s> [-0.748981]	my [-10.809455]	sneeze [-11.396632]	</s> [-0.781023]	hundred [-11.085980]	bring [-9.851403]	</s> [-0.773121]	hundred [-11.043312]	run [-7.062692]	</s> [-0.549310]	your [-11.871306]	run [-6.900089]	</s> [-0.647072]	hundred [-11.078265]	circumcise [-9.806049]	</s> [-0.921117]	your [-11.702742]	key [-10.401008]	</s> [-0.777307]	hundred [-10.947421]	key [-10.314228]	</s> [-0.841143]	hundred [-10.996637]	revitalise [-9.158787]	</s> [-0.759592]	a [-11.407184]	bump [-9.217026]	</s> [-1.034845]	your [-12.023004]	misbehave [-9.899122]	</s> [-0.814315]	hundred [-11.190314]	misbehave [-10.288116]	</s> [-0.795457]	an [-11.117387]	misbehave [-10.782566]	</s> [-0.768718]	hundred [-11.005088]	fall [-9.247500]	</s> [-0.959782]	one [-11.042881]	fall [-9.976774]	</s> [-0.877823]	one [-11.155779]	replenish [-10.681787]	</s> [-1.009942]	my [-10.849069]	carry [-10.238684]	</s> [-1.253865]	your [-11.718418]	crash [-8.746037]	</s> [-1.158824]	my [-10.915877]	provision [-10.662392]	</s> [-0.931512]	your [-11.687812]	splash [-11.529390]	</s> [-1.601675]	a [-11.539663]	splash [-10.782082]	</s> [-1.574264]	my [-10.750662]	splash [-11.173306]	</s> [-1.674524]	the [-11.375726]	mispronounce [-10.497330]	</s> [-1.303174]	hundred [-10.982450]	mispronounce [-10.604128]	</s> [-1.153257]	hundred [-11.011852]	fertilize [-10.429655]	</s> [-0.715850]	my [-11.036180]	fertilize [-10.108575]	</s> [-0.811077]	a [-11.767082]	number [-10.945928]	</s> [-0.757852]	hundred [-11.346037]	hydroplane [-10.034354]	</s> [-0.943793]	your [-11.739416]	hydroplane [-9.812301]	</s> [-0.925781]	my [-11.189255]	hydroplane [-10.446747]	</s> [-0.921792]	hundred [-11.390576]	disrespect [-10.932883]	</s> [-1.811707]	hundred [-11.246734]	overemphasize [-10.395663]	</s> [-0.921636]	an [-11.005880]	backtrack [-10.670090]	</s> [-1.917260]	a [-11.366862]	prescribe [-10.026397]	</s> [-0.839779]	your [-11.782553]	prescribe [-9.974558]	</s> [-0.853827]	hundred [-11.122315]	prescribe [-9.874966]	</s> [-0.812157]	the [-11.131970]	sweet [-5.792140]	</s> [-1.629550]	your [-11.799500]	criticise [-10.175255]	</s> [-0.947133]	hundred [-11.004955]	criticise [-9.613075]	</s> [-0.932274]	my [-10.702460]	burlesque [-11.090824]	</s> [-0.717805]	your [-11.765153]	burlesque [-11.176361]	</s> [-0.769893]	your [-11.583155]	commercialize [-11.079180]	</s> [-1.101462]	hundred [-11.099860]	commercialize [-11.053452]	</s> [-1.195016]	hundred [-11.417726]	rustle [-9.716249]	</s> [-0.915278]	one [-11.168159]	soft [-9.855074]	</s> [-1.129709]	your [-11.514409]	soft [-9.959435]	</s> [-1.261585]	my [-10.930103]	soft [-10.398210]	</s> [-1.039287]	your [-11.646627]	keep [-9.621735]	</s> [-0.843567]	hundred [-11.200236]	keep [-9.870695]	</s> [-0.823470]	one [-11.092863]	keep [-10.722538]	</s> [-0.814335]	a [-11.651014]	rubber [-10.134202]	</s> [-1.134402]	your [-11.515352]	rubber [-10.681583]	</s> [-1.094331]	hundred [-11.044902]	rubber [-10.339926]	</s> [-1.041150]	one [-11.173129]	secularize [-9.929237]	</s> [-1.209754]	your [-11.750240]	glamorize [-9.931278]	</s> [-0.842564]	my [-10.874896]	glamorize [-9.494903]	</s> [-0.865241]	hundred [-11.360398]	close [-9.549830]	</s> [-1.083986]	hundred [-11.562507]	stanchion [-11.268180]	</s> [-1.598450]	the [-11.488169]	get [-9.929605]	</s> [-0.896601]	the [-11.151710]	blackguard [-10.135797]	</s> [-0.684538]	a [-11.678164]	blackguard [-10.185905]	</s> [-0.687017]	hundred [-11.256648]	blackguard [-9.467138]	</s> [-0.671547]	your [-11.599053]	racketeer [-10.327888]	</s> [-1.234929]	hundred [-11.251347]	racketeer [-10.166292]	</s> [-1.297849]	your [-11.655151]	shove [-10.385896]	</s> [-1.078397]	an [-11.209554]	shove [-11.132347]	</s> [-1.005179]	hundred [-11.233900]	disembowel [-10.050590]	</s> [-0.689592]	your [-11.666968]	disembowel [-10.589725]	</s> [-0.814096]	your [-11.110833]	decentralize [-9.857919]	</s> [-1.010207]	hundred [-10.660300]	decentralize [-9.623196]	</s> [-0.996617]	hundred [-10.693063]	dress [-10.985110]	</s> [-1.434116]	your [-11.265069]	dress [-10.527225]	</s> [-1.592938]	your [-11.284501]	intellectualize [-9.641824]	</s> [-1.061214]	hundred [-10.758701]	intellectualize [-9.963387]	</s> [-0.879593]	the [-10.921574]	jeopardize [-10.876095]	</s> [-0.941038]	my [-11.174685]	intermarry [-10.084366]	</s> [-0.698283]	the [-11.249467]	weird [-5.850447]	</s> [-1.386328]	hundred [-10.967972]	booby [-10.317042]	</s> [-1.267658]	one [-11.096394]	booby [-10.922499]	</s> [-1.197824]	a [-11.372801]	pitchfork [-9.882189]	</s> [-0.792991]	hundred [-10.896827]	ginger [-9.497730]	</s> [-1.115495]	my [-10.908465]	ginger [-10.265645]	</s> [-1.099096]	the [-10.909437]	ginger [-10.117271]	</s> [-1.060351]	my [-10.857173]	crosscheck [-9.686182]	</s> [-0.959628]	hundred [-10.766043]	power [-10.802702]	</s> [-0.932347]	your [-11.620646]	folk [-9.573938]	</s> [-0.999309]	an [-11.137263]	mountebank [-11.642581]	</s> [-0.937261]	your [-11.427999]	smooth [-9.596955]	</s> [-0.893158]	an [-11.049623]	blaspheme [-10.555792]	</s> [-0.884156]	hundred [-10.769138]	demagnetise [-10.435905]	</s> [-0.987750]	your [-11.339625]	demagnetise [-10.301243]	</s> [-1.003758]	hundred [-10.721051]	come [-8.966875]	</s> [-0.705336]	a [-11.513425]	prejudice [-10.356037]	</s> [-0.924682]	an [-11.101204]	prejudice [-10.189900]	</s> [-0.921857]	one [-11.223031]	prejudice [-10.227275]	</s> [-0.823112]	the [-11.217442]	brutalize [-10.328088]	</s> [-0.681893]	one [-11.219982]	square [-9.770926]	</s> [-0.804515]	the [-11.274750]	square [-9.366717]	</s> [-0.894490]	an [-11.227968]	square [-10.377124]	</s> [-0.893979]	hundred [-10.928141]	still [-10.222311]	</s> [-0.831446]	your [-11.486274]	still [-10.382772]	</s> [-0.844292]	my [-10.925207]	still [-10.881935]	</s> [-0.887637]
2024-12-19 08:07:17 | INFO | fairseq_cli.eval_lm | Evaluated 426 tokens in 0.9s (492.99 tokens/s)
2024-12-19 08:07:17 | INFO | fairseq_cli.eval_lm | Loss (base 2): 10.6405, Perplexity: 1596.30
