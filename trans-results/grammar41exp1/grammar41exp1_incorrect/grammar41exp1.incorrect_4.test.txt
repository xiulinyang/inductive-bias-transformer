2024-12-19 08:39:23 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX
2024-12-19 08:39:25 | INFO | fairseq_cli.eval_lm | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/grammar41exp1/4-transformer/checkpoint_best.pt', 'post_process': None, 'quiet': True, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': True, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': None, 'task': {'_name': 'language_modeling', 'data': 'data-bin/grammar41exp1/incorrect_4-dataset', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2024-12-19 08:39:25 | INFO | fairseq.tasks.language_modeling | dictionary: 1136 types
2024-12-19 08:39:25 | INFO | fairseq_cli.eval_lm | loading model(s) from checkpoints/grammar41exp1/4-transformer/checkpoint_best.pt
/workspace/artificial-languages/fairseq/fairseq/checkpoint_utils.py:340: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(f, map_location=torch.device("cpu"))
2024-12-19 08:39:28 | INFO | fairseq_cli.eval_lm | num. model params: 10,038,784
2024-12-19 08:39:28 | INFO | fairseq.data.data_utils | loaded 134 examples from: data-bin/grammar41exp1/incorrect_4-dataset/test
2024-12-19 08:39:28 | INFO | fairseq_cli.eval_lm | data-bin/grammar41exp1/incorrect_4-dataset test 1 examples
2024-12-19 08:39:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-12-19 08:39:28 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2024-12-19 08:39:28 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2024-12-19 08:39:28 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1
2024-12-19 08:39:30 | INFO | fairseq_cli.eval_lm | 0 brutalize [-16.174080]	incomplete [-8.329270]	</s> [-7.995615]	brutalize [-17.705587]	meek [-9.049363]	</s> [-3.056480]	brutalize [-17.574335]	smile [-10.139293]	</s> [-2.228669]	racketeer [-17.881529]	the [-4.379550]	</s> [-10.741456]	disembowel [-18.517658]	narrow [-6.840989]	</s> [-10.609850]	slaughter [-19.486490]	exercise [-8.048923]	</s> [-1.719792]	slaughter [-19.352919]	shift [-7.287094]	</s> [-2.502065]	headquarter [-19.780033]	one [-4.181510]	</s> [-9.643452]	apologize [-19.318138]	rice [-8.310241]	</s> [-1.473951]	apologize [-19.315691]	vague [-7.605751]	</s> [-8.327413]	fertilize [-17.699749]	ill [-6.877565]	</s> [-11.284062]	fertilize [-17.580153]	one [-4.159793]	</s> [-9.736256]	misconduct [-17.964737]	boat [-7.813932]	</s> [-1.784905]	misconduct [-17.935801]	a [-4.968667]	</s> [-9.525143]	circumcise [-19.316851]	broad [-7.390129]	</s> [-1.867805]	circumcise [-19.503725]	massive [-9.107070]	</s> [-7.568172]	circumcise [-19.555382]	the [-3.983127]	</s> [-10.581822]	misbehave [-18.753435]	employee [-8.546189]	</s> [-2.613155]	misbehave [-18.501905]	one [-4.346829]	</s> [-9.960061]	misbehave [-18.365953]	my [-4.286901]	</s> [-8.389244]	back [-18.340687]	hundred [-3.140307]	</s> [-9.299536]	disrespect [-19.657583]	garbage [-7.894768]	</s> [-2.136346]	disrespect [-19.566385]	chubby [-6.871548]	</s> [-10.177702]	privilege [-17.583647]	fickle [-6.095977]	</s> [-10.091010]	come [-19.173061]	tourist [-8.104785]	</s> [-1.773121]	blaspheme [-18.920202]	professional [-8.021533]	</s> [-1.406810]	blaspheme [-18.953564]	the [-3.947097]	</s> [-10.662017]	outbalance [-18.859180]	milk [-8.728846]	</s> [-1.901628]	outbalance [-18.990852]	a [-4.625472]	</s> [-9.599239]	outbalance [-18.996857]	an [-3.624573]	</s> [-9.083778]	number [-17.427692]	concrete [-7.190982]	</s> [-8.343948]	criticise [-17.731308]	subtle [-7.174726]	</s> [-8.450763]	criticise [-17.496994]	perky [-6.287630]	</s> [-10.074318]	criticise [-17.660122]	one [-4.019009]	</s> [-10.001091]	blackguard [-19.707748]	accomplished [-6.425559]	</s> [-8.061083]	blackguard [-19.822998]	an [-3.979220]	</s> [-9.079491]	constrict [-18.380146]	a [-4.172745]	</s> [-9.419654]	introvert [-17.146505]	a [-4.030007]	</s> [-9.645620]	interchange [-18.477734]	direction [-8.801116]	</s> [-2.123780]	interchange [-18.631685]	one [-3.679546]	</s> [-9.715748]	listen [-18.768433]	used [-6.586326]	</s> [-8.635606]	listen [-18.593100]	extroverted [-7.430003]	</s> [-4.364089]	listen [-18.693909]	a [-3.960705]	</s> [-9.701565]	break [-18.249702]	my [-3.733696]	</s> [-7.948492]	blind [-16.339394]	gene [-7.699118]	</s> [-2.153182]	blind [-16.473413]	merry [-5.260361]	</s> [-5.109246]	soft [-17.081781]	loyal [-7.823185]	</s> [-7.473901]	rediscover [-17.466293]	subdued [-6.499536]	</s> [-10.135854]	replenish [-19.091372]	foolish [-7.066593]	</s> [-10.008155]	replenish [-19.011219]	an [-4.131763]	</s> [-9.139000]	track [-18.046480]	the [-4.093508]	</s> [-10.857417]	dillydally [-18.869606]	my [-3.976654]	</s> [-8.874164]	give [-17.880966]	inexperienced [-6.759690]	</s> [-7.775796]	decentralize [-17.881493]	objective [-8.695964]	</s> [-1.921201]	decentralize [-17.943634]	delectable [-7.606056]	</s> [-10.397120]	decentralize [-18.014252]	luxurious [-6.783781]	</s> [-6.282612]	folk [-18.439238]	a [-4.100125]	</s> [-9.754770]	folk [-18.373993]	clock [-8.158998]	</s> [-1.894662]	bring [-17.833609]	engineering [-7.536281]	</s> [-1.435289]	bring [-17.848846]	an [-4.140743]	</s> [-9.169647]	get [-19.119658]	voluminous [-6.459010]	</s> [-8.015820]	burlesque [-18.047365]	cat [-8.471939]	</s> [-1.866424]	burlesque [-18.167374]	married [-6.609311]	</s> [-10.525715]	booby [-18.232624]	secret [-7.461307]	</s> [-3.179976]	booby [-18.211828]	one [-4.262308]	</s> [-9.809064]	secularize [-19.126602]	an [-4.213258]	</s> [-9.379519]	secularize [-19.076488]	alienated [-6.814312]	</s> [-10.351784]	interfere [-19.314150]	harmful [-7.568721]	</s> [-8.397453]	interfere [-19.237015]	a [-4.560534]	</s> [-9.533920]	prejudice [-18.479713]	an [-4.021110]	</s> [-9.501345]	hero [-18.792496]	one [-4.377046]	</s> [-9.878141]	intellectualize [-18.073881]	hope [-6.617583]	</s> [-2.044609]	stanchion [-18.352406]	valuable [-6.341257]	</s> [-9.781501]	stanchion [-18.226486]	routine [-8.690449]	</s> [-1.379645]	stanchion [-18.141817]	vague [-6.803728]	</s> [-9.479095]	sneeze [-18.936104]	earnest [-7.665114]	</s> [-11.270769]	inconvenience [-17.405111]	a [-4.038964]	</s> [-9.300010]	rubber [-19.554680]	sudden [-8.234818]	</s> [-8.346674]	provision [-19.443008]	effect [-8.260868]	</s> [-1.571192]	provision [-19.466612]	stop [-8.210028]	</s> [-1.923446]	provision [-19.586065]	unequaled [-7.139002]	</s> [-7.291495]	sound [-17.887383]	an [-4.017853]	</s> [-10.144941]	commingle [-18.082720]	bold [-7.717558]	</s> [-9.943725]	overemphasize [-19.174734]	reward [-8.966443]	</s> [-2.655467]	bump [-17.799807]	middle [-8.094662]	</s> [-2.037990]	bump [-17.767450]	menu [-7.745581]	</s> [-1.688793]	bump [-17.754299]	my [-4.127756]	</s> [-8.085013]	bump [-17.678766]	an [-4.533830]	</s> [-9.192451]	jeopardize [-18.809128]	multicolored [-6.492947]	</s> [-6.463036]	medium [-16.465912]	the [-8.074712]	</s> [-10.455853]	square [-17.520823]	magnificent [-7.862424]	</s> [-8.203964]	follow [-18.909025]	organic [-5.914299]	</s> [-9.246284]	follow [-18.942709]	my [-3.171554]	</s> [-8.464916]	smooth [-18.247879]	my [-3.434840]	</s> [-8.944185]	smooth [-18.370106]	the [-3.878608]	</s> [-10.691399]	commercialize [-17.878866]	elderly [-6.179169]	</s> [-10.850023]	run [-18.308599]	investment [-8.314952]	</s> [-1.675651]	run [-18.284052]	innocent [-6.770537]	</s> [-8.146159]	weasel [-18.370033]	weird [-7.318187]	</s> [-2.184170]	rustle [-17.588072]	split [-8.667376]	</s> [-2.772105]	choke [-18.431643]	unequaled [-6.699228]	</s> [-7.404044]	hydroplane [-17.934650]	blind [-5.881122]	</s> [-1.798785]	hydroplane [-18.102036]	inexperienced [-6.296396]	</s> [-8.383491]	hydroplane [-18.041697]	my [-3.587161]	</s> [-9.025919]	buttonhole [-19.293236]	parallel [-5.691452]	</s> [-10.772697]	close [-19.045685]	ladder [-8.377860]	</s> [-1.460595]	close [-19.094479]	candy [-8.053250]	</s> [-1.834452]	close [-19.064207]	the [-3.930177]	</s> [-11.622149]	undervalue [-19.185266]	a [-3.823869]	</s> [-9.914845]	overspecialize [-18.847502]	stupid [-7.494944]	</s> [-8.906118]	promenade [-18.412451]	blind [-6.426278]	</s> [-1.893372]	still [-17.949888]	an [-3.808709]	</s> [-10.013584]	slap [-19.266623]	my [-3.608626]	</s> [-9.337539]	mispronounce [-18.655781]	phone [-9.624701]	</s> [-2.338920]	mispronounce [-18.606237]	gruesome [-6.631182]	</s> [-7.254785]	sweet [-15.632104]	capitalize [-8.003855]	</s> [-0.479910]	mountebank [-17.972536]	legal [-8.975583]	</s> [-9.015344]	mountebank [-17.960783]	a [-3.936202]	</s> [-9.881843]	bucket [-18.030441]	perfect [-6.772900]	</s> [-10.582778]	bucket [-17.887121]	blind [-6.205961]	</s> [-1.766042]	pressurize [-18.020859]	violent [-6.585466]	</s> [-6.714090]	influence [-18.473738]	an [-3.961621]	</s> [-9.420464]	dress [-18.869261]	stingy [-6.949634]	</s> [-9.588202]	dress [-18.895405]	boot [-7.719920]	</s> [-1.555642]	crosscheck [-17.366732]	prickly [-7.213753]	</s> [-6.695167]	crosscheck [-17.286783]	straight [-6.523437]	</s> [-8.605781]	crosscheck [-17.319914]	my [-4.071633]	</s> [-9.116251]	weird [-16.635736]	player [-7.888421]	</s> [-1.847445]	intercede [-18.526098]	the [-4.449185]	</s> [-10.325103]	intercede [-18.487097]	rich [-7.011105]	</s> [-1.720211]	reemphasize [-19.014494]	dependent [-7.031446]	</s> [-10.949143]	reemphasize [-19.012110]	special [-9.269808]	</s> [-2.555299]	reemphasize [-18.988115]	a [-4.295530]	</s> [-9.789700]	spearhead [-18.567562]	stupid [-7.136783]	</s> [-9.278779]
2024-12-19 08:39:30 | INFO | fairseq_cli.eval_lm | Evaluated 402 tokens in 0.8s (494.86 tokens/s)
2024-12-19 08:39:30 | INFO | fairseq_cli.eval_lm | Loss (base 2): 15.2566, Perplexity: 39146.99
