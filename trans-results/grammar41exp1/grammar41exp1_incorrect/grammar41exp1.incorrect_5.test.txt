2024-12-19 09:04:00 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX
2024-12-19 09:04:02 | INFO | fairseq_cli.eval_lm | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/grammar41exp1/5-transformer/checkpoint_best.pt', 'post_process': None, 'quiet': True, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': True, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': None, 'task': {'_name': 'language_modeling', 'data': 'data-bin/grammar41exp1/incorrect_5-dataset', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2024-12-19 09:04:02 | INFO | fairseq.tasks.language_modeling | dictionary: 1136 types
2024-12-19 09:04:02 | INFO | fairseq_cli.eval_lm | loading model(s) from checkpoints/grammar41exp1/5-transformer/checkpoint_best.pt
/workspace/artificial-languages/fairseq/fairseq/checkpoint_utils.py:340: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(f, map_location=torch.device("cpu"))
2024-12-19 09:04:05 | INFO | fairseq_cli.eval_lm | num. model params: 10,038,784
2024-12-19 09:04:05 | INFO | fairseq.data.data_utils | loaded 133 examples from: data-bin/grammar41exp1/incorrect_5-dataset/test
2024-12-19 09:04:05 | INFO | fairseq_cli.eval_lm | data-bin/grammar41exp1/incorrect_5-dataset test 1 examples
2024-12-19 09:04:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-12-19 09:04:05 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2024-12-19 09:04:05 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2024-12-19 09:04:05 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1
2024-12-19 09:04:06 | INFO | fairseq_cli.eval_lm | 0 ginger [-14.513254]	appeal [-8.132069]	</s> [-1.538600]	give [-16.781628]	low [-9.689384]	</s> [-9.382300]	give [-16.409946]	rich [-6.596084]	</s> [-2.307703]	demagnetise [-18.396685]	my [-3.315646]	</s> [-10.523266]	racketeer [-18.043606]	one [-4.143637]	</s> [-10.338245]	slap [-17.431755]	thoughtful [-6.429025]	</s> [-11.340168]	pitchfork [-17.808611]	an [-4.503459]	</s> [-10.922274]	carry [-16.656218]	rich [-6.436060]	</s> [-2.305725]	carry [-16.559626]	one [-3.268523]	</s> [-9.945699]	rich [-13.788879]	insidious [-6.425416]	</s> [-6.302094]	rich [-14.052558]	hundred [-7.632521]	</s> [-8.234505]	follow [-17.925751]	animated [-7.433343]	</s> [-5.978880]	disembowel [-17.848333]	stable [-8.041037]	</s> [-3.234954]	folk [-17.897121]	status [-7.551055]	</s> [-1.872592]	apologize [-17.594391]	highway [-7.266730]	</s> [-1.712852]	apologize [-17.697113]	the [-3.291489]	</s> [-9.984786]	brutalize [-18.224852]	towel [-7.163948]	</s> [-2.009755]	hero [-17.283052]	crisp [-5.510168]	</s> [-7.851135]	hero [-17.290003]	a [-3.830212]	</s> [-10.240468]	slaughter [-17.785231]	the [-3.101115]	</s> [-10.625429]	smooth [-18.908527]	a [-4.332259]	</s> [-10.311033]	smooth [-19.092533]	the [-3.137655]	</s> [-10.652889]	smooth [-18.984945]	animated [-6.811636]	</s> [-6.534666]	run [-16.860010]	your [-3.415914]	</s> [-9.176584]	glamorize [-17.672541]	an [-3.893786]	</s> [-11.339624]	burlesque [-16.819740]	faint [-7.329435]	</s> [-8.104290]	disrespect [-18.922562]	likely [-7.826202]	</s> [-5.097990]	disrespect [-18.993876]	hate [-8.493225]	</s> [-2.359701]	listen [-18.970200]	a [-3.247243]	</s> [-10.486216]	booby [-18.072811]	noxious [-9.314213]	</s> [-9.247929]	jeopardize [-17.437801]	lavish [-6.437490]	</s> [-5.989323]	overcharge [-17.958061]	one [-4.183926]	</s> [-10.901991]	rubber [-17.687546]	easy-going [-6.742220]	</s> [-6.317378]	undervalue [-17.293802]	subtle [-6.228490]	</s> [-8.559750]	undervalue [-17.345181]	an [-3.927329]	</s> [-11.505644]	inconvenience [-18.668980]	my [-3.388515]	</s> [-10.927649]	sound [-18.905552]	communication [-8.181597]	</s> [-2.379131]	come [-16.736078]	ideal [-6.704072]	</s> [-7.903255]	outbalance [-17.741125]	my [-3.741391]	</s> [-10.801768]	crash [-17.156488]	street [-6.755589]	</s> [-3.487797]	crash [-17.197454]	an [-3.419135]	</s> [-11.735079]	stand [-18.130819]	violent [-6.042557]	</s> [-11.421204]	dehumanize [-18.864216]	overdue [-8.924336]	</s> [-6.761689]	dehumanize [-18.842585]	my [-3.784970]	</s> [-10.994521]	dehumanize [-18.898388]	one [-3.942286]	</s> [-10.922984]	sneeze [-18.408417]	long [-7.340826]	</s> [-1.618970]	back [-15.811277]	run [-7.718259]	</s> [-1.050852]	back [-16.011269]	your [-3.531904]	</s> [-9.077920]	spearhead [-18.390179]	the [-3.630903]	</s> [-10.805906]	crosscheck [-17.999918]	front [-6.749936]	</s> [-9.202084]	crosscheck [-18.156275]	valid [-6.987823]	</s> [-6.869892]	prescribe [-18.214605]	glad [-7.874251]	</s> [-1.744246]	mountebank [-17.634119]	stingy [-6.179276]	</s> [-10.091335]	mountebank [-17.558725]	an [-4.032501]	</s> [-11.768723]	mountebank [-17.524179]	one [-4.236274]	</s> [-10.900204]	close [-17.900562]	thoughtful [-7.024444]	</s> [-12.077032]	blaspheme [-17.601540]	spry [-6.066461]	</s> [-10.242851]	bucket [-18.360050]	wood [-8.209525]	</s> [-1.733731]	bucket [-18.253218]	hope [-7.027221]	</s> [-2.209756]	decentralize [-19.020235]	one [-4.494312]	</s> [-10.542303]	privilege [-18.072901]	superb [-7.425872]	</s> [-3.767153]	privilege [-18.129448]	a [-3.765761]	</s> [-9.948230]	keep [-18.856804]	the [-3.593845]	</s> [-10.053648]	break [-17.817177]	national [-7.614054]	</s> [-2.280041]	influence [-18.198912]	whopping [-7.404107]	</s> [-11.560564]	influence [-18.387829]	one [-4.069273]	</s> [-10.312794]	headquarter [-18.560328]	sweet [-6.464384]	</s> [-1.842273]	headquarter [-18.462574]	the [-3.541323]	</s> [-10.388586]	headquarter [-18.346052]	my [-3.490126]	</s> [-10.762707]	sweet [-15.773246]	prize [-7.901769]	</s> [-7.902963]	sweet [-15.736015]	hundred [-6.514198]	</s> [-9.110756]	get [-19.227106]	attack [-7.646292]	</s> [-2.651915]	get [-19.108601]	the [-3.350443]	</s> [-10.780766]	splash [-16.112595]	compassionate [-6.662141]	</s> [-8.129475]	commercialize [-18.088671]	the [-3.647256]	</s> [-10.096065]	overspecialize [-18.365292]	rich [-6.359163]	</s> [-2.633165]	criticise [-18.032558]	happy [-6.923607]	</s> [-7.320387]	gallivant [-18.325586]	delicious [-6.536927]	</s> [-8.346501]	intellectualize [-17.851357]	sharp [-6.251069]	</s> [-3.627934]	prejudice [-17.761492]	multicolored [-6.591209]	</s> [-8.104536]	prejudice [-17.910652]	one [-3.975191]	</s> [-10.974966]	secularize [-18.371822]	reasonable [-8.620804]	</s> [-9.596599]	secularize [-18.327915]	estimate [-7.825876]	</s> [-1.821734]	revitalise [-16.982597]	sharp [-7.105865]	</s> [-3.840937]	stanchion [-17.809336]	an [-4.207836]	</s> [-11.413233]	stanchion [-17.786333]	a [-3.887533]	</s> [-9.822045]	fall [-19.102888]	spicy [-6.326611]	</s> [-6.476393]	track [-17.861532]	valuable [-6.458849]	</s> [-6.867877]	shove [-17.360735]	the [-3.547916]	</s> [-10.017518]	shove [-17.363668]	a [-3.643094]	</s> [-9.715252]	overemphasize [-16.589321]	close [-10.037558]	</s> [-0.783396]	overemphasize [-16.567448]	the [-3.119235]	</s> [-10.596827]	medium [-14.332703]	smooth [-7.620072]	</s> [-1.124062]	medium [-14.342375]	quizzical [-7.375291]	</s> [-5.769319]	medium [-14.189487]	vault [-7.420604]	</s> [-1.022793]	weasel [-17.296356]	wise [-6.567516]	</s> [-8.807159]	buttonhole [-18.386629]	the [-3.468657]	</s> [-11.149226]	square [-17.155472]	print [-8.430990]	</s> [-2.174208]	pressurize [-17.669109]	my [-3.653290]	</s> [-11.562740]	replenish [-17.053722]	the [-2.949389]	</s> [-10.759138]	blackguard [-17.752350]	stretch [-7.473936]	</s> [-2.473830]	blackguard [-17.727110]	my [-3.442517]	</s> [-10.794966]	bring [-18.332003]	neat [-6.629107]	</s> [-7.597904]	bring [-18.226660]	ironclad [-7.017286]	</s> [-6.820241]	bring [-18.204157]	my [-4.020282]	</s> [-11.100156]	still [-17.245127]	appeal [-7.518195]	</s> [-2.012631]	still [-17.332552]	skin [-9.353040]	</s> [-2.371073]	constrict [-17.503366]	physical [-6.057646]	</s> [-8.718655]	rediscover [-18.155928]	negligible [-6.672122]	</s> [-4.617613]	rediscover [-18.123722]	my [-3.582910]	</s> [-10.717124]	predetermine [-16.621309]	mortified [-7.564586]	</s> [-10.475780]	misbehave [-17.326792]	inspection [-7.219091]	</s> [-2.318502]	misbehave [-17.267473]	math [-7.700405]	</s> [-3.001506]	misbehave [-17.237806]	charge [-7.017067]	</s> [-2.139196]	misbehave [-17.410667]	a [-3.579454]	</s> [-10.183095]	fertilize [-18.264870]	pool [-9.425207]	</s> [-2.320736]	fertilize [-18.148037]	one [-4.500226]	</s> [-11.123470]	key [-18.784744]	carefree [-5.961411]	</s> [-6.166774]	commingle [-17.467176]	estimate [-8.373775]	</s> [-1.743943]	misconduct [-17.183111]	unequaled [-6.999058]	</s> [-5.022309]	misconduct [-17.402092]	an [-3.612087]	</s> [-11.724087]	wheelbarrow [-17.573734]	overjoyed [-7.664997]	</s> [-9.508184]	wheelbarrow [-17.504711]	narrow [-6.453424]	</s> [-10.808336]	rustle [-16.632811]	bruised [-7.616190]	</s> [-8.813482]	rustle [-16.822861]	the [-3.605465]	</s> [-10.253292]	provision [-17.211184]	grade [-7.187179]	</s> [-2.221807]	provision [-17.165417]	a [-3.574597]	</s> [-10.216474]	circumcise [-17.945393]	silent [-6.776883]	</s> [-6.744560]	circumcise [-17.811798]	damaged [-6.268398]	</s> [-9.780684]	distemper [-18.982174]	sweet [-6.624311]	</s> [-1.777199]	interchange [-17.760571]	hunt [-9.201278]	</s> [-2.406488]	blind [-14.210335]	ashamed [-7.831895]	</s> [-6.136060]	blind [-14.165637]	a [-6.328556]	</s> [-9.796431]
2024-12-19 09:04:06 | INFO | fairseq_cli.eval_lm | Evaluated 399 tokens in 0.8s (524.18 tokens/s)
2024-12-19 09:04:06 | INFO | fairseq_cli.eval_lm | Loss (base 2): 14.8551, Perplexity: 29637.44
