2024-12-19 06:56:49 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX
2024-12-19 06:56:52 | INFO | fairseq_cli.eval_lm | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/grammar41exp1_permutation/1-transformer/checkpoint_best.pt', 'post_process': None, 'quiet': True, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': True, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': None, 'task': {'_name': 'language_modeling', 'data': 'data-bin/grammar41exp1_permutation/correct_1-dataset', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2024-12-19 06:56:52 | INFO | fairseq.tasks.language_modeling | dictionary: 1136 types
2024-12-19 06:56:52 | INFO | fairseq_cli.eval_lm | loading model(s) from checkpoints/grammar41exp1_permutation/1-transformer/checkpoint_best.pt
/workspace/artificial-languages/fairseq/fairseq/checkpoint_utils.py:340: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(f, map_location=torch.device("cpu"))
2024-12-19 06:56:55 | INFO | fairseq_cli.eval_lm | num. model params: 10,038,784
2024-12-19 06:56:55 | INFO | fairseq.data.data_utils | loaded 134 examples from: data-bin/grammar41exp1_permutation/correct_1-dataset/test
2024-12-19 06:56:55 | INFO | fairseq_cli.eval_lm | data-bin/grammar41exp1_permutation/correct_1-dataset test 1 examples
2024-12-19 06:56:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-12-19 06:56:55 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2024-12-19 06:56:55 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2024-12-19 06:56:55 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1
2024-12-19 06:56:58 | INFO | fairseq_cli.eval_lm | 0 rediscover [-13.255514]	an [-9.597639]	</s> [-2.171417]	bring [-19.170689]	the [-4.481061]	</s> [-2.807577]	number [-19.334179]	one [-4.785577]	</s> [-3.086944]	folk [-19.730219]	the [-4.642007]	</s> [-2.938344]	folk [-19.901714]	one [-5.390873]	</s> [-3.162831]	still [-18.747623]	your [-7.770487]	</s> [-2.788742]	still [-18.573656]	the [-4.242208]	</s> [-2.927093]	prejudice [-17.573252]	the [-4.052645]	</s> [-3.056525]	crosscheck [-19.166826]	your [-7.510793]	</s> [-2.955042]	crosscheck [-19.231522]	the [-3.888133]	</s> [-2.907995]	intercede [-20.522461]	hundred [-7.787033]	</s> [-2.486707]	intercede [-20.584400]	an [-3.844802]	</s> [-2.423356]	sneeze [-17.967337]	the [-4.323609]	</s> [-2.983706]	smooth [-18.095251]	an [-4.238613]	</s> [-2.570248]	smooth [-18.144234]	one [-4.870816]	</s> [-3.313214]	back [-17.575426]	hundred [-6.712684]	</s> [-2.717906]	interchange [-18.995832]	hundred [-8.867250]	</s> [-2.804952]	interchange [-19.067970]	the [-4.051792]	</s> [-3.095040]	intermarry [-20.542957]	the [-4.139949]	</s> [-3.085119]	intermarry [-20.644156]	an [-4.267089]	</s> [-2.784421]	misbehave [-19.301075]	my [-4.135033]	</s> [-3.040786]	misbehave [-19.295513]	one [-4.049491]	</s> [-3.217976]	stand [-17.762512]	hundred [-8.497128]	</s> [-2.761140]	disembowel [-19.716040]	an [-4.356231]	</s> [-2.628450]	disembowel [-19.925510]	a [-4.824278]	</s> [-2.522671]	disembowel [-19.692406]	the [-4.170977]	</s> [-3.139021]	wheelbarrow [-17.917372]	hundred [-8.038406]	</s> [-2.724720]	wheelbarrow [-18.124718]	your [-7.678555]	</s> [-2.987037]	circumcise [-18.865658]	hundred [-7.827299]	</s> [-2.799898]	give [-17.630482]	your [-6.095865]	</s> [-2.922395]	burlesque [-18.004004]	a [-4.944144]	</s> [-2.602234]	dillydally [-19.171993]	your [-7.922471]	</s> [-2.819839]	listen [-19.330872]	your [-8.137439]	</s> [-2.875439]	listen [-19.292511]	an [-4.155674]	</s> [-2.656895]	bucket [-18.507893]	my [-3.489775]	</s> [-3.051653]	bucket [-18.448814]	your [-7.598064]	</s> [-3.052517]	dehumanize [-19.059750]	hundred [-8.393426]	</s> [-2.833944]	criticise [-17.533583]	one [-4.768249]	</s> [-3.138785]	criticise [-17.617149]	a [-4.747780]	</s> [-2.734739]	predetermine [-19.207060]	one [-3.635471]	</s> [-3.070730]	hero [-18.478174]	an [-3.409890]	</s> [-2.552857]	dress [-19.654388]	my [-3.705153]	</s> [-3.029866]	intellectualize [-19.164114]	your [-8.776155]	</s> [-2.782718]	constrict [-18.103827]	hundred [-8.185850]	</s> [-2.739098]	constrict [-17.984383]	a [-4.649786]	</s> [-2.797743]	run [-16.942854]	my [-3.258655]	</s> [-3.014551]	run [-17.128160]	your [-5.887890]	</s> [-2.968096]	square [-16.943100]	a [-4.275126]	</s> [-2.218724]	square [-16.763597]	one [-4.740399]	</s> [-2.947065]	blind [-13.931840]	one [-2.803855]	</s> [-3.072777]	blind [-13.982063]	a [-3.103867]	</s> [-2.625342]	headquarter [-19.001301]	an [-5.044938]	</s> [-2.500795]	headquarter [-18.984404]	your [-8.404148]	</s> [-2.902789]	headquarter [-18.830568]	my [-4.420578]	</s> [-2.817666]	ginger [-20.198685]	one [-5.402459]	</s> [-2.995833]	ginger [-20.130129]	my [-4.516792]	</s> [-2.908851]	sweet [-16.460434]	a [-3.388949]	</s> [-2.423418]	sweet [-16.451632]	hundred [-4.629999]	</s> [-2.549557]	weasel [-19.600311]	hundred [-7.817359]	</s> [-2.464163]	weasel [-19.518700]	one [-4.017643]	</s> [-2.977868]	stanchion [-19.774549]	one [-5.050422]	</s> [-3.049045]	stanchion [-19.893919]	a [-4.710057]	</s> [-2.542523]	stanchion [-19.730618]	the [-4.566787]	</s> [-2.940952]	replenish [-18.614922]	your [-8.248009]	</s> [-2.874131]	weird [-14.662220]	my [-2.789824]	</s> [-3.058188]	weird [-14.772532]	the [-2.856271]	</s> [-2.960284]	misconduct [-19.044874]	a [-4.698189]	</s> [-2.507073]	misconduct [-18.881519]	an [-3.978218]	</s> [-2.342865]	inconvenience [-19.196127]	my [-4.562342]	</s> [-2.886890]	inconvenience [-19.203791]	your [-8.314762]	</s> [-2.945090]	promenade [-19.293301]	your [-8.279237]	</s> [-2.824900]	slap [-20.277445]	the [-2.646519]	</s> [-2.868409]	slap [-20.411669]	a [-3.545878]	</s> [-2.340590]	fertilize [-18.731367]	hundred [-8.698310]	</s> [-2.923419]	fertilize [-18.881741]	your [-8.578063]	</s> [-3.047955]	mispronounce [-19.391798]	my [-4.132326]	</s> [-2.867121]	break [-19.451534]	my [-4.776103]	</s> [-2.825511]	influence [-18.334768]	an [-4.238188]	</s> [-2.484457]	influence [-18.200127]	the [-4.285346]	</s> [-2.940033]	influence [-18.222563]	one [-4.478176]	</s> [-3.145819]	crash [-20.249889]	my [-4.162852]	</s> [-2.898903]	crash [-20.249966]	a [-4.575740]	</s> [-2.360970]	vault [-19.258656]	my [-4.034362]	</s> [-2.938838]	glamorize [-18.608265]	one [-5.003335]	</s> [-3.245401]	glamorize [-18.384174]	a [-4.368025]	</s> [-2.456743]	blackguard [-18.232569]	your [-7.918105]	</s> [-2.805776]	blackguard [-18.220900]	one [-4.109577]	</s> [-3.129939]	reemphasize [-18.304720]	one [-4.287140]	</s> [-3.264737]	apologize [-19.633945]	a [-4.791983]	</s> [-2.554513]	demagnetise [-19.117903]	my [-4.044061]	</s> [-3.076482]	demagnetise [-19.171070]	a [-4.932547]	</s> [-2.221105]	shove [-18.635908]	hundred [-7.654272]	</s> [-2.995854]	choke [-18.673086]	the [-4.064912]	</s> [-3.308435]	choke [-18.661531]	your [-7.754683]	</s> [-3.036041]	decentralize [-19.998802]	one [-4.727526]	</s> [-3.198764]	decentralize [-19.927158]	hundred [-9.284480]	</s> [-2.741804]	track [-19.362560]	one [-4.765376]	</s> [-3.077590]	track [-19.579353]	my [-4.556138]	</s> [-2.821087]	secularize [-18.728981]	your [-8.910913]	</s> [-2.891677]	splash [-17.723406]	my [-3.652106]	</s> [-2.884009]	overspecialize [-18.479761]	a [-4.870737]	</s> [-2.499233]	blaspheme [-19.333954]	a [-4.455164]	</s> [-2.456121]	blaspheme [-19.319084]	your [-7.808568]	</s> [-2.872940]	spearhead [-19.388264]	a [-4.397061]	</s> [-2.771994]	jeopardize [-19.046053]	your [-8.068439]	</s> [-2.987466]	jeopardize [-19.033510]	hundred [-8.065723]	</s> [-2.700180]	jeopardize [-19.040106]	a [-4.825285]	</s> [-2.601688]	revitalise [-19.038797]	my [-4.048835]	</s> [-2.909066]	provision [-19.502024]	the [-3.923339]	</s> [-3.004771]	provision [-19.416145]	one [-4.660197]	</s> [-3.109571]	pitchfork [-17.132298]	the [-4.042167]	</s> [-2.955139]	key [-17.584070]	your [-7.953913]	</s> [-2.908602]	key [-17.596701]	the [-4.330889]	</s> [-2.838429]	get [-19.507227]	your [-8.053273]	</s> [-2.881947]	sound [-18.623127]	an [-4.370117]	</s> [-2.485309]	sound [-18.662722]	hundred [-8.271392]	</s> [-2.723394]	soft [-18.131479]	hundred [-7.854110]	</s> [-2.830418]	hydroplane [-18.827078]	your [-7.477443]	</s> [-2.776286]	privilege [-19.360620]	an [-4.184826]	</s> [-2.444397]	overemphasize [-20.029665]	my [-3.887255]	</s> [-2.832776]	overemphasize [-20.043133]	one [-4.674605]	</s> [-2.962683]	rustle [-18.517410]	your [-7.877232]	</s> [-2.758647]	rustle [-18.566078]	a [-4.221782]	</s> [-2.414714]	keep [-19.009354]	an [-4.118541]	</s> [-2.337759]	undervalue [-18.497007]	your [-7.509414]	</s> [-2.775665]	undervalue [-18.420818]	hundred [-7.831575]	</s> [-2.389101]	interfere [-17.307762]	your [-8.264136]	</s> [-2.650250]	prescribe [-19.328070]	an [-4.513138]	</s> [-2.514316]	slaughter [-17.234726]	a [-4.683752]	</s> [-2.543761]	rich [-15.241535]	one [-3.075137]	</s> [-3.089367]	rich [-15.392859]	a [-3.083973]	</s> [-2.275711]	backtrack [-18.272030]	my [-3.352026]	</s> [-2.717510]	rubber [-19.203880]	hundred [-8.322068]	</s> [-2.527907]	rubber [-19.171101]	one [-5.285885]	</s> [-3.079956]
2024-12-19 06:56:58 | INFO | fairseq_cli.eval_lm | Evaluated 402 tokens in 1.1s (353.38 tokens/s)
2024-12-19 06:56:58 | INFO | fairseq_cli.eval_lm | Loss (base 2): 12.9342, Perplexity: 7826.68
