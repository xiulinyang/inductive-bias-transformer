2024-12-19 09:53:19 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX
2024-12-19 09:53:21 | INFO | fairseq_cli.eval_lm | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/grammar41exp1/7-transformer/checkpoint_best.pt', 'post_process': None, 'quiet': True, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': True, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': None, 'task': {'_name': 'language_modeling', 'data': 'data-bin/grammar41exp1/incorrect_7-dataset', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2024-12-19 09:53:21 | INFO | fairseq.tasks.language_modeling | dictionary: 1136 types
2024-12-19 09:53:21 | INFO | fairseq_cli.eval_lm | loading model(s) from checkpoints/grammar41exp1/7-transformer/checkpoint_best.pt
/workspace/artificial-languages/fairseq/fairseq/checkpoint_utils.py:340: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(f, map_location=torch.device("cpu"))
2024-12-19 09:53:24 | INFO | fairseq_cli.eval_lm | num. model params: 10,038,784
2024-12-19 09:53:24 | INFO | fairseq.data.data_utils | loaded 136 examples from: data-bin/grammar41exp1/incorrect_7-dataset/test
2024-12-19 09:53:24 | INFO | fairseq_cli.eval_lm | data-bin/grammar41exp1/incorrect_7-dataset test 1 examples
2024-12-19 09:53:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-12-19 09:53:24 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2024-12-19 09:53:24 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2024-12-19 09:53:24 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1
2024-12-19 09:53:25 | INFO | fairseq_cli.eval_lm | 0 introvert [-16.862478]	wooden [-6.596573]	</s> [-10.475848]	close [-19.147640]	my [-3.888392]	</s> [-7.882240]	intercede [-19.321781]	an [-4.685694]	</s> [-8.653515]	folk [-19.309700]	perky [-5.970688]	</s> [-12.073275]	slaughter [-18.595804]	total [-6.638544]	</s> [-9.402734]	slaughter [-18.726370]	one [-3.989707]	</s> [-9.564696]	rich [-16.917280]	stanchion [-6.807283]	</s> [-0.570239]	rich [-16.790768]	an [-8.018213]	</s> [-8.691813]	rich [-16.650379]	the [-6.587056]	</s> [-8.544950]	secularize [-18.781561]	concrete [-8.097821]	</s> [-7.772408]	secularize [-18.706921]	professional [-7.967077]	</s> [-1.810280]	secularize [-18.782528]	chief [-6.212972]	</s> [-8.706926]	racketeer [-19.994085]	elderly [-7.058887]	</s> [-8.978743]	racketeer [-20.021544]	my [-4.035124]	</s> [-8.590033]	still [-19.427948]	an [-4.003799]	</s> [-8.722511]	number [-18.039040]	late [-8.623444]	</s> [-8.030689]	revitalise [-18.750713]	contract [-8.663517]	</s> [-2.432913]	overspecialize [-18.564373]	frequent [-7.150839]	</s> [-10.551815]	weird [-17.145247]	poem [-7.847316]	</s> [-2.294005]	weird [-17.213120]	my [-7.675801]	</s> [-8.326408]	rediscover [-18.888756]	one [-4.568073]	</s> [-10.125666]	mispronounce [-18.533510]	the [-2.576950]	</s> [-9.675245]	overemphasize [-18.239330]	innocent [-6.618177]	</s> [-11.165078]	choke [-18.510574]	unwieldy [-6.196327]	</s> [-11.998363]	choke [-18.653587]	the [-3.103655]	</s> [-9.163643]	choke [-18.726341]	alienated [-7.115171]	</s> [-11.582889]	crosscheck [-19.338438]	paltry [-6.736192]	</s> [-12.429689]	crosscheck [-19.477932]	female [-6.833092]	</s> [-8.046809]	crash [-19.699459]	split [-8.932696]	</s> [-2.290937]	crash [-19.524866]	reading [-8.766112]	</s> [-2.234457]	splash [-18.818739]	blind [-7.352273]	</s> [-2.155564]	splash [-19.004803]	an [-4.563670]	</s> [-8.485648]	spearhead [-19.099880]	fickle [-7.066589]	</s> [-11.130013]	spearhead [-19.152176]	my [-3.779768]	</s> [-8.303716]	spearhead [-18.978996]	medium [-6.628160]	</s> [-1.941023]	influence [-19.271490]	trainer [-8.973140]	</s> [-1.930860]	influence [-19.097198]	impassioned [-7.747669]	</s> [-9.999912]	jeopardize [-18.783112]	fuzzy [-5.896143]	</s> [-9.785387]	sweet [-15.745999]	day [-7.448833]	</s> [-2.069904]	sweet [-15.804636]	airline [-7.425675]	</s> [-1.898109]	get [-19.912800]	the [-3.538863]	</s> [-8.954742]	dehumanize [-18.059601]	dependent [-7.454370]	</s> [-9.541600]	dehumanize [-18.035089]	easy-going [-6.760564]	</s> [-9.896528]	slap [-19.222578]	ill [-6.637132]	</s> [-9.761789]	slap [-19.234575]	harmonious [-6.254683]	</s> [-11.632335]	blaspheme [-19.279810]	live [-7.227884]	</s> [-10.609278]	blaspheme [-19.312675]	the [-3.021620]	</s> [-9.109450]	back [-17.653568]	school [-8.778292]	</s> [-2.383852]	follow [-18.692755]	gruesome [-6.939789]	</s> [-11.547976]	blackguard [-19.041910]	conventional [-7.082982]	</s> [-11.784161]	blackguard [-19.087204]	one [-4.340948]	</s> [-9.714243]	shove [-18.237246]	the [-3.387842]	</s> [-9.786209]	provision [-18.544683]	my [-4.325399]	</s> [-7.998515]	undervalue [-18.729916]	a [-3.605155]	</s> [-10.141183]	undervalue [-18.879913]	an [-3.997461]	</s> [-8.915116]	give [-18.665243]	this [-5.079412]	</s> [-9.937974]	privilege [-18.398069]	my [-3.946518]	</s> [-8.301559]	privilege [-18.271549]	ruddy [-7.462327]	</s> [-8.181311]	decentralize [-18.646042]	an [-3.947407]	</s> [-8.507197]	misbehave [-20.220917]	a [-4.046376]	</s> [-10.459815]	misbehave [-20.246691]	the [-3.636864]	</s> [-8.935053]	brutalize [-19.431528]	a [-4.089108]	</s> [-10.299591]	brutalize [-19.542725]	feisty [-6.803605]	</s> [-9.971001]	dress [-19.098650]	chubby [-6.910508]	</s> [-10.341039]	rustle [-19.633129]	sweet [-7.216158]	</s> [-2.443574]	rustle [-19.667732]	French [-6.008884]	</s> [-12.203294]	wheelbarrow [-19.871923]	value [-8.185637]	</s> [-1.741567]	wheelbarrow [-19.803658]	narrow [-6.619098]	</s> [-8.759420]	wheelbarrow [-19.913561]	my [-3.975069]	</s> [-8.332266]	bring [-19.223042]	adolescent [-7.354939]	</s> [-10.998768]	bring [-19.201921]	plastic [-7.986443]	</s> [-3.443779]	hydroplane [-18.361612]	spry [-7.030638]	</s> [-9.619741]	hydroplane [-18.384979]	perky [-6.105794]	</s> [-12.276239]	inconvenience [-19.776030]	amusing [-6.549844]	</s> [-9.883661]	inconvenience [-19.872416]	perfect [-6.072484]	</s> [-10.854323]	inconvenience [-19.957674]	sympathetic [-6.326898]	</s> [-11.896323]	inconvenience [-19.809450]	a [-3.652377]	</s> [-9.796387]	track [-19.314222]	the [-3.497162]	</s> [-8.679853]	mountebank [-18.599518]	gruesome [-6.502331]	</s> [-10.407614]	pressurize [-19.100958]	one [-4.403808]	</s> [-9.347587]	soft [-17.606571]	optimal [-5.966324]	</s> [-11.183975]	promenade [-20.090542]	metallic [-7.483768]	</s> [-9.563799]	sneeze [-18.314644]	tip [-8.770212]	</s> [-1.815514]	booby [-19.018625]	trainer [-8.835384]	</s> [-2.138508]	booby [-18.928967]	a [-3.236487]	</s> [-10.367783]	carry [-18.081829]	pessimistic [-7.163571]	</s> [-9.807468]	weasel [-18.536667]	novel [-7.731678]	</s> [-3.225822]	glamorize [-19.076248]	honey [-8.454842]	</s> [-1.821429]	headquarter [-18.850027]	simplistic [-7.034035]	</s> [-8.628582]	headquarter [-19.039425]	an [-4.967868]	</s> [-8.210725]	come [-19.019558]	tree [-8.006269]	</s> [-1.704863]	criticise [-18.246693]	long [-8.024190]	</s> [-2.654634]	criticise [-18.284742]	straight [-6.803153]	</s> [-8.424713]	key [-19.531479]	my [-4.406564]	</s> [-8.369190]	bump [-19.624554]	reflecting [-6.310442]	</s> [-11.003567]	power [-18.774626]	that [-7.793395]	</s> [-9.314108]	power [-18.813166]	the [-3.145836]	</s> [-9.123758]	intermarry [-18.088806]	narrow [-5.813099]	</s> [-9.476108]	hero [-18.238272]	juvenile [-7.643178]	</s> [-8.389490]	hero [-18.123909]	incomplete [-6.197814]	</s> [-11.643814]	vault [-19.290712]	likely [-6.144930]	</s> [-12.039230]	vault [-19.170242]	one [-3.638390]	</s> [-9.627429]	vault [-19.134890]	the [-2.810977]	</s> [-8.857821]	prescribe [-18.557762]	wooden [-5.757259]	</s> [-10.516937]	buttonhole [-20.178816]	tough [-6.010167]	</s> [-12.030414]	apologize [-19.605299]	analysis [-7.737638]	</s> [-2.442188]	medium [-15.951039]	inspection [-7.874503]	</s> [-2.027065]	medium [-15.790856]	unfolded [-5.814367]	</s> [-5.495088]	replenish [-19.104912]	rewarding [-6.623800]	</s> [-10.299726]	replenish [-18.994965]	my [-3.989001]	</s> [-8.363586]	stanchion [-18.802469]	complex [-7.022094]	</s> [-2.112880]	stanchion [-18.698004]	my [-4.020336]	</s> [-8.830655]	interfere [-19.463753]	good [-7.362258]	</s> [-3.402502]	interfere [-19.363972]	the [-3.081258]	</s> [-8.477905]	disrespect [-19.966661]	the [-3.325299]	</s> [-9.565441]	rubber [-20.057272]	whirlwind [-5.445743]	</s> [-12.013091]	demagnetise [-18.911148]	easy-going [-7.010265]	</s> [-11.536955]	misconduct [-18.944839]	useful [-9.286822]	</s> [-7.750779]	misconduct [-18.814796]	my [-3.995772]	</s> [-8.330080]	constrict [-18.116812]	bubbly [-6.694539]	</s> [-8.744277]	constrict [-18.148514]	sing [-7.676129]	</s> [-2.003140]	constrict [-18.434645]	my [-4.232552]	</s> [-8.820982]	bucket [-18.473534]	introduction [-7.888644]	</s> [-2.661562]	bucket [-18.542524]	an [-4.004271]	</s> [-8.808411]	interchange [-19.759932]	paltry [-6.232588]	</s> [-12.311230]	disembowel [-19.915857]	currency [-8.800189]	</s> [-2.000082]	disembowel [-20.055008]	a [-3.851031]	</s> [-10.700738]	square [-17.582632]	ride [-7.600040]	</s> [-2.798930]	distemper [-19.289206]	subtle [-6.913249]	</s> [-8.446247]	fall [-19.321686]	sector [-7.717752]	</s> [-2.715926]	fall [-19.351194]	the [-3.137289]	</s> [-8.829970]	pitchfork [-18.894785]	charge [-8.822493]	</s> [-2.506818]	pitchfork [-18.873867]	one [-4.033312]	</s> [-10.159016]	intellectualize [-20.205862]	one [-4.331582]	</s> [-10.124165]	run [-17.699585]	folk [-7.002101]	</s> [-0.768534]	run [-17.770041]	your [-6.258803]	</s> [-10.108521]
2024-12-19 09:53:25 | INFO | fairseq_cli.eval_lm | Evaluated 408 tokens in 0.8s (514.04 tokens/s)
2024-12-19 09:53:25 | INFO | fairseq_cli.eval_lm | Loss (base 2): 15.8063, Perplexity: 57302.81
