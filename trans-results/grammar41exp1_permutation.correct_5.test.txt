2024-12-19 08:59:53 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX
2024-12-19 08:59:55 | INFO | fairseq_cli.eval_lm | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/grammar41exp1_permutation/5-transformer/checkpoint_best.pt', 'post_process': None, 'quiet': True, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': True, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': None, 'task': {'_name': 'language_modeling', 'data': 'data-bin/grammar41exp1_permutation/correct_5-dataset', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2024-12-19 08:59:55 | INFO | fairseq.tasks.language_modeling | dictionary: 1136 types
2024-12-19 08:59:55 | INFO | fairseq_cli.eval_lm | loading model(s) from checkpoints/grammar41exp1_permutation/5-transformer/checkpoint_best.pt
/workspace/artificial-languages/fairseq/fairseq/checkpoint_utils.py:340: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(f, map_location=torch.device("cpu"))
2024-12-19 08:59:58 | INFO | fairseq_cli.eval_lm | num. model params: 10,038,784
2024-12-19 08:59:58 | INFO | fairseq.data.data_utils | loaded 133 examples from: data-bin/grammar41exp1_permutation/correct_5-dataset/test
2024-12-19 08:59:58 | INFO | fairseq_cli.eval_lm | data-bin/grammar41exp1_permutation/correct_5-dataset test 1 examples
2024-12-19 08:59:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-12-19 08:59:58 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2024-12-19 08:59:58 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2024-12-19 08:59:58 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1
2024-12-19 08:59:59 | INFO | fairseq_cli.eval_lm | 0 ginger [-13.920899]	your [-7.824324]	</s> [-2.561455]	give [-17.225594]	hundred [-7.414946]	</s> [-2.506126]	give [-17.345064]	the [-4.673845]	</s> [-2.474538]	demagnetise [-18.809881]	hundred [-9.180643]	</s> [-2.561633]	racketeer [-17.887459]	a [-5.338391]	</s> [-2.598680]	slap [-17.549200]	one [-4.233724]	</s> [-2.842396]	pitchfork [-18.551313]	an [-4.382269]	</s> [-2.813410]	carry [-18.004488]	the [-4.963620]	</s> [-2.838316]	carry [-18.188200]	a [-4.723460]	</s> [-2.805482]	rich [-17.018900]	my [-2.212363]	</s> [-2.891895]	rich [-16.690580]	a [-3.335819]	</s> [-2.651738]	follow [-17.761122]	an [-4.200695]	</s> [-2.822158]	disembowel [-18.735703]	your [-8.930337]	</s> [-3.011587]	folk [-19.422354]	your [-8.685186]	</s> [-2.880684]	apologize [-19.446409]	hundred [-9.917496]	</s> [-2.847923]	apologize [-19.483196]	an [-4.484716]	</s> [-2.923271]	brutalize [-18.329283]	an [-4.388818]	</s> [-2.968396]	hero [-19.647928]	an [-5.765064]	</s> [-3.019861]	hero [-19.640059]	a [-5.041079]	</s> [-2.899661]	slaughter [-17.571396]	my [-3.632532]	</s> [-2.878161]	smooth [-19.482361]	an [-3.991156]	</s> [-2.917411]	smooth [-19.767855]	the [-4.377297]	</s> [-2.846192]	smooth [-19.684845]	hundred [-10.085916]	</s> [-2.910344]	run [-18.276512]	hundred [-8.698183]	</s> [-2.913984]	glamorize [-19.504023]	hundred [-9.788484]	</s> [-2.934663]	burlesque [-18.890799]	the [-4.924730]	</s> [-2.901151]	disrespect [-17.971521]	my [-3.840790]	</s> [-2.894183]	disrespect [-18.006689]	an [-4.269961]	</s> [-3.012477]	listen [-18.347834]	a [-3.848212]	</s> [-2.853480]	booby [-18.373857]	one [-4.046422]	</s> [-3.087265]	jeopardize [-17.920607]	your [-8.592232]	</s> [-2.946234]	overcharge [-18.647835]	a [-5.032697]	</s> [-2.836500]	rubber [-19.034420]	an [-3.954892]	</s> [-3.033580]	undervalue [-17.987669]	one [-4.632948]	</s> [-3.154057]	undervalue [-18.174374]	the [-5.188610]	</s> [-2.865059]	inconvenience [-19.452154]	one [-4.142668]	</s> [-3.189036]	sound [-18.925705]	hundred [-9.907306]	</s> [-2.845786]	come [-18.260052]	a [-5.059227]	</s> [-2.738944]	outbalance [-18.127554]	hundred [-8.589375]	</s> [-2.800114]	crash [-19.115179]	hundred [-9.132622]	</s> [-2.936857]	crash [-18.843071]	a [-4.130955]	</s> [-2.848335]	stand [-17.762121]	one [-3.973708]	</s> [-3.214615]	dehumanize [-18.992683]	my [-4.308845]	</s> [-2.909700]	dehumanize [-18.588566]	hundred [-9.326955]	</s> [-2.870466]	dehumanize [-18.462761]	one [-4.386981]	</s> [-3.295974]	sneeze [-19.048168]	the [-4.900696]	</s> [-2.848127]	back [-16.494896]	an [-3.022909]	</s> [-2.912387]	back [-16.401896]	one [-3.039423]	</s> [-3.075521]	spearhead [-18.001078]	a [-4.201008]	</s> [-2.802268]	crosscheck [-19.384693]	the [-5.143036]	</s> [-2.850404]	crosscheck [-19.488970]	an [-4.778192]	</s> [-2.970530]	prescribe [-19.513147]	one [-4.485666]	</s> [-3.098336]	mountebank [-20.002573]	your [-7.969228]	</s> [-2.963713]	mountebank [-19.930777]	an [-4.345626]	</s> [-3.016397]	mountebank [-20.059311]	one [-3.920922]	</s> [-3.207814]	close [-17.833689]	the [-5.781449]	</s> [-2.882561]	blaspheme [-18.341413]	the [-4.365204]	</s> [-2.877657]	bucket [-18.634375]	one [-4.217268]	</s> [-3.077214]	bucket [-18.639627]	my [-3.722253]	</s> [-2.927502]	decentralize [-19.161936]	a [-4.450767]	</s> [-2.752858]	privilege [-18.925922]	hundred [-9.246778]	</s> [-2.823041]	privilege [-19.051796]	the [-4.394098]	</s> [-2.747635]	keep [-18.647842]	an [-4.614482]	</s> [-2.924932]	break [-18.176258]	a [-4.242385]	</s> [-2.820149]	influence [-18.420748]	hundred [-9.613132]	</s> [-2.926831]	influence [-18.574537]	an [-4.258495]	</s> [-2.978912]	headquarter [-18.268137]	the [-4.480940]	</s> [-2.859835]	headquarter [-18.142342]	one [-4.776311]	</s> [-3.023637]	headquarter [-18.267704]	a [-4.692929]	</s> [-2.749794]	sweet [-15.354580]	an [-2.884364]	</s> [-3.006840]	sweet [-14.939917]	my [-2.048484]	</s> [-2.892089]	get [-19.516987]	my [-3.758180]	</s> [-2.864204]	get [-19.591755]	the [-4.063273]	</s> [-2.763784]	splash [-17.453072]	one [-4.265392]	</s> [-3.015434]	commercialize [-18.938158]	a [-4.103254]	</s> [-2.789134]	overspecialize [-18.486883]	an [-4.369547]	</s> [-2.878372]	criticise [-19.579540]	a [-4.710357]	</s> [-2.716721]	gallivant [-18.783930]	an [-4.049621]	</s> [-2.883611]	intellectualize [-18.809132]	the [-4.503323]	</s> [-2.669709]	prejudice [-18.090563]	the [-5.249570]	</s> [-2.834204]	prejudice [-17.963612]	a [-4.036986]	</s> [-2.682403]	secularize [-17.922316]	one [-3.905813]	</s> [-2.878248]	secularize [-18.188913]	a [-4.458766]	</s> [-2.686091]	revitalise [-17.668804]	hundred [-9.489800]	</s> [-2.816715]	stanchion [-19.374044]	an [-4.694288]	</s> [-2.999678]	stanchion [-19.503391]	a [-4.972189]	</s> [-2.674327]	fall [-19.153187]	your [-8.693064]	</s> [-2.806061]	track [-19.184570]	an [-4.545465]	</s> [-2.873731]	shove [-19.729269]	an [-4.218256]	</s> [-2.874537]	shove [-19.676449]	a [-4.867019]	</s> [-2.723725]	overemphasize [-18.001347]	a [-4.684006]	</s> [-2.731485]	overemphasize [-17.821524]	hundred [-9.171759]	</s> [-2.789905]	medium [-13.390295]	hundred [-4.380394]	</s> [-2.824238]	medium [-13.844838]	the [-3.137096]	</s> [-2.734350]	medium [-13.536870]	a [-3.692950]	</s> [-2.686604]	weasel [-18.276443]	hundred [-7.664344]	</s> [-2.797841]	buttonhole [-19.269653]	a [-4.678679]	</s> [-2.709652]	square [-16.729374]	one [-4.311370]	</s> [-2.982822]	pressurize [-17.883459]	the [-4.815514]	</s> [-2.694435]	replenish [-19.464605]	your [-9.314483]	</s> [-2.851819]	blackguard [-17.563223]	the [-4.381344]	</s> [-2.682792]	blackguard [-17.808559]	hundred [-8.794679]	</s> [-2.728317]	bring [-18.299761]	hundred [-9.347631]	</s> [-2.803655]	bring [-18.278351]	the [-5.067321]	</s> [-2.742709]	bring [-18.198353]	one [-4.524757]	</s> [-2.988413]	still [-18.676155]	a [-4.390398]	</s> [-2.663553]	still [-18.703178]	your [-8.184316]	</s> [-2.880451]	constrict [-20.128927]	your [-8.804111]	</s> [-2.923748]	rediscover [-19.429951]	one [-4.533438]	</s> [-3.063173]	rediscover [-19.360909]	an [-4.603435]	</s> [-2.927497]	predetermine [-19.136417]	one [-4.075920]	</s> [-2.936575]	misbehave [-18.766401]	my [-3.928638]	</s> [-2.784335]	misbehave [-18.860353]	an [-3.693318]	</s> [-2.825550]	misbehave [-19.077887]	a [-4.650401]	</s> [-2.599929]	misbehave [-19.196699]	hundred [-9.244583]	</s> [-2.804625]	fertilize [-19.821133]	an [-4.765246]	</s> [-2.849660]	fertilize [-19.973013]	your [-8.932444]	</s> [-2.893024]	key [-19.484871]	my [-5.072157]	</s> [-2.869453]	commingle [-19.566328]	your [-8.084954]	</s> [-2.833145]	misconduct [-20.257641]	my [-4.125534]	</s> [-2.885247]	misconduct [-20.324492]	an [-4.337121]	</s> [-2.894764]	wheelbarrow [-18.206257]	a [-4.825994]	</s> [-2.661321]	wheelbarrow [-18.149284]	the [-4.136178]	</s> [-2.663920]	rustle [-18.780651]	one [-4.521021]	</s> [-2.866507]	rustle [-18.907911]	the [-4.538386]	</s> [-2.614184]	provision [-18.627684]	one [-4.768114]	</s> [-2.985180]	provision [-18.766556]	the [-4.586941]	</s> [-2.624799]	circumcise [-19.303080]	your [-8.858932]	</s> [-2.687557]	circumcise [-19.444618]	hundred [-9.049097]	</s> [-2.672419]	distemper [-18.154852]	hundred [-9.295467]	</s> [-2.735199]	interchange [-19.318390]	the [-5.195521]	</s> [-2.692964]	blind [-15.356990]	your [-3.517806]	</s> [-2.927321]	blind [-15.483447]	the [-3.293529]	</s> [-2.740533]
2024-12-19 08:59:59 | INFO | fairseq_cli.eval_lm | Evaluated 399 tokens in 0.7s (555.57 tokens/s)
2024-12-19 08:59:59 | INFO | fairseq_cli.eval_lm | Loss (base 2): 12.8509, Perplexity: 7387.84
