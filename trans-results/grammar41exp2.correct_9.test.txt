2024-12-19 10:49:58 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX
2024-12-19 10:50:00 | INFO | fairseq_cli.eval_lm | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/grammar41exp2/9-transformer/checkpoint_best.pt', 'post_process': None, 'quiet': True, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': True, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': None, 'task': {'_name': 'language_modeling', 'data': 'data-bin/grammar41exp2/correct_9-dataset', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2024-12-19 10:50:00 | INFO | fairseq.tasks.language_modeling | dictionary: 1136 types
2024-12-19 10:50:00 | INFO | fairseq_cli.eval_lm | loading model(s) from checkpoints/grammar41exp2/9-transformer/checkpoint_best.pt
/workspace/artificial-languages/fairseq/fairseq/checkpoint_utils.py:340: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(f, map_location=torch.device("cpu"))
2024-12-19 10:50:03 | INFO | fairseq_cli.eval_lm | num. model params: 10,038,784
2024-12-19 10:50:03 | INFO | fairseq.data.data_utils | loaded 218 examples from: data-bin/grammar41exp2/correct_9-dataset/test
2024-12-19 10:50:03 | INFO | fairseq_cli.eval_lm | data-bin/grammar41exp2/correct_9-dataset test 2 examples
2024-12-19 10:50:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-12-19 10:50:03 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2024-12-19 10:50:03 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2024-12-19 10:50:03 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1
/opt/conda/envs/art/lib/python3.9/site-packages/torch/nn/functional.py:5849: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
2024-12-19 10:50:04 | INFO | fairseq_cli.eval_lm | 1 </s> [-0.990319]	the [-13.379316]	apologize [-7.734053]	</s> [-1.226973]	the [-12.482800]	splash [-8.234884]	</s> [-0.877409]	a [-11.423837]	splash [-9.569635]	</s> [-0.838602]	your [-12.350699]	overspecialize [-9.680334]	</s> [-0.891305]	my [-12.213871]	overspecialize [-9.118929]	</s> [-0.806928]	one [-13.584157]	sneeze [-8.967545]	</s> [-0.742547]	the [-12.751760]	sneeze [-8.020234]	</s> [-0.771972]	the [-12.590909]	carry [-8.960882]	</s> [-0.953532]	my [-12.154374]	criticise [-8.667164]	</s> [-0.849928]	the [-12.358207]	criticise [-8.303023]	</s> [-0.881559]	one [-13.352655]	criticise [-9.440282]	</s> [-0.837611]	a [-11.538026]	criticise [-8.783427]	</s> [-0.906113]	the [-12.309817]	choke [-8.462188]	</s> [-1.024995]	one [-13.223772]	choke [-9.391880]	</s> [-0.964656]	my [-12.090836]	blaspheme [-9.267920]	</s> [-0.793665]	one [-13.284584]	blaspheme [-9.198517]	</s> [-0.841838]	your [-12.153872]	undervalue [-9.107210]	</s> [-1.067878]	an [-12.518193]	undervalue [-8.793463]	</s> [-1.170678]	an [-12.468632]	promenade [-8.530144]	</s> [-1.101012]	my [-12.111279]	promenade [-8.523001]	</s> [-1.093099]	the [-12.606141]	jeopardize [-7.465283]	</s> [-0.934511]	a [-11.913595]	jeopardize [-8.327406]	</s> [-0.757604]	one [-13.381729]	jeopardize [-8.109597]	</s> [-0.818526]	an [-12.655883]	jeopardize [-7.870544]	</s> [-0.886622]	hundred [-12.957516]	rustle [-8.719651]	</s> [-0.708528]	your [-12.255956]	blind [-6.160977]	</s> [-2.757760]	my [-12.167852]	decentralize [-10.139303]	</s> [-1.775899]	the [-12.662954]	decentralize [-9.445772]	</s> [-1.836190]	hundred [-12.830420]	decentralize [-8.955469]	</s> [-1.851787]	your [-12.276555]	come [-8.971453]	</s> [-0.970203]	your [-12.280491]	overcharge [-10.474834]	</s> [-0.870460]	a [-11.833887]	overcharge [-9.704729]	</s> [-0.849390]	the [-12.427193]	overcharge [-9.135868]	</s> [-0.882612]	one [-13.320100]	overcharge [-9.618137]	</s> [-0.905399]	a [-11.835926]	intercede [-8.103982]	</s> [-1.286231]	one [-13.208134]	intercede [-8.467523]	</s> [-1.241526]	hundred [-12.609782]	buttonhole [-9.170969]	</s> [-0.877560]	hundred [-12.751465]	blackguard [-8.666403]	</s> [-1.268154]	your [-12.093143]	blackguard [-9.362274]	</s> [-1.336868]	the [-12.486547]	blackguard [-8.235744]	</s> [-1.375335]	hundred [-12.537002]	pressurize [-8.276286]	</s> [-0.857702]	the [-12.287159]	pressurize [-8.248451]	</s> [-0.836840]	my [-11.976128]	pitchfork [-8.487453]	</s> [-1.063235]	one [-13.297903]	pitchfork [-8.587170]	</s> [-1.063979]	your [-12.169872]	pitchfork [-8.300051]	</s> [-1.150144]	my [-11.980085]	ginger [-9.183266]	</s> [-0.833035]	your [-12.140087]	ginger [-8.860571]	</s> [-0.818116]
2024-12-19 10:50:04 | INFO | fairseq_cli.eval_lm | 0 the [-13.271867]	glamorize [-7.125941]	</s> [-1.315806]	your [-11.843774]	glamorize [-9.439187]	</s> [-1.301066]	one [-12.970868]	fall [-8.244957]	</s> [-1.053623]	the [-12.547364]	fall [-7.927907]	</s> [-1.079104]	one [-13.287127]	key [-8.662897]	</s> [-0.901007]	your [-12.180210]	key [-8.839275]	</s> [-0.916152]	an [-12.456160]	dress [-8.862390]	</s> [-1.193333]	hundred [-12.621874]	dress [-8.334798]	</s> [-1.171066]	one [-13.259926]	weasel [-8.867495]	</s> [-0.882453]	my [-11.748645]	weasel [-8.985283]	</s> [-0.933127]	an [-12.368766]	get [-8.345293]	</s> [-0.872544]	a [-11.470259]	get [-8.150344]	</s> [-0.887840]	your [-11.865698]	keep [-9.441872]	</s> [-1.047212]	my [-12.152513]	intellectualize [-8.784293]	</s> [-0.668321]	a [-11.859178]	shove [-10.042095]	</s> [-1.060987]	your [-12.322913]	shove [-10.807344]	</s> [-1.105599]	your [-12.329878]	brutalize [-10.344225]	</s> [-1.038186]	the [-12.648297]	brutalize [-9.558142]	</s> [-1.104911]	one [-13.341563]	brutalize [-10.463253]	</s> [-1.074052]	my [-12.184036]	brutalize [-9.804342]	</s> [-1.004342]	your [-12.538961]	disrespect [-9.282255]	</s> [-1.162395]	hundred [-13.012696]	disrespect [-7.947121]	</s> [-0.954018]	a [-11.860978]	disrespect [-8.318322]	</s> [-0.931622]	one [-13.402235]	provision [-9.551580]	</s> [-1.044267]	an [-12.678618]	provision [-9.144567]	</s> [-1.079999]	the [-12.542826]	provision [-9.079151]	</s> [-1.344088]	your [-12.393746]	close [-8.091520]	</s> [-1.238976]	an [-12.631995]	close [-8.018977]	</s> [-1.164222]	the [-12.805921]	close [-7.041079]	</s> [-1.293538]	your [-12.388115]	spearhead [-9.173120]	</s> [-1.042650]	my [-12.174317]	spearhead [-8.845038]	</s> [-1.066092]	an [-12.611908]	spearhead [-9.059919]	</s> [-1.072762]	hundred [-12.603909]	bucket [-8.646696]	</s> [-1.038298]	an [-12.470519]	bucket [-8.471115]	</s> [-1.118257]	your [-12.218181]	bucket [-9.361418]	</s> [-1.150976]	hundred [-12.561905]	influence [-8.603525]	</s> [-1.118327]	the [-12.455173]	influence [-8.210242]	</s> [-1.068585]	a [-11.842915]	influence [-8.982890]	</s> [-1.116157]	an [-12.299870]	vault [-8.363628]	</s> [-0.942138]	one [-13.118973]	vault [-8.778316]	</s> [-0.981651]	hundred [-12.660315]	medium [-5.294045]	</s> [-2.480678]	a [-11.534793]	medium [-5.365059]	</s> [-2.365296]	my [-11.956063]	run [-6.401886]	</s> [-1.142053]	one [-13.294405]	intermarry [-9.224716]	</s> [-0.939084]	the [-12.566879]	intermarry [-8.711170]	</s> [-0.961237]	a [-11.743769]	secularize [-10.209769]	</s> [-0.938379]	my [-11.985600]	secularize [-10.414957]	</s> [-0.885092]	the [-12.600713]	secularize [-10.182678]	</s> [-0.945182]	one [-13.224161]	secularize [-10.354841]	</s> [-0.868189]	an [-12.382629]	booby [-8.984564]	</s> [-1.143098]	your [-12.169798]	booby [-8.896301]	</s> [-1.182239]	one [-13.218639]	booby [-8.945673]	</s> [-1.088352]	the [-12.472320]	bump [-8.375319]	</s> [-1.111810]	an [-12.372484]	bump [-8.887869]	</s> [-1.193913]	hundred [-12.353524]	back [-4.635234]	</s> [-1.321839]	one [-13.209221]	back [-5.284406]	</s> [-1.365828]	your [-12.149301]	back [-4.970746]	</s> [-1.291346]	one [-13.024589]	hero [-8.900087]	</s> [-0.935004]	my [-11.944331]	hero [-8.641167]	</s> [-0.983169]	hundred [-12.518901]	hero [-8.309883]	</s> [-0.970659]	a [-11.761113]	hero [-8.574566]	</s> [-0.968761]	the [-12.406771]	interfere [-9.139795]	</s> [-1.033689]	one [-13.174148]	interfere [-9.765135]	</s> [-0.993368]	an [-12.295464]	square [-8.756709]	</s> [-0.995646]	my [-12.167967]	square [-8.829185]	</s> [-0.880972]	an [-12.646604]	stand [-8.286211]	</s> [-0.908280]	the [-12.595464]	stand [-7.650878]	</s> [-0.967803]	my [-12.075019]	distemper [-8.589012]	</s> [-0.975932]	hundred [-12.480207]	distemper [-7.505857]	</s> [-1.026875]	one [-13.270471]	outbalance [-8.890048]	</s> [-0.914213]	a [-11.917019]	outbalance [-9.681024]	</s> [-0.946954]	hundred [-12.638824]	outbalance [-7.881232]	</s> [-0.907729]	one [-13.138389]	stanchion [-10.364893]	</s> [-1.005346]	my [-11.869779]	stanchion [-10.408741]	</s> [-1.031497]	one [-13.253686]	headquarter [-9.729954]	</s> [-1.100633]	hundred [-12.610128]	headquarter [-9.333415]	</s> [-1.040730]	an [-12.458359]	headquarter [-9.382616]	</s> [-1.091580]	your [-12.166842]	soft [-9.111614]	</s> [-1.202347]	a [-11.814414]	soft [-8.674347]	</s> [-1.290688]	your [-12.107105]	crash [-10.406527]	</s> [-0.987969]	a [-11.775709]	crash [-9.557559]	</s> [-0.992156]	your [-12.204218]	reemphasize [-9.464751]	</s> [-1.026404]	my [-11.689523]	reemphasize [-8.964132]	</s> [-1.090280]	hundred [-12.384790]	reemphasize [-7.990602]	</s> [-1.002705]	an [-12.325016]	track [-9.059484]	</s> [-1.093974]	one [-12.979798]	track [-9.047603]	</s> [-1.243411]	hundred [-12.395594]	commingle [-9.234270]	</s> [-1.236022]	the [-12.389055]	commingle [-8.749711]	</s> [-1.223099]	a [-11.875656]	commingle [-9.303776]	</s> [-1.213335]	my [-12.153494]	mispronounce [-9.145569]	</s> [-1.017153]	an [-12.605175]	mispronounce [-8.907457]	</s> [-1.052940]	a [-12.125088]	misconduct [-9.460786]	</s> [-0.816077]	hundred [-12.621985]	misconduct [-9.010864]	</s> [-0.813520]	a [-12.044158]	give [-5.488605]	</s> [-1.289537]	an [-12.541473]	give [-5.941409]	</s> [-1.102605]	one [-12.935938]	give [-5.620606]	</s> [-1.177129]	your [-12.429324]	give [-6.032792]	</s> [-1.021237]	my [-12.188976]	still [-9.843893]	</s> [-0.971702]	a [-12.093377]	still [-9.556098]	</s> [-1.106987]	an [-12.457296]	still [-9.613250]	</s> [-1.023818]	hundred [-12.390182]	slap [-9.542469]	</s> [-1.041350]	hundred [-12.420320]	dehumanize [-9.809377]	</s> [-0.990542]	the [-12.474846]	dehumanize [-9.844603]	</s> [-1.017402]	hundred [-12.405598]	number [-9.147333]	</s> [-1.324744]	a [-11.879526]	number [-9.519176]	</s> [-1.355839]	one [-13.023665]	dillydally [-9.639182]	</s> [-0.991052]	your [-12.251876]	dillydally [-9.642698]	</s> [-1.013546]	an [-12.495913]	dillydally [-9.456768]	</s> [-1.160309]	one [-12.834048]	replenish [-9.558021]	</s> [-0.951625]	the [-12.132140]	replenish [-9.041789]	</s> [-0.956158]	hundred [-12.242124]	disembowel [-7.926733]	</s> [-1.237113]	one [-12.848484]	disembowel [-8.605230]	</s> [-1.225735]	hundred [-12.260924]	listen [-8.403806]	</s> [-0.977082]	one [-12.863858]	listen [-8.764303]	</s> [-0.889420]	hundred [-12.246990]	hydroplane [-7.602382]	</s> [-0.995033]	your [-12.341438]	hydroplane [-9.751235]	</s> [-1.009499]	the [-12.335368]	revitalise [-8.798053]	</s> [-1.175857]	your [-12.255870]	revitalise [-10.152283]	</s> [-1.319476]	one [-13.073725]	revitalise [-9.466411]	</s> [-1.127552]	an [-12.146322]	inconvenience [-8.397728]	</s> [-1.035692]	one [-13.013472]	inconvenience [-8.912219]	</s> [-1.126009]	your [-12.212073]	power [-9.955889]	</s> [-1.115024]	a [-11.724577]	power [-9.138159]	</s> [-1.103211]	one [-12.896243]	prescribe [-9.598944]	</s> [-0.982738]	my [-11.927253]	prescribe [-9.204237]	</s> [-0.925588]	your [-12.397060]	slaughter [-9.068466]	</s> [-0.780406]	your [-12.358902]	crosscheck [-9.960489]	</s> [-1.180209]	an [-12.507446]	crosscheck [-9.554899]	</s> [-1.084288]	the [-12.335573]	crosscheck [-9.220376]	</s> [-1.153770]	my [-11.904916]	introvert [-8.507136]	</s> [-0.936052]	a [-11.869235]	introvert [-8.432243]	</s> [-0.923610]	the [-12.283308]	rich [-5.563589]	</s> [-2.054493]	a [-11.731756]	rich [-5.093607]	</s> [-1.863482]	a [-11.729701]	burlesque [-9.361386]	</s> [-1.032751]	an [-12.527323]	burlesque [-9.345688]	</s> [-0.928453]	hundred [-12.529943]	sound [-8.624868]	</s> [-0.774346]	a [-11.917469]	sound [-8.376868]	</s> [-0.793759]	hundred [-12.655253]	privilege [-8.844877]	</s> [-1.135005]	my [-11.933809]	privilege [-9.471828]	</s> [-1.115522]	a [-12.065947]	misbehave [-8.357883]	</s> [-1.128080]	hundred [-12.517190]	misbehave [-7.883364]	</s> [-1.113524]	your [-12.321547]	misbehave [-9.258513]	</s> [-1.145835]	one [-13.106086]	break [-8.836981]	</s> [-0.840631]	hundred [-12.701292]	commercialize [-9.523634]	</s> [-0.875444]	your [-12.350173]	commercialize [-10.755457]	</s> [-0.898135]	the [-12.468424]	rubber [-8.911618]	</s> [-1.057755]	your [-12.231884]	rubber [-9.395391]	</s> [-1.100071]	an [-12.370989]	rubber [-8.530567]	</s> [-1.091371]	my [-11.867816]	racketeer [-8.933758]	</s> [-0.923218]	one [-13.051722]	racketeer [-8.959223]	</s> [-0.969736]	an [-12.256884]	racketeer [-9.306785]	</s> [-1.003013]	the [-12.243571]	racketeer [-8.798588]	</s> [-0.946965]	an [-12.315475]	smooth [-10.032498]	</s> [-1.203029]	a [-11.715938]	smooth [-9.657850]	</s> [-1.164031]	an [-12.450265]	follow [-8.983208]	</s> [-0.887445]	a [-12.025505]	weird [-5.580360]	</s> [-2.361737]	one [-13.294437]	wheelbarrow [-9.850445]	</s> [-1.315464]	a [-11.867857]	wheelbarrow [-10.220181]	</s> [-1.293288]	hundred [-12.478378]	folk [-8.389916]	</s> [-0.946495]	the [-12.535985]	folk [-9.232170]	</s> [-0.913439]	the [-12.250278]	overemphasize [-8.801065]	</s> [-0.811669]	an [-12.498899]	rediscover [-8.801493]	</s> [-0.966077]	a [-11.916008]	rediscover [-8.911509]	</s> [-0.978967]	one [-13.246381]	rediscover [-9.334828]	</s> [-1.055574]	hundred [-12.752939]	backtrack [-8.057119]	</s> [-0.724868]	the [-12.391003]	prejudice [-8.089981]	</s> [-0.974816]	hundred [-12.622587]	prejudice [-8.755423]	</s> [-1.030293]	the [-12.532498]	demagnetise [-8.373663]	</s> [-1.159253]	hundred [-12.576636]	constrict [-8.644297]	</s> [-1.038090]	the [-12.325521]	constrict [-9.436535]	</s> [-0.969585]	an [-12.378098]	apologize [-8.781349]
2024-12-19 10:50:04 | INFO | fairseq_cli.eval_lm | Evaluated 654 tokens in 1.0s (648.63 tokens/s)
2024-12-19 10:50:04 | INFO | fairseq_cli.eval_lm | Loss (base 2): 10.7148, Perplexity: 1680.61
