2024-12-19 09:34:19 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX
2024-12-19 09:34:22 | INFO | fairseq_cli.eval_lm | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/grammar41exp2/6-transformer/checkpoint_best.pt', 'post_process': None, 'quiet': True, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': True, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': None, 'task': {'_name': 'language_modeling', 'data': 'data-bin/grammar41exp2/correct_6-dataset', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2024-12-19 09:34:22 | INFO | fairseq.tasks.language_modeling | dictionary: 1136 types
2024-12-19 09:34:22 | INFO | fairseq_cli.eval_lm | loading model(s) from checkpoints/grammar41exp2/6-transformer/checkpoint_best.pt
/workspace/artificial-languages/fairseq/fairseq/checkpoint_utils.py:340: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(f, map_location=torch.device("cpu"))
2024-12-19 09:34:25 | INFO | fairseq_cli.eval_lm | num. model params: 10,038,784
2024-12-19 09:34:25 | INFO | fairseq.data.data_utils | loaded 247 examples from: data-bin/grammar41exp2/correct_6-dataset/test
2024-12-19 09:34:25 | INFO | fairseq_cli.eval_lm | data-bin/grammar41exp2/correct_6-dataset test 2 examples
2024-12-19 09:34:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-12-19 09:34:25 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2024-12-19 09:34:25 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2024-12-19 09:34:25 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1
/opt/conda/envs/art/lib/python3.9/site-packages/torch/nn/functional.py:5849: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
2024-12-19 09:34:26 | INFO | fairseq_cli.eval_lm | 1 </s> [-0.365981]	the [-14.958046]	prejudice [-8.420863]	</s> [-0.711841]	my [-14.616145]	slap [-7.953949]	</s> [-0.697110]	one [-14.914758]	slap [-8.515013]	</s> [-0.707805]	the [-13.870607]	slap [-8.849508]	</s> [-0.803007]	your [-13.138062]	bring [-9.081489]	</s> [-0.772249]	a [-14.582285]	bring [-8.723613]	</s> [-0.803684]	one [-15.147521]	blackguard [-10.934544]	</s> [-0.861875]	an [-13.777914]	blackguard [-10.517370]	</s> [-0.786656]	hundred [-13.791171]	blackguard [-10.293727]	</s> [-0.788607]	the [-13.771794]	fall [-9.518141]	</s> [-0.739474]	my [-14.034842]	fall [-10.046917]	</s> [-0.763885]	a [-14.295090]	interfere [-10.542467]	</s> [-1.006715]	my [-14.175936]	interfere [-10.826056]	</s> [-1.084975]	one [-14.502022]	interfere [-10.505430]	</s> [-1.276639]	an [-13.803628]	interfere [-10.576196]	</s> [-1.180413]	the [-13.795966]	prescribe [-10.182436]	</s> [-0.713639]	a [-14.269997]	prescribe [-9.475744]	</s> [-0.750237]	my [-14.267411]	racketeer [-10.401754]	</s> [-0.642502]	a [-14.296316]	racketeer [-10.388121]	</s> [-0.637310]	one [-14.450571]	number [-10.459909]	</s> [-0.829582]	my [-14.294341]	number [-11.083426]	</s> [-0.849305]	your [-13.482588]	number [-10.273880]	</s> [-0.770993]	my [-14.485451]	secularize [-9.408829]	</s> [-0.731674]	one [-14.652813]	secularize [-9.374222]	</s> [-0.748562]	one [-14.658773]	commercialize [-10.331113]	</s> [-0.872508]	a [-14.398819]	commercialize [-9.684317]	</s> [-0.943119]	an [-13.960842]	commercialize [-10.088641]	</s> [-0.948417]	the [-13.832954]	jeopardize [-8.711851]	</s> [-0.770912]	an [-13.847952]	crash [-9.870703]	</s> [-0.816411]	your [-13.262596]	crash [-10.162046]	</s> [-0.851209]	a [-14.300787]	shove [-9.008949]	</s> [-1.244171]	the [-13.773593]	shove [-9.093737]	</s> [-1.170204]	your [-13.061004]	fertilize [-9.983929]	</s> [-1.016717]	my [-14.138386]	fertilize [-10.450882]	</s> [-1.048525]	an [-13.718097]	fertilize [-10.439337]	</s> [-1.028692]	one [-14.232677]	fertilize [-10.439974]	</s> [-1.082640]	one [-14.310972]	intermarry [-10.153207]	</s> [-0.706332]	an [-13.743278]	misconduct [-9.301338]	</s> [-0.865312]	the [-13.416519]	vault [-9.220849]	</s> [-0.900967]	a [-14.185028]	follow [-9.531332]	</s> [-0.950052]	an [-13.819309]	square [-8.949869]	</s> [-1.186761]	the [-13.694436]	buttonhole [-11.008900]	</s> [-0.820184]	hundred [-13.976488]	predetermine [-10.376242]	</s> [-0.549611]	a [-14.320706]	predetermine [-10.283804]	</s> [-0.568620]	hundred [-14.006505]	replenish [-9.695124]	</s> [-1.017418]	the [-13.848375]	replenish [-9.627036]	</s> [-1.136311]	a [-14.234695]	replenish [-9.142357]	</s> [-1.104372]	an [-13.814801]	replenish [-9.313448]	</s> [-1.154749]	a [-14.322384]	promenade [-9.756929]	</s> [-0.790857]	hundred [-14.076158]	promenade [-10.370691]	</s> [-0.820657]	my [-13.958651]	promenade [-10.535292]	</s> [-0.921692]	one [-14.299402]	pitchfork [-9.991550]	</s> [-0.805926]	my [-13.870737]	pitchfork [-9.952485]	</s> [-0.797759]	a [-14.288545]	pitchfork [-9.539539]	</s> [-0.742797]	my [-14.024192]	power [-10.808287]	</s> [-0.744806]	your [-13.230295]	power [-10.547134]	</s> [-0.832148]	hundred [-14.196453]	power [-10.348039]	</s> [-0.783096]	one [-14.166024]	hero [-10.003304]	</s> [-0.850374]	your [-13.267128]	hero [-10.039341]	</s> [-0.931034]	hundred [-14.309278]	hero [-10.263859]	</s> [-0.911710]	hundred [-14.205816]	misbehave [-10.277711]	</s> [-1.372711]	my [-14.244010]	misbehave [-10.865514]	</s> [-0.961070]	my [-14.166439]	undervalue [-10.413353]	</s> [-1.005462]	hundred [-13.976412]	undervalue [-9.643023]	</s> [-1.036576]	my [-14.231345]	burlesque [-10.595946]	</s> [-1.249792]	the [-14.001048]	burlesque [-10.665739]	</s> [-1.218113]	an [-13.819534]	burlesque [-9.897532]	</s> [-1.257838]	a [-14.305061]	burlesque [-10.106476]	</s> [-1.322626]	one [-14.217622]	burlesque [-10.662666]	</s> [-1.357374]	the [-13.949971]	provision [-10.268568]	</s> [-1.035191]	hundred [-14.137625]	provision [-10.357061]	</s> [-0.888267]	a [-14.195150]	dress [-9.977859]	</s> [-1.119927]	my [-13.824430]	dress [-10.535053]	</s> [-1.079461]	one [-14.433416]	slaughter [-10.125111]	</s> [-0.783453]	your [-13.303227]	slaughter [-10.060343]	</s> [-0.765554]	my [-14.113241]	slaughter [-10.356521]	</s> [-0.698863]
2024-12-19 09:34:26 | INFO | fairseq_cli.eval_lm | 0 one [-15.172286]	stand [-8.980680]	</s> [-0.868819]	an [-14.291865]	key [-8.296261]	</s> [-0.727276]	an [-13.590364]	rich [-5.147143]	</s> [-1.992838]	my [-13.875754]	rich [-5.918177]	</s> [-2.166389]	hundred [-13.477403]	rich [-4.706674]	</s> [-2.149527]	your [-13.142935]	rubber [-9.292958]	</s> [-0.915844]	a [-14.458663]	rubber [-9.160503]	</s> [-1.030090]	a [-14.374799]	weasel [-9.133968]	</s> [-0.794933]	one [-14.695737]	weasel [-9.461691]	</s> [-0.787415]	an [-13.849297]	weasel [-8.907597]	</s> [-0.809100]	the [-13.793442]	weasel [-9.717434]	</s> [-0.748822]	your [-13.373395]	choke [-9.449177]	</s> [-1.179917]	one [-14.601159]	choke [-9.235042]	</s> [-1.032256]	my [-14.098207]	choke [-10.219732]	</s> [-1.295342]	one [-14.491181]	gallivant [-9.623043]	</s> [-0.909966]	hundred [-14.111419]	gallivant [-9.249563]	</s> [-0.855950]	a [-14.450212]	gallivant [-9.263959]	</s> [-0.805923]	one [-14.594275]	break [-10.080219]	</s> [-1.143704]	a [-14.401141]	break [-9.819066]	</s> [-1.143881]	my [-14.384098]	break [-9.759624]	</s> [-1.036351]	my [-14.364990]	criticise [-11.018859]	</s> [-0.600579]	the [-14.088764]	criticise [-10.751673]	</s> [-0.603742]	your [-13.562914]	criticise [-9.480719]	</s> [-0.559196]	an [-14.130437]	disembowel [-9.849066]	</s> [-0.841947]	an [-14.153979]	introvert [-9.278942]	</s> [-0.952356]	a [-14.396281]	introvert [-9.092572]	</s> [-0.993240]	my [-14.239414]	introvert [-10.177815]	</s> [-1.077376]	a [-14.377119]	get [-9.135424]	</s> [-1.032932]	my [-14.248563]	get [-10.194040]	</s> [-1.035022]	a [-14.320668]	come [-9.095686]	</s> [-0.820432]	one [-14.554032]	come [-9.239437]	</s> [-0.955546]	your [-13.420982]	smooth [-8.923874]	</s> [-1.010509]	one [-14.409714]	smooth [-8.957983]	</s> [-0.938374]	a [-14.320334]	smooth [-9.142744]	</s> [-0.994161]	an [-13.968525]	smooth [-8.999422]	</s> [-1.032372]	the [-13.737384]	smooth [-8.818401]	</s> [-0.939800]	hundred [-14.009727]	disrespect [-10.142435]	</s> [-1.300211]	the [-13.823457]	disrespect [-9.626904]	</s> [-1.185395]	your [-13.197766]	disrespect [-10.214403]	</s> [-1.315889]	a [-14.263165]	disrespect [-9.376668]	</s> [-1.210295]	one [-14.590256]	wheelbarrow [-9.919474]	</s> [-0.688993]	an [-13.934979]	wheelbarrow [-9.724653]	</s> [-0.736102]	the [-13.924111]	influence [-9.988558]	</s> [-0.675675]	your [-13.442017]	influence [-9.629652]	</s> [-0.702446]	a [-14.409280]	influence [-9.591652]	</s> [-0.725531]	hundred [-14.128115]	commingle [-10.318152]	</s> [-0.679718]	your [-13.451027]	commingle [-9.829634]	</s> [-0.697297]	an [-13.987835]	commingle [-9.692324]	</s> [-0.693866]	an [-13.983244]	rustle [-9.780068]	</s> [-1.004332]	my [-13.998050]	rustle [-10.229269]	</s> [-1.019334]	hundred [-14.232735]	constrict [-9.361429]	</s> [-1.583846]	one [-14.493643]	constrict [-9.224154]	</s> [-1.335629]	the [-13.815548]	constrict [-10.173525]	</s> [-1.200793]	a [-14.369987]	keep [-9.378023]	</s> [-0.649066]	hundred [-14.089331]	keep [-9.423048]	</s> [-0.593943]	your [-13.392410]	keep [-9.526751]	</s> [-0.707332]	one [-14.360930]	keep [-10.440238]	</s> [-0.676546]	one [-14.348233]	run [-5.636047]	</s> [-1.454059]	your [-13.381880]	run [-5.653533]	</s> [-1.754627]	the [-14.055936]	back [-5.684011]	</s> [-1.925941]	an [-14.001646]	back [-5.572383]	</s> [-1.903221]	your [-13.447362]	back [-5.875884]	</s> [-1.906695]	my [-14.278999]	dillydally [-11.134748]	</s> [-0.802590]	an [-13.860408]	headquarter [-9.531370]	</s> [-0.700587]	my [-14.134171]	headquarter [-10.610836]	</s> [-0.653481]	your [-13.522158]	headquarter [-9.701708]	</s> [-0.676989]	an [-13.985516]	mispronounce [-8.983736]	</s> [-1.058912]	one [-14.362008]	mispronounce [-9.183060]	</s> [-1.079520]	your [-13.349352]	mispronounce [-8.835619]	</s> [-1.080570]	the [-13.953763]	still [-10.150629]	</s> [-1.246450]	one [-14.610894]	still [-10.175835]	</s> [-1.115300]	my [-13.992052]	still [-10.638201]	</s> [-1.135411]	one [-14.481730]	apologize [-10.467363]	</s> [-0.981662]	your [-13.266561]	apologize [-10.103298]	</s> [-0.929295]	hundred [-14.280199]	apologize [-10.234722]	</s> [-0.974282]	an [-14.054365]	apologize [-9.640215]	</s> [-0.869880]	a [-14.530160]	apologize [-10.201095]	</s> [-0.837636]	the [-14.117704]	decentralize [-10.523993]	</s> [-0.549297]	a [-14.454508]	booby [-9.047375]	</s> [-0.737085]	one [-14.631087]	booby [-9.353664]	</s> [-0.707878]	the [-14.133144]	booby [-9.330229]	</s> [-0.662718]	my [-14.125240]	overemphasize [-10.625444]	</s> [-0.897839]	your [-13.280205]	overemphasize [-10.046327]	</s> [-0.803804]	one [-14.547600]	overemphasize [-10.259983]	</s> [-0.743075]	an [-14.172654]	overemphasize [-10.000900]	</s> [-0.791291]	one [-14.468684]	soft [-9.542111]	</s> [-1.101129]	your [-13.452006]	soft [-9.465723]	</s> [-1.029058]	the [-13.883977]	soft [-9.435479]	</s> [-1.037873]	a [-14.404653]	interchange [-10.215858]	</s> [-0.574993]	an [-14.224214]	interchange [-9.947677]	</s> [-0.624332]	hundred [-14.356533]	interchange [-10.212663]	</s> [-0.685113]	your [-13.516810]	stanchion [-10.158562]	</s> [-1.285090]	one [-14.622214]	bump [-11.353512]	</s> [-1.084000]	hundred [-14.477406]	bump [-10.890650]	</s> [-1.269117]	one [-14.778064]	ginger [-10.501331]	</s> [-1.411037]	my [-14.071390]	ginger [-11.129935]	</s> [-1.361070]	a [-14.482788]	ginger [-10.729704]	</s> [-1.260918]	the [-14.000466]	crosscheck [-9.750257]	</s> [-0.878509]	one [-14.711288]	crosscheck [-9.573778]	</s> [-0.877919]	my [-14.186868]	crosscheck [-9.924621]	</s> [-0.903516]	hundred [-14.123650]	track [-9.513048]	</s> [-0.694418]	your [-13.383652]	track [-9.668554]	</s> [-0.687223]	your [-13.454275]	sneeze [-11.626743]	</s> [-0.866916]	my [-14.212033]	sneeze [-11.612521]	</s> [-0.943846]	the [-13.836449]	sneeze [-10.889144]	</s> [-0.953671]	a [-14.350389]	give [-6.697760]	</s> [-1.621816]	your [-13.515821]	give [-6.695808]	</s> [-1.778751]	my [-14.241692]	reemphasize [-10.828207]	</s> [-1.112455]	the [-13.703506]	reemphasize [-10.689216]	</s> [-1.087321]	an [-14.063428]	brutalize [-10.231562]	</s> [-0.896472]	one [-14.402570]	brutalize [-10.575354]	</s> [-1.018878]	the [-13.732940]	backtrack [-10.364193]	</s> [-1.327263]	your [-13.339302]	backtrack [-10.036623]	</s> [-1.007684]	one [-14.396283]	splash [-10.336889]	</s> [-0.770599]	an [-14.207732]	splash [-10.661936]	</s> [-0.845752]	a [-14.552052]	splash [-10.551151]	</s> [-0.883483]	hundred [-14.118990]	medium [-5.169423]	</s> [-2.515679]	a [-14.197082]	medium [-5.095746]	</s> [-2.382207]	your [-13.532719]	medium [-4.880152]	</s> [-2.321471]	an [-13.938538]	outbalance [-9.040693]	</s> [-1.010124]	hundred [-14.005197]	outbalance [-8.847669]	</s> [-1.151669]	a [-14.241597]	glamorize [-9.299070]	</s> [-1.713169]	an [-14.000451]	glamorize [-9.781591]	</s> [-1.540913]	your [-13.400599]	spearhead [-10.754751]	</s> [-0.642272]	the [-13.643273]	spearhead [-10.952557]	</s> [-0.606689]	a [-14.236707]	spearhead [-10.236417]	</s> [-0.693839]	one [-14.396008]	spearhead [-11.144207]	</s> [-0.719280]	hundred [-14.034699]	blaspheme [-8.924919]	</s> [-0.886420]	your [-13.454681]	blaspheme [-9.784540]	</s> [-0.958961]	your [-13.507956]	intercede [-9.281353]	</s> [-0.958117]	my [-14.164705]	intercede [-10.272617]	</s> [-1.033240]	the [-13.679270]	intercede [-9.621346]	</s> [-1.026399]	a [-14.377338]	intercede [-9.104349]	</s> [-1.097630]	hundred [-13.938214]	intercede [-9.513924]	</s> [-1.287687]	one [-14.506706]	distemper [-9.879954]	</s> [-0.865518]	the [-13.930492]	distemper [-9.806502]	</s> [-0.780584]	hundred [-14.165614]	distemper [-9.130404]	</s> [-0.777023]	an [-14.318222]	rediscover [-9.371365]	</s> [-0.778149]	hundred [-14.155612]	listen [-10.040472]	</s> [-1.231151]	one [-14.412011]	listen [-10.333225]	</s> [-1.414404]	your [-13.576497]	sweet [-5.535614]	</s> [-2.365147]	the [-13.817194]	bucket [-10.175896]	</s> [-0.701403]	a [-14.570577]	bucket [-9.449856]	</s> [-0.771881]	your [-13.543222]	bucket [-10.283438]	</s> [-0.782243]	a [-14.716882]	demagnetise [-9.653325]	</s> [-0.917519]	the [-13.963021]	demagnetise [-10.370727]	</s> [-0.860313]	my [-14.101798]	close [-10.260820]	</s> [-0.883226]	hundred [-13.989307]	close [-10.104442]	</s> [-0.868293]	your [-13.495289]	close [-10.532413]	</s> [-0.868818]	the [-13.835016]	dehumanize [-10.998863]	</s> [-1.032007]	hundred [-13.991884]	folk [-9.881137]	</s> [-0.891201]	an [-14.180486]	folk [-9.303595]	</s> [-1.150282]	the [-13.837094]	folk [-9.645931]	</s> [-0.935780]	one [-14.372585]	folk [-9.453279]	</s> [-0.925525]	an [-14.183814]	blind [-5.386053]	</s> [-2.760153]	one [-14.592199]	carry [-10.646553]	</s> [-0.925600]	hundred [-14.331235]	carry [-10.115337]	</s> [-1.059616]	an [-14.228664]	carry [-10.264479]	</s> [-0.903269]	the [-13.737817]	carry [-10.177789]	</s> [-0.930689]	one [-14.346478]	inconvenience [-10.099020]	</s> [-0.837389]	a [-14.415543]	inconvenience [-9.951863]	</s> [-0.787630]	the [-13.738192]	mountebank [-9.714893]	</s> [-1.080329]	hundred [-14.074770]	mountebank [-9.621165]	</s> [-1.235483]	one [-14.406062]	mountebank [-10.159273]	</s> [-1.188333]	your [-13.643143]	pressurize [-9.960353]	</s> [-0.882746]	a [-14.577692]	pressurize [-9.964857]	</s> [-0.944592]	hundred [-14.138618]	pressurize [-10.732780]	</s> [-0.852545]	one [-14.555055]	pressurize [-10.960247]	</s> [-0.745971]	hundred [-14.267793]	circumcise [-10.067466]	</s> [-0.948933]	an [-14.181174]	overcharge [-10.170231]	</s> [-0.924486]	one [-14.504676]	prejudice [-10.326034]
2024-12-19 09:34:26 | INFO | fairseq_cli.eval_lm | Evaluated 741 tokens in 1.0s (736.38 tokens/s)
2024-12-19 09:34:26 | INFO | fairseq_cli.eval_lm | Loss (base 2): 11.8961, Perplexity: 3811.48
