2024-12-19 09:10:01 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX
2024-12-19 09:10:04 | INFO | fairseq_cli.eval_lm | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/grammar41exp2/5-transformer/checkpoint_best.pt', 'post_process': None, 'quiet': True, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': True, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': None, 'task': {'_name': 'language_modeling', 'data': 'data-bin/grammar41exp2/correct_5-dataset', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2024-12-19 09:10:04 | INFO | fairseq.tasks.language_modeling | dictionary: 1136 types
2024-12-19 09:10:04 | INFO | fairseq_cli.eval_lm | loading model(s) from checkpoints/grammar41exp2/5-transformer/checkpoint_best.pt
/workspace/artificial-languages/fairseq/fairseq/checkpoint_utils.py:340: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(f, map_location=torch.device("cpu"))
2024-12-19 09:10:06 | INFO | fairseq_cli.eval_lm | num. model params: 10,038,784
2024-12-19 09:10:06 | INFO | fairseq.data.data_utils | loaded 207 examples from: data-bin/grammar41exp2/correct_5-dataset/test
2024-12-19 09:10:06 | INFO | fairseq_cli.eval_lm | data-bin/grammar41exp2/correct_5-dataset test 2 examples
2024-12-19 09:10:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-12-19 09:10:06 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2024-12-19 09:10:06 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2024-12-19 09:10:06 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1
/opt/conda/envs/art/lib/python3.9/site-packages/torch/nn/functional.py:5849: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
2024-12-19 09:10:08 | INFO | fairseq_cli.eval_lm | 1 </s> [-0.912213]	one [-15.276138]	promenade [-8.915619]	</s> [-1.051336]	my [-12.594699]	power [-9.632835]	</s> [-0.756244]	one [-13.253511]	power [-9.816723]	</s> [-0.836873]	one [-13.155996]	constrict [-9.145965]	</s> [-0.730158]	an [-12.877043]	constrict [-9.877387]	</s> [-0.700612]	the [-11.814899]	rediscover [-10.052024]	</s> [-0.625487]	your [-12.815233]	rediscover [-9.955702]	</s> [-0.669680]	one [-12.911060]	rediscover [-10.045280]	</s> [-0.763412]	your [-12.603539]	blind [-4.960788]	</s> [-2.480803]	the [-11.751245]	blind [-5.255455]	</s> [-2.695967]	your [-12.617880]	predetermine [-8.767966]	</s> [-0.990871]	your [-12.532577]	misbehave [-9.783766]	</s> [-0.861016]	the [-11.834397]	misbehave [-10.362167]	</s> [-0.852947]	a [-12.867175]	fertilize [-10.129645]	</s> [-0.842911]	hundred [-13.207639]	fertilize [-9.700405]	</s> [-0.819858]	the [-11.877306]	key [-10.417936]	</s> [-0.771372]	a [-12.572536]	key [-10.661493]	</s> [-0.926461]	my [-12.285860]	key [-10.346483]	</s> [-0.960786]	your [-12.236505]	key [-11.135206]	</s> [-0.845877]	my [-12.227383]	commingle [-8.993678]	</s> [-0.707001]	the [-11.752884]	commingle [-8.948156]	</s> [-0.675804]	a [-12.621614]	commingle [-9.221668]	</s> [-0.692773]	an [-12.432415]	misconduct [-9.625719]	</s> [-0.750984]	my [-12.481344]	misconduct [-9.500368]	</s> [-0.764090]	one [-12.577570]	wheelbarrow [-9.806619]	</s> [-0.719254]	hundred [-13.045583]	wheelbarrow [-10.065557]	</s> [-0.793929]	the [-11.612389]	rustle [-10.708400]	</s> [-0.718478]	my [-12.168904]	rustle [-10.152355]	</s> [-0.799696]	hundred [-12.786464]	rustle [-9.882396]	</s> [-0.818985]	hundred [-12.924973]	provision [-9.267995]	</s> [-0.623250]	the [-11.668118]	circumcise [-9.190705]	</s> [-0.770689]	your [-11.742270]	circumcise [-8.973280]	</s> [-0.818177]	a [-11.746385]	circumcise [-9.264347]	</s> [-0.818854]	the [-11.325616]	interchange [-9.549057]	</s> [-0.985598]	hundred [-12.605622]	interchange [-8.932785]	</s> [-0.956346]	my [-11.640113]	intermarry [-10.155791]	</s> [-1.004651]
2024-12-19 09:10:08 | INFO | fairseq_cli.eval_lm | 0 an [-12.250872]	ginger [-8.811517]	</s> [-0.858631]	your [-11.185815]	ginger [-8.833976]	</s> [-0.832163]	one [-11.910803]	give [-6.198976]	</s> [-0.849817]	the [-11.124418]	give [-6.598350]	</s> [-0.958971]	hundred [-12.889474]	give [-6.596281]	</s> [-0.873462]	my [-11.721666]	demagnetise [-10.577330]	</s> [-0.895181]	hundred [-12.887761]	racketeer [-9.677244]	</s> [-0.772209]	a [-12.152930]	slap [-9.493306]	</s> [-0.963273]	one [-12.014087]	slap [-9.240844]	</s> [-0.959135]	an [-12.221631]	slap [-9.240976]	</s> [-0.965356]	the [-11.237123]	pitchfork [-9.519461]	</s> [-0.602056]	an [-12.426950]	pitchfork [-9.381229]	</s> [-0.556891]	one [-12.136711]	carry [-9.243298]	</s> [-0.906026]	an [-12.405873]	carry [-9.599632]	</s> [-0.954443]	the [-11.109387]	rich [-5.029670]	</s> [-2.797998]	your [-11.971140]	follow [-9.895926]	</s> [-0.646310]	an [-12.512062]	follow [-10.029510]	</s> [-0.704814]	the [-11.254128]	disembowel [-9.532986]	</s> [-0.803398]	an [-12.370996]	disembowel [-9.980370]	</s> [-0.736762]	your [-11.698131]	disembowel [-10.822763]	</s> [-0.691046]	an [-12.269461]	folk [-9.369751]	</s> [-0.806075]	hundred [-12.953149]	apologize [-9.316982]	</s> [-0.841927]	an [-12.154322]	apologize [-9.774981]	</s> [-0.863564]	one [-12.287650]	apologize [-9.203996]	</s> [-0.814004]	my [-11.982473]	brutalize [-9.895862]	</s> [-0.674228]	a [-11.983799]	brutalize [-10.053239]	</s> [-0.722613]	one [-12.139938]	brutalize [-9.959611]	</s> [-0.719462]	hundred [-12.865633]	brutalize [-9.647476]	</s> [-0.743625]	your [-11.585064]	hero [-9.864584]	</s> [-0.771420]	hundred [-12.576913]	hero [-9.729744]	</s> [-0.706360]	one [-12.145755]	slaughter [-9.941790]	</s> [-1.338729]	hundred [-12.743450]	smooth [-10.157177]	</s> [-1.016401]	a [-11.516187]	run [-6.478559]	</s> [-1.149518]	hundred [-12.390151]	run [-6.093452]	</s> [-1.162678]	an [-11.887060]	glamorize [-9.693897]	</s> [-1.088381]	a [-11.461700]	glamorize [-9.876729]	</s> [-1.036560]	your [-11.195580]	burlesque [-9.941332]	</s> [-1.527002]	a [-11.564633]	burlesque [-9.552367]	</s> [-1.594825]	an [-11.673492]	disrespect [-10.011330]	</s> [-0.924241]	one [-11.868999]	reemphasize [-9.402643]	</s> [-0.731645]	a [-11.645811]	listen [-9.790657]	</s> [-0.949690]	your [-11.105311]	booby [-9.101461]	</s> [-0.908631]	an [-11.839873]	booby [-9.406636]	</s> [-0.889778]	one [-11.875851]	booby [-9.171058]	</s> [-0.894615]	my [-11.580111]	jeopardize [-9.234638]	</s> [-1.485051]	an [-11.752164]	jeopardize [-9.605795]	</s> [-1.187910]	the [-10.991286]	overcharge [-9.241585]	</s> [-0.679241]	an [-11.966750]	overcharge [-9.435072]	</s> [-0.737836]	the [-11.115211]	rubber [-10.063728]	</s> [-1.110262]	hundred [-12.087156]	rubber [-9.678892]	</s> [-1.155167]	hundred [-12.087266]	undervalue [-8.666169]	</s> [-0.761221]	my [-11.486056]	undervalue [-9.684162]	</s> [-0.759353]	an [-11.766061]	inconvenience [-10.668580]	</s> [-0.809986]	an [-11.869156]	hydroplane [-9.332571]	</s> [-0.807254]	a [-11.461844]	hydroplane [-10.189858]	</s> [-0.849849]	hundred [-12.183566]	sound [-8.812759]	</s> [-0.664563]	a [-11.652916]	dillydally [-10.088957]	</s> [-0.907067]	my [-11.685978]	come [-9.451639]	</s> [-0.755057]	one [-12.017546]	outbalance [-9.557972]	</s> [-0.894312]	a [-11.679606]	outbalance [-9.930933]	</s> [-0.878238]	the [-10.650999]	crash [-10.522335]	</s> [-1.032152]	my [-11.562990]	crash [-9.904184]	</s> [-1.073367]	your [-11.291422]	crash [-9.749473]	</s> [-1.028443]	a [-11.524078]	stand [-10.123557]	</s> [-1.153974]	hundred [-12.168605]	stand [-8.240254]	</s> [-1.077707]	my [-11.844919]	dehumanize [-9.624304]	</s> [-0.782588]	hundred [-12.214313]	dehumanize [-9.460191]	</s> [-0.787049]	your [-11.434077]	dehumanize [-10.620273]	</s> [-0.741254]	one [-12.007818]	sneeze [-9.227015]	</s> [-0.792943]	an [-11.869777]	sneeze [-9.641422]	</s> [-0.715416]	the [-10.857704]	back [-5.060129]	</s> [-1.317331]	hundred [-11.964433]	back [-4.657430]	</s> [-1.332378]	my [-11.376375]	back [-5.579772]	</s> [-1.300529]	an [-11.671062]	spearhead [-11.108107]	</s> [-0.964034]	hundred [-12.354527]	spearhead [-10.375021]	</s> [-0.819846]	one [-12.043143]	spearhead [-10.841757]	</s> [-0.852269]	my [-11.743316]	intercede [-9.072116]	</s> [-0.904558]	the [-11.077909]	intercede [-9.383430]	</s> [-0.918695]	an [-11.800632]	intercede [-9.230627]	</s> [-0.997849]	your [-11.300513]	crosscheck [-9.571766]	</s> [-0.768230]	an [-11.803830]	crosscheck [-9.183685]	</s> [-0.747769]	hundred [-12.003994]	crosscheck [-8.724575]	</s> [-0.821496]	an [-11.545547]	prescribe [-9.908034]	</s> [-0.951424]	my [-11.474089]	introvert [-9.655608]	</s> [-0.973884]	the [-10.691431]	mountebank [-10.012728]	</s> [-0.915485]	an [-11.763091]	mountebank [-9.589514]	</s> [-1.014838]	one [-11.941894]	close [-9.940833]	</s> [-0.768845]	my [-11.392937]	close [-9.965419]	</s> [-0.726073]	a [-11.682755]	backtrack [-9.779160]	</s> [-0.957709]	an [-12.001813]	backtrack [-9.470928]	</s> [-1.003940]	your [-11.479974]	blaspheme [-8.505243]	</s> [-0.967829]	an [-11.995005]	interfere [-9.950647]	</s> [-0.984772]	your [-11.401585]	bucket [-9.826858]	</s> [-1.044713]	the [-10.962524]	bucket [-9.616343]	</s> [-1.016813]	hundred [-12.215117]	bucket [-9.510199]	</s> [-1.111223]	my [-11.206949]	decentralize [-10.453519]	</s> [-0.914179]	your [-11.330744]	privilege [-9.614758]	</s> [-0.912902]	an [-11.940823]	keep [-9.195951]	</s> [-0.836851]	my [-11.548662]	keep [-8.633899]	</s> [-0.872601]	a [-11.529103]	keep [-9.777518]	</s> [-0.822672]	one [-11.756453]	soft [-10.062109]	</s> [-1.449309]	my [-11.551640]	break [-9.603124]	</s> [-0.821181]	the [-10.768282]	break [-9.483896]	</s> [-0.792211]	one [-11.814582]	break [-9.845579]	</s> [-0.833413]	one [-11.674134]	influence [-10.868631]	</s> [-1.013178]	hundred [-11.921844]	influence [-9.783443]	</s> [-0.952532]	an [-11.856062]	influence [-10.161858]	</s> [-1.014347]	hundred [-12.005600]	headquarter [-8.862149]	</s> [-0.878040]	your [-11.202609]	headquarter [-9.778534]	</s> [-0.840799]	my [-11.279801]	headquarter [-9.660365]	</s> [-0.871050]	my [-11.260710]	sweet [-5.606101]	</s> [-1.862118]	the [-10.528763]	get [-8.769486]	</s> [-1.129893]	one [-11.552352]	get [-9.159263]	</s> [-1.103832]	hundred [-11.783772]	splash [-9.671644]	</s> [-0.715155]	my [-11.492205]	splash [-9.805472]	</s> [-0.808160]	an [-12.107756]	commercialize [-9.952854]	</s> [-1.186225]	one [-12.001098]	overspecialize [-8.715750]	</s> [-0.675634]	hundred [-11.972793]	overspecialize [-8.618155]	</s> [-0.691776]	my [-11.503496]	overspecialize [-9.165406]	</s> [-0.668900]	an [-11.612505]	criticise [-9.972937]	</s> [-0.739365]	your [-11.392195]	criticise [-9.378578]	</s> [-0.704758]	the [-10.730451]	criticise [-9.456206]	</s> [-0.802936]	my [-11.228023]	gallivant [-9.495258]	</s> [-0.592730]	your [-11.343227]	gallivant [-9.482655]	</s> [-0.556588]	the [-11.052379]	gallivant [-9.831846]	</s> [-0.535012]	a [-11.692158]	gallivant [-9.997468]	</s> [-0.555446]	my [-11.587107]	intellectualize [-10.171957]	</s> [-0.953443]	the [-10.947168]	intellectualize [-10.100830]	</s> [-0.906641]	a [-11.572205]	number [-9.904802]	</s> [-0.959206]	one [-12.071993]	number [-9.548368]	</s> [-0.921953]	an [-11.916060]	prejudice [-10.410076]	</s> [-1.123974]	the [-10.755023]	prejudice [-10.826518]	</s> [-1.167876]	my [-11.643034]	choke [-10.753402]	</s> [-0.761721]	a [-11.547011]	secularize [-9.809258]	</s> [-0.727726]	hundred [-12.323647]	secularize [-8.983158]	</s> [-0.628238]	an [-12.023010]	secularize [-9.518277]	</s> [-0.589293]	hundred [-12.431253]	revitalise [-8.599771]	</s> [-0.716849]	a [-12.048464]	revitalise [-9.820517]	</s> [-0.732780]	an [-12.028780]	revitalise [-9.968989]	</s> [-0.778141]	the [-10.855792]	stanchion [-10.072770]	</s> [-0.818750]	my [-11.791436]	fall [-9.444553]	</s> [-0.705331]	hundred [-12.265555]	fall [-8.536825]	</s> [-0.693230]	a [-11.767762]	fall [-10.125571]	</s> [-0.696737]	an [-12.012531]	track [-10.054006]	</s> [-0.835078]	my [-11.726179]	track [-9.962404]	</s> [-0.864400]	the [-10.742599]	track [-10.260603]	</s> [-0.879805]	the [-10.717569]	overemphasize [-10.384607]	</s> [-1.212624]	a [-11.573849]	overemphasize [-10.730989]	</s> [-1.272519]	hundred [-12.034819]	overemphasize [-10.058413]	</s> [-1.405289]	your [-11.309438]	medium [-5.998328]	</s> [-5.184545]	my [-11.525750]	weasel [-10.724825]	</s> [-1.110050]	your [-11.527593]	weasel [-10.742563]	</s> [-1.193954]	a [-11.559739]	weasel [-10.481743]	</s> [-1.267684]	an [-11.457131]	weasel [-10.311337]	</s> [-1.178033]	my [-11.521839]	buttonhole [-9.544805]	</s> [-0.664136]	the [-10.864472]	square [-9.885486]	</s> [-0.938364]	a [-12.064543]	pressurize [-9.003714]	</s> [-0.673852]	one [-11.811624]	pressurize [-8.569277]	</s> [-0.675115]	a [-11.826171]	mispronounce [-9.800671]	</s> [-0.471163]	one [-12.025966]	replenish [-9.065643]	</s> [-1.229678]	hundred [-12.104666]	replenish [-8.365511]	</s> [-1.143021]	a [-12.111624]	blackguard [-9.931552]	</s> [-0.645916]	hundred [-12.032534]	blackguard [-8.885534]	</s> [-0.663121]	my [-11.655010]	blackguard [-9.810843]	</s> [-0.694055]	one [-11.877608]	blackguard [-9.641527]	</s> [-0.689284]	the [-10.699471]	bring [-8.982977]	</s> [-0.651699]	hundred [-12.100331]	bring [-8.900002]	</s> [-0.627684]	an [-11.916695]	still [-9.467528]	</s> [-0.908503]	one [-11.914837]	still [-9.329520]	</s> [-0.904244]	the [-10.753938]	still [-10.004124]	</s> [-0.913646]	a [-11.815665]	promenade [-10.845027]
2024-12-19 09:10:08 | INFO | fairseq_cli.eval_lm | Evaluated 621 tokens in 0.8s (791.07 tokens/s)
2024-12-19 09:10:08 | INFO | fairseq_cli.eval_lm | Loss (base 2): 10.6809, Perplexity: 1641.57
