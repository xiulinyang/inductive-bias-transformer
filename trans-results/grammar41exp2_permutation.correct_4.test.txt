2024-12-19 08:41:33 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX
2024-12-19 08:41:35 | INFO | fairseq_cli.eval_lm | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/grammar41exp2_permutation/4-transformer/checkpoint_best.pt', 'post_process': None, 'quiet': True, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': True, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': None, 'task': {'_name': 'language_modeling', 'data': 'data-bin/grammar41exp2_permutation/correct_4-dataset', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2024-12-19 08:41:35 | INFO | fairseq.tasks.language_modeling | dictionary: 1136 types
2024-12-19 08:41:35 | INFO | fairseq_cli.eval_lm | loading model(s) from checkpoints/grammar41exp2_permutation/4-transformer/checkpoint_best.pt
/workspace/artificial-languages/fairseq/fairseq/checkpoint_utils.py:340: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(f, map_location=torch.device("cpu"))
2024-12-19 08:41:38 | INFO | fairseq_cli.eval_lm | num. model params: 10,038,784
2024-12-19 08:41:38 | INFO | fairseq.data.data_utils | loaded 209 examples from: data-bin/grammar41exp2_permutation/correct_4-dataset/test
2024-12-19 08:41:38 | INFO | fairseq_cli.eval_lm | data-bin/grammar41exp2_permutation/correct_4-dataset test 2 examples
2024-12-19 08:41:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-12-19 08:41:38 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2024-12-19 08:41:38 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2024-12-19 08:41:38 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1
/opt/conda/envs/art/lib/python3.9/site-packages/torch/nn/functional.py:5849: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
2024-12-19 08:41:39 | INFO | fairseq_cli.eval_lm | 1 </s> [-1.112004]	undervalue [-18.455315]	my [-8.284058]	</s> [-3.269042]	undervalue [-18.013964]	your [-8.944485]	</s> [-1.991745]	overspecialize [-18.527203]	your [-9.979218]	</s> [-2.494785]	overspecialize [-18.439505]	a [-9.170772]	</s> [-0.985981]	promenade [-18.755779]	my [-8.608248]	</s> [-3.432382]	promenade [-18.654190]	your [-9.422035]	</s> [-3.005870]	promenade [-18.428448]	a [-8.229737]	</s> [-1.352653]	still [-16.466070]	a [-8.539744]	</s> [-0.639384]	slap [-17.898811]	the [-7.486701]	</s> [-2.076448]	slap [-18.177671]	your [-8.168694]	</s> [-3.313610]	mispronounce [-17.898443]	a [-9.062527]	</s> [-1.517106]	mispronounce [-17.982862]	your [-9.294534]	</s> [-2.774776]	mispronounce [-18.049141]	hundred [-9.095984]	</s> [-2.640312]	mountebank [-17.693304]	my [-7.533823]	</s> [-2.963434]	bucket [-17.257166]	an [-8.807900]	</s> [-1.184463]	bucket [-17.375307]	the [-8.246047]	</s> [-1.948722]	pressurize [-18.089575]	hundred [-8.966615]	</s> [-3.215640]	pressurize [-17.922203]	an [-9.056297]	</s> [-1.651722]	key [-19.435452]	an [-8.947556]	</s> [-1.027446]	key [-19.332630]	my [-8.564903]	</s> [-3.196928]	pitchfork [-17.749044]	your [-8.903077]	</s> [-2.975143]	pitchfork [-17.780573]	a [-8.576267]	</s> [-1.755983]	influence [-17.898703]	an [-9.040908]	</s> [-1.650229]	influence [-18.029226]	your [-8.347429]	</s> [-3.138154]	influence [-18.014751]	one [-9.304010]	</s> [-3.044362]	dress [-17.960051]	hundred [-8.776008]	</s> [-3.126850]	dress [-18.146597]	your [-8.824384]	</s> [-3.030286]	dress [-17.959818]	a [-9.011210]	</s> [-0.802118]	crosscheck [-17.225025]	one [-10.148386]	</s> [-3.098665]	crosscheck [-17.332602]	my [-8.759377]	</s> [-2.629261]	intercede [-18.315218]	one [-9.544943]	</s> [-3.083606]	intercede [-18.361221]	hundred [-8.920673]	</s> [-2.328362]	reemphasize [-18.224001]	one [-8.870703]	</s> [-3.238821]	reemphasize [-18.273094]	a [-8.584045]	</s> [-1.729863]	reemphasize [-18.107714]	hundred [-8.884172]	</s> [-3.208971]	reemphasize [-18.100670]	an [-8.679713]	</s> [-1.936975]	reemphasize [-18.192699]	my [-7.950650]	</s> [-3.607176]	reemphasize [-18.212440]	your [-8.770609]	</s> [-3.410265]
2024-12-19 08:41:39 | INFO | fairseq_cli.eval_lm | 0 brutalize [-14.449886]	hundred [-7.135005]	</s> [-2.236579]	brutalize [-17.972845]	your [-7.715176]	</s> [-2.104238]	brutalize [-18.019211]	a [-8.572392]	</s> [-0.775907]	racketeer [-18.020191]	your [-8.948697]	</s> [-2.752930]	disembowel [-17.641445]	your [-7.982029]	</s> [-2.688832]	slaughter [-18.865612]	hundred [-8.785346]	</s> [-2.823047]	slaughter [-18.707924]	my [-7.757384]	</s> [-3.232110]	slaughter [-18.739803]	a [-8.339339]	</s> [-0.734787]	slaughter [-18.560917]	one [-9.270513]	</s> [-2.733746]	headquarter [-18.054127]	my [-7.713699]	</s> [-2.971961]	headquarter [-18.224930]	a [-8.383869]	</s> [-1.120328]	apologize [-18.169407]	your [-8.542449]	</s> [-2.821016]	apologize [-18.241186]	a [-8.166127]	</s> [-0.760361]	apologize [-18.433197]	an [-8.340314]	</s> [-0.929115]	splash [-18.106735]	your [-8.570086]	</s> [-2.531718]	splash [-17.888632]	an [-8.750553]	</s> [-1.160282]	fertilize [-17.607218]	an [-8.457402]	</s> [-1.686885]	fertilize [-17.609879]	the [-7.992825]	</s> [-2.263188]	fertilize [-17.426943]	your [-9.003828]	</s> [-2.344019]	ginger [-16.953930]	one [-9.362791]	</s> [-3.143920]	ginger [-16.968981]	the [-8.104475]	</s> [-2.064471]	misconduct [-17.247810]	a [-8.903181]	</s> [-1.436153]	misconduct [-17.351562]	one [-9.143655]	</s> [-3.464722]	misconduct [-17.314442]	hundred [-8.581659]	</s> [-2.669843]	power [-17.272062]	the [-8.156525]	</s> [-1.850430]	power [-17.298407]	hundred [-8.724997]	</s> [-2.652755]	circumcise [-18.564913]	the [-8.118220]	</s> [-1.797484]	circumcise [-18.653837]	your [-8.721931]	</s> [-2.922615]	circumcise [-18.490726]	one [-8.946864]	</s> [-3.236856]	distemper [-17.403330]	my [-8.289572]	</s> [-2.500136]	distemper [-17.425400]	one [-9.272594]	</s> [-3.064883]	distemper [-17.636648]	the [-8.007942]	</s> [-1.406077]	misbehave [-17.685843]	your [-8.331985]	</s> [-2.780588]	back [-16.412731]	one [-6.043851]	</s> [-3.293770]	disrespect [-18.794142]	the [-8.227623]	</s> [-2.223570]	disrespect [-18.654037]	your [-9.088299]	</s> [-3.375772]	disrespect [-18.800844]	one [-8.873442]	</s> [-3.852673]	intermarry [-18.486824]	my [-9.051646]	</s> [-3.487871]	intermarry [-18.180731]	your [-9.497023]	</s> [-3.453501]	privilege [-17.763323]	the [-7.689823]	</s> [-2.384178]	come [-19.044531]	a [-7.794365]	</s> [-1.065066]	come [-19.010620]	an [-8.063808]	</s> [-1.028419]	blaspheme [-17.744596]	my [-8.382906]	</s> [-2.673182]	blaspheme [-17.695229]	one [-9.017816]	</s> [-3.122274]	vault [-18.005823]	your [-9.319263]	</s> [-3.569795]	outbalance [-18.725227]	your [-8.490996]	</s> [-2.875599]	outbalance [-18.855783]	the [-8.095996]	</s> [-1.306868]	predetermine [-17.649660]	an [-8.632425]	</s> [-1.069417]	predetermine [-17.610846]	one [-8.801626]	</s> [-3.245040]	number [-17.194674]	your [-8.436779]	</s> [-3.262645]	number [-17.309206]	an [-8.688805]	</s> [-1.483845]	criticise [-17.756449]	an [-8.902356]	</s> [-1.105108]	criticise [-17.756792]	one [-9.170371]	</s> [-3.516094]	blackguard [-18.272085]	one [-9.157062]	</s> [-3.505706]	crash [-16.909571]	your [-8.792656]	</s> [-2.945868]	constrict [-17.267115]	my [-8.045242]	</s> [-3.526851]	constrict [-17.228748]	one [-9.412480]	</s> [-3.694679]	constrict [-17.151810]	the [-8.035829]	</s> [-1.448677]	constrict [-16.897049]	an [-9.150192]	</s> [-1.432718]	introvert [-16.372879]	one [-9.345086]	</s> [-3.216849]	introvert [-16.421654]	an [-8.913258]	</s> [-1.054820]	interchange [-17.758709]	a [-8.497805]	</s> [-0.707086]	interchange [-17.863657]	the [-8.287485]	</s> [-1.541664]	gallivant [-17.851757]	one [-9.416960]	</s> [-3.344912]	listen [-17.685146]	hundred [-8.943789]	</s> [-3.075369]	listen [-18.017523]	your [-8.923603]	</s> [-3.112542]	listen [-17.822102]	an [-8.895029]	</s> [-1.196056]	break [-17.853821]	hundred [-9.004316]	</s> [-1.928947]	break [-17.891064]	your [-8.920547]	</s> [-2.856763]	blind [-13.321346]	an [-3.382542]	</s> [-1.866989]	soft [-16.482330]	the [-8.676939]	</s> [-2.496066]	keep [-19.181370]	an [-9.393094]	</s> [-1.858548]	rediscover [-17.175924]	an [-8.892966]	</s> [-1.837652]	rediscover [-17.234829]	my [-8.412240]	</s> [-3.036920]	rediscover [-17.371574]	one [-9.158838]	</s> [-3.418199]	replenish [-17.834770]	the [-8.239287]	</s> [-1.770250]	track [-17.549959]	my [-8.383213]	</s> [-2.807444]	wheelbarrow [-18.551363]	my [-8.523438]	</s> [-2.419863]	dillydally [-18.272919]	your [-9.484052]	</s> [-2.517398]	give [-16.177542]	my [-6.000964]	</s> [-2.918094]	give [-16.189470]	one [-6.965784]	</s> [-3.695823]	decentralize [-17.725615]	a [-8.490563]	</s> [-1.250640]	decentralize [-17.778111]	the [-8.015561]	</s> [-2.032454]	decentralize [-17.890770]	my [-8.262801]	</s> [-3.287923]	decentralize [-17.851027]	hundred [-8.635782]	</s> [-3.259341]	folk [-16.911633]	the [-8.288969]	</s> [-1.816562]	folk [-16.949860]	a [-8.859780]	</s> [-1.245672]	folk [-17.061874]	your [-9.354155]	</s> [-2.126454]	bring [-17.519493]	an [-8.956157]	</s> [-1.091199]	bring [-17.624107]	the [-8.099292]	</s> [-2.067097]	bring [-17.646589]	hundred [-8.648928]	</s> [-3.017530]	get [-18.566339]	one [-8.671795]	</s> [-3.333831]	get [-18.386045]	hundred [-8.274119]	</s> [-2.838492]	burlesque [-17.733940]	your [-9.341513]	</s> [-3.439298]	burlesque [-17.671963]	an [-9.315886]	</s> [-1.607508]	burlesque [-17.509621]	the [-8.689459]	</s> [-2.177830]	booby [-17.310850]	hundred [-9.279126]	</s> [-2.755512]	booby [-17.689619]	an [-8.778152]	</s> [-1.462895]	booby [-17.615831]	a [-8.807051]	</s> [-1.834742]	booby [-17.559864]	my [-8.540744]	</s> [-3.171463]	booby [-17.435966]	the [-8.195148]	</s> [-2.640752]	secularize [-18.734457]	hundred [-9.081866]	</s> [-2.931422]	secularize [-18.867950]	a [-8.681531]	</s> [-1.501590]	interfere [-19.403463]	your [-9.292099]	</s> [-3.364261]	backtrack [-16.355349]	hundred [-8.327309]	</s> [-2.881689]	prejudice [-17.958101]	your [-9.124884]	</s> [-3.177587]	prejudice [-17.966614]	the [-7.816059]	</s> [-2.641607]	hero [-17.927832]	one [-9.214891]	</s> [-3.592180]	hero [-17.598087]	your [-8.987232]	</s> [-2.880354]	hero [-17.599499]	hundred [-8.796827]	</s> [-2.876721]	intellectualize [-17.551340]	one [-8.668514]	</s> [-3.063101]	intellectualize [-17.570528]	your [-8.072031]	</s> [-3.057064]	intellectualize [-17.502386]	a [-8.129216]	</s> [-1.291190]	stanchion [-17.640999]	one [-9.061788]	</s> [-3.058192]	stanchion [-17.463612]	my [-8.484632]	</s> [-3.028981]	stanchion [-17.293684]	the [-8.181883]	</s> [-1.493347]	sneeze [-17.871578]	hundred [-9.009427]	</s> [-3.163601]	sneeze [-18.062180]	my [-8.392683]	</s> [-3.270622]	sneeze [-18.009684]	your [-9.354012]	</s> [-3.027348]	sneeze [-17.784515]	a [-8.927376]	</s> [-1.374670]	inconvenience [-16.828346]	a [-8.687997]	</s> [-1.671599]	inconvenience [-16.894194]	the [-8.038765]	</s> [-2.237309]	rubber [-18.598261]	my [-8.427255]	</s> [-3.053414]	provision [-18.239140]	hundred [-9.458473]	</s> [-2.781598]	provision [-18.329901]	a [-9.439851]	</s> [-1.052998]	provision [-18.317266]	one [-9.620864]	</s> [-3.165192]	commingle [-16.868935]	the [-7.842388]	</s> [-0.929618]	commingle [-16.950359]	hundred [-8.803390]	</s> [-2.465959]	overemphasize [-17.187979]	a [-8.317928]	</s> [-1.383460]	glamorize [-17.221197]	hundred [-8.307490]	</s> [-2.560818]	bump [-17.758493]	my [-8.259163]	</s> [-3.021885]	bump [-17.648230]	a [-8.573793]	</s> [-0.847452]	bump [-17.647585]	your [-8.845535]	</s> [-2.829209]	bump [-17.797337]	an [-8.799971]	</s> [-2.008419]	jeopardize [-18.081818]	hundred [-8.748843]	</s> [-3.007880]	jeopardize [-18.009026]	my [-8.374599]	</s> [-2.693645]	medium [-13.620318]	the [-3.557482]	</s> [-2.435126]	square [-15.862802]	an [-8.579876]	</s> [-1.705341]	follow [-17.240824]	an [-8.722233]	</s> [-1.532795]	follow [-17.189621]	a [-8.564835]	</s> [-1.662749]	follow [-17.285908]	your [-8.821574]	</s> [-3.057934]	smooth [-18.129473]	the [-7.641393]	</s> [-2.254978]	smooth [-18.126749]	my [-7.866142]	</s> [-3.104039]	commercialize [-17.200401]	a [-8.608347]	</s> [-1.415341]	commercialize [-17.375803]	your [-8.396518]	</s> [-2.929770]	commercialize [-17.347078]	one [-8.998910]	</s> [-3.391355]	run [-15.904425]	one [-6.084241]	</s> [-2.953998]	run [-15.790764]	my [-5.298818]	</s> [-2.905850]	run [-15.671451]	an [-6.876992]	</s> [-1.529082]	run [-15.864126]	your [-5.748295]	</s> [-3.251947]	run [-15.757627]	hundred [-6.778320]	</s> [-3.400523]	weasel [-17.168915]	my [-7.831873]	</s> [-2.926471]	weasel [-17.371120]	hundred [-8.418908]	</s> [-3.179437]	rustle [-17.294497]	your [-8.311601]	</s> [-2.598801]	rustle [-17.144123]	the [-7.660917]	</s> [-1.628872]	rustle [-17.326202]	a [-8.517348]	</s> [-1.748646]	rustle [-17.235006]	my [-7.857218]	</s> [-3.238285]	choke [-17.750647]	the [-8.681036]	</s> [-1.491908]	choke [-17.814024]	hundred [-9.087538]	</s> [-2.493761]	choke [-17.861671]	a [-9.180979]	</s> [-1.169578]	choke [-17.731300]	one [-9.317665]	</s> [-3.218404]	hydroplane [-17.411638]	my [-8.434413]	</s> [-3.038054]	hydroplane [-17.240355]	the [-8.638120]	</s> [-1.779604]	hydroplane [-17.317251]	an [-9.237818]	</s> [-1.662080]	hydroplane [-17.378880]	one [-9.570988]	</s> [-2.884331]	buttonhole [-18.715729]	hundred [-8.756320]	</s> [-2.868874]	revitalise [-18.298319]	your [-8.544213]	</s> [-2.495979]	revitalise [-18.135757]	my [-8.015863]	</s> [-2.299886]	close [-18.256653]	hundred [-8.701172]	</s> [-2.355105]	close [-18.173151]	the [-8.162321]	</s> [-0.924300]	close [-18.280571]	my [-8.420296]
2024-12-19 08:41:39 | INFO | fairseq_cli.eval_lm | Evaluated 627 tokens in 0.8s (737.79 tokens/s)
2024-12-19 08:41:39 | INFO | fairseq_cli.eval_lm | Loss (base 2): 13.7592, Perplexity: 13865.13
