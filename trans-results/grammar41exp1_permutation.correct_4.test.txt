2024-12-19 08:35:37 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX
2024-12-19 08:35:39 | INFO | fairseq_cli.eval_lm | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/grammar41exp1_permutation/4-transformer/checkpoint_best.pt', 'post_process': None, 'quiet': True, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': True, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': None, 'task': {'_name': 'language_modeling', 'data': 'data-bin/grammar41exp1_permutation/correct_4-dataset', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2024-12-19 08:35:39 | INFO | fairseq.tasks.language_modeling | dictionary: 1136 types
2024-12-19 08:35:39 | INFO | fairseq_cli.eval_lm | loading model(s) from checkpoints/grammar41exp1_permutation/4-transformer/checkpoint_best.pt
/workspace/artificial-languages/fairseq/fairseq/checkpoint_utils.py:340: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(f, map_location=torch.device("cpu"))
2024-12-19 08:35:42 | INFO | fairseq_cli.eval_lm | num. model params: 10,038,784
2024-12-19 08:35:42 | INFO | fairseq.data.data_utils | loaded 134 examples from: data-bin/grammar41exp1_permutation/correct_4-dataset/test
2024-12-19 08:35:42 | INFO | fairseq_cli.eval_lm | data-bin/grammar41exp1_permutation/correct_4-dataset test 1 examples
2024-12-19 08:35:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2024-12-19 08:35:42 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2024-12-19 08:35:42 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2024-12-19 08:35:42 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1
2024-12-19 08:35:43 | INFO | fairseq_cli.eval_lm | 0 brutalize [-15.392027]	hundred [-6.341141]	</s> [-1.355609]	brutalize [-17.818117]	your [-7.363313]	</s> [-1.878671]	brutalize [-18.035660]	a [-4.661606]	</s> [-1.592057]	racketeer [-18.064007]	the [-4.419378]	</s> [-1.507633]	disembowel [-18.811129]	your [-8.257076]	</s> [-1.990308]	slaughter [-17.598907]	hundred [-8.329242]	</s> [-1.737590]	slaughter [-17.716425]	a [-5.147525]	</s> [-1.761490]	headquarter [-17.842913]	a [-4.719248]	</s> [-1.720430]	apologize [-17.709354]	your [-7.854644]	</s> [-1.923368]	apologize [-17.857586]	an [-4.642617]	</s> [-1.684258]	fertilize [-18.009104]	an [-4.230559]	</s> [-1.763875]	fertilize [-17.919247]	the [-4.231689]	</s> [-1.554035]	misconduct [-17.051594]	a [-4.286940]	</s> [-1.691661]	misconduct [-17.070417]	one [-4.210888]	</s> [-1.680214]	circumcise [-19.383760]	the [-4.408162]	</s> [-1.604599]	circumcise [-19.396111]	hundred [-7.723299]	</s> [-1.717812]	circumcise [-19.470800]	your [-7.719552]	</s> [-2.050121]	misbehave [-18.576723]	an [-4.574884]	</s> [-1.826454]	misbehave [-18.420820]	your [-8.508213]	</s> [-1.958956]	misbehave [-18.245605]	one [-4.272355]	</s> [-1.783766]	back [-17.025902]	one [-4.059093]	</s> [-1.804910]	disrespect [-18.057238]	hundred [-7.468808]	</s> [-1.758730]	disrespect [-18.062658]	the [-4.321566]	</s> [-1.638646]	privilege [-18.072031]	the [-4.039751]	</s> [-1.717157]	come [-19.734921]	a [-4.707155]	</s> [-1.765311]	blaspheme [-18.822725]	one [-4.808434]	</s> [-1.825727]	blaspheme [-18.791452]	hundred [-8.438988]	</s> [-1.719347]	outbalance [-18.196388]	your [-9.017899]	</s> [-2.009037]	outbalance [-18.320591]	the [-3.928109]	</s> [-1.675185]	outbalance [-18.139915]	an [-4.735400]	</s> [-1.795820]	number [-18.115929]	your [-8.072948]	</s> [-1.981956]	criticise [-18.742254]	an [-4.233559]	</s> [-1.799665]	criticise [-18.652203]	one [-4.189640]	</s> [-1.745559]	criticise [-18.635376]	my [-4.194552]	</s> [-1.887724]	blackguard [-19.726645]	one [-4.687086]	</s> [-1.808222]	blackguard [-19.597965]	your [-8.586831]	</s> [-2.063688]	constrict [-16.731440]	the [-4.180102]	</s> [-1.679189]	introvert [-18.487892]	one [-4.919166]	</s> [-1.831531]	interchange [-18.047403]	the [-3.814283]	</s> [-1.736437]	interchange [-18.256716]	hundred [-7.819733]	</s> [-1.705706]	listen [-19.339493]	hundred [-7.756809]	</s> [-1.762148]	listen [-19.255772]	an [-4.502279]	</s> [-1.757958]	listen [-18.997652]	the [-3.969225]	</s> [-1.508884]	break [-17.944786]	your [-8.326464]	</s> [-2.096136]	blind [-14.412655]	an [-3.332526]	</s> [-1.818449]	blind [-14.349541]	the [-2.437391]	</s> [-1.589974]	soft [-17.439947]	the [-3.786701]	</s> [-1.609093]	rediscover [-17.059465]	one [-4.086125]	</s> [-1.769120]	replenish [-19.001987]	the [-4.311310]	</s> [-1.703412]	replenish [-19.111876]	an [-4.777347]	</s> [-1.770509]	track [-19.773552]	my [-4.405137]	</s> [-1.903940]	dillydally [-19.095413]	my [-4.262134]	</s> [-1.915722]	give [-17.514450]	my [-4.164314]	</s> [-1.913016]	decentralize [-18.515038]	a [-4.815580]	</s> [-1.726629]	decentralize [-18.528931]	the [-4.413613]	</s> [-1.600297]	decentralize [-18.658115]	hundred [-7.390764]	</s> [-1.704729]	folk [-18.142050]	your [-7.693600]	</s> [-2.122163]	folk [-17.940405]	one [-4.104734]	</s> [-1.759461]	bring [-17.865448]	the [-3.893800]	</s> [-1.620698]	bring [-17.937157]	my [-4.130649]	</s> [-1.876244]	get [-18.772112]	hundred [-7.954464]	</s> [-1.712817]	burlesque [-18.724865]	your [-8.348881]	</s> [-2.032473]	burlesque [-18.821810]	the [-4.199676]	</s> [-1.608503]	booby [-18.445898]	hundred [-7.760274]	</s> [-1.689022]	booby [-18.361778]	an [-4.032528]	</s> [-1.715600]	secularize [-18.030724]	hundred [-8.109290]	</s> [-1.714718]	secularize [-17.818058]	a [-4.734734]	</s> [-1.694530]	interfere [-18.668243]	your [-8.261769]	</s> [-1.876112]	interfere [-18.758223]	the [-3.933248]	</s> [-1.631686]	prejudice [-18.432377]	the [-3.999757]	</s> [-1.687051]	hero [-18.373001]	an [-5.048564]	</s> [-1.676781]	intellectualize [-17.855938]	one [-4.134339]	</s> [-1.656319]	stanchion [-18.615511]	one [-4.354644]	</s> [-1.618407]	stanchion [-18.586739]	my [-4.261105]	</s> [-1.923521]	stanchion [-18.802570]	the [-4.091490]	</s> [-1.603055]	sneeze [-17.427366]	your [-7.896397]	</s> [-1.933093]	inconvenience [-18.075809]	one [-4.659719]	</s> [-1.681552]	rubber [-18.431549]	my [-4.669240]	</s> [-1.902163]	provision [-18.300306]	hundred [-7.960909]	</s> [-1.688796]	provision [-18.218874]	a [-4.628530]	</s> [-1.796143]	provision [-18.030066]	one [-4.630122]	</s> [-1.833005]	sound [-18.662882]	a [-4.409410]	</s> [-1.708330]	commingle [-17.773203]	the [-3.751384]	</s> [-1.632071]	overemphasize [-18.473059]	a [-4.870129]	</s> [-1.789774]	bump [-17.373665]	my [-4.860435]	</s> [-2.035508]	bump [-17.463659]	a [-4.545835]	</s> [-1.731688]	bump [-17.759384]	an [-4.618584]	</s> [-1.785646]	bump [-17.655260]	one [-4.623775]	</s> [-1.766430]	jeopardize [-17.564133]	my [-4.701456]	</s> [-1.973726]	medium [-14.658826]	hundred [-2.416092]	</s> [-1.747760]	square [-17.385731]	your [-8.155730]	</s> [-2.163136]	follow [-17.240133]	an [-4.087077]	</s> [-1.748359]	follow [-17.176033]	your [-8.584286]	</s> [-2.218081]	smooth [-19.606304]	my [-4.840696]	</s> [-2.072521]	smooth [-19.331345]	hundred [-7.340039]	</s> [-1.753455]	commercialize [-18.061846]	your [-8.357797]	</s> [-2.072781]	run [-16.237783]	one [-3.852409]	</s> [-1.661341]	run [-16.240749]	my [-4.252115]	</s> [-1.883545]	weasel [-18.752649]	hundred [-7.946469]	</s> [-1.637851]	rustle [-18.775183]	your [-8.582438]	</s> [-1.981332]	choke [-18.039732]	a [-4.941410]	</s> [-1.690619]	hydroplane [-18.817465]	an [-4.451854]	</s> [-1.802132]	hydroplane [-19.033409]	one [-4.196559]	</s> [-1.750738]	hydroplane [-19.037672]	hundred [-7.268277]	</s> [-1.752018]	buttonhole [-18.714151]	hundred [-8.495266]	</s> [-1.714622]	close [-17.827379]	hundred [-8.763614]	</s> [-1.675483]	close [-17.821228]	my [-4.943933]	</s> [-1.902781]	close [-18.104940]	one [-4.411280]	</s> [-1.758820]	undervalue [-17.882025]	your [-8.002268]	</s> [-1.961721]	overspecialize [-17.624180]	your [-8.929121]	</s> [-1.884945]	promenade [-18.537518]	your [-8.862360]	</s> [-1.953558]	still [-18.297976]	my [-5.082544]	</s> [-1.900812]	slap [-18.141325]	an [-4.395062]	</s> [-1.751396]	mispronounce [-18.959507]	a [-4.923322]	</s> [-1.788252]	mispronounce [-19.153997]	hundred [-8.945902]	</s> [-1.759727]	sweet [-13.351084]	the [-2.540149]	</s> [-1.651661]	mountebank [-18.725769]	my [-5.219098]	</s> [-2.020471]	mountebank [-18.710711]	hundred [-7.788670]	</s> [-1.718801]	bucket [-19.029192]	an [-5.053075]	</s> [-1.829282]	bucket [-18.826742]	the [-4.406943]	</s> [-1.621938]	pressurize [-18.275415]	hundred [-7.858280]	</s> [-1.675719]	influence [-18.109440]	one [-3.915615]	</s> [-1.773608]	dress [-18.984461]	your [-8.000098]	</s> [-2.009805]	dress [-18.792896]	a [-4.959152]	</s> [-1.703225]	crosscheck [-18.658302]	one [-4.268663]	</s> [-1.733955]	crosscheck [-18.481762]	my [-4.537896]	</s> [-1.925113]	crosscheck [-18.368267]	your [-8.599542]	</s> [-1.928307]	weird [-14.131411]	a [-3.886842]	</s> [-1.718854]	intercede [-19.419500]	hundred [-7.278406]	</s> [-1.748165]	intercede [-19.186275]	an [-4.478280]	</s> [-1.707498]	reemphasize [-17.625814]	one [-4.347728]	</s> [-1.787500]	reemphasize [-17.507532]	a [-4.613560]	</s> [-1.821905]	reemphasize [-17.652393]	my [-4.733097]	</s> [-1.918862]	spearhead [-18.020592]	one [-4.214713]	</s> [-1.704317]
2024-12-19 08:35:43 | INFO | fairseq_cli.eval_lm | Evaluated 402 tokens in 1.0s (417.41 tokens/s)
2024-12-19 08:35:43 | INFO | fairseq_cli.eval_lm | Loss (base 2): 12.2410, Perplexity: 4840.71
